<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<link rel="StyleSheet" href="./style.css" type="text/css" media="all" /> 

<title>PixelNN</title> 
<style type="text/css">
#primarycontent h1 {
	font-variant: small-caps;
}
#primarycontent h3 {
}
#primarycontent teasertext {
	text-align: center;
}
#primarycontent p {
	text-align: center;
}
#primarycontent {
	text-align: justify;
}
#primarycontent p {
	text-align: justify;
}
#primarycontent p iframe {
	text-align: center;
}
.featart {
  margin:4px;
}
</style>

<script type="text/javascript">
  function togglevis(elid){
    el=document.getElementById(elid);
    aelid=elid+"a";
    ael=document.getElementById(aelid);
    if(el.style.display=='none'){
      el.style.display='inline-table';
      ael.innerHTML="[Hide BibTex]";
    }else{
      el.style.display='none';
      ael.innerHTML="[Show BibTex]";
    }
  }
</script>
</head> 
<body itemscope="" itemtype="http://schema.org/ScholarlyArticle"> 
<div id="primarycontent"> 
<p class="hiddenDiv" itemprop="url">http://cs.cmu.edu/~aayushb/hallucinate/</p>
<h1 align="center" itemprop="name"><strong>PixelNN: Example-based Image Synthesis</strong></h1>
<h4>
<center><a href="http://www.cs.cmu.edu/~aayushb/">Aayush Bansal</a>, <a href="http://www.cs.cmu.edu/~yaser">Yaser Sheikh</a>, <a href="http://www.cs.cmu.edu/~deva">Deva Ramanan</a> </center>
</h4>
<div class="blank"></div>

<center><img src="./teaser1.png" itemprop="image" width="800" alt="teaserImage" /></center>
<center><img src="./teaser2.png" itemprop="image" width="800" alt="teaserImage" /></center>


<h3>Abstract</h3>
<p style="padding-left: 10px;&#9;padding-right: 10px;">

We present a simple nearest-neighbor (NN) approach that synthesizes high-frequency photorealistic images from an "incomplete" signal such as a low-resolution image, a surface normal map, or edges. Current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem. (2) they are not interpretable, making it difficult to control the synthesized output. We demonstrate that NN approaches potentially address such limitations, but suffer in accuracy on small datasets. We design a simple pipeline that combines the best of both worlds:  the first stage uses a convolutional neural network (CNN) to maps the input to a (overly-smoothed) image, and the second stage uses a pixel-wise nearest neighbor method to map the smoothed output to multiple high-quality, high-frequency outputs in a controllable manner.  We demonstrate our approach for various input modalities, and for various domains ranging from human faces to cats-and-dogs to shoes and handbags.
</p>

<h3> Paper <!-- & Presentation--></h3>
<br /><br />
<table><tbody><tr>
  <td>
    <p style="padding-left: 10px;       padding-right: 0px;text-align:right;">
    <a href="https://arxiv.org/pdf/1708.05349.pdf"><img style="border:solid 0px #000;margin-right:00px;" src="./paper.png" width="150" /></a></p>
  </td>
  <td valign="middle">
    <p style="text-align:left;">
    <b> PixelNN: Example-based Image Synthesis. </b> <br /> <br />
    A. Bansal, Y. Sheikh, and D. Ramanan <br /><br />
    <a href="https://arxiv.org/pdf/1708.05349.pdf">arXiv</a> | <a href="bibtex_pixelNN.html" target="_blank">bibtex</a>    <br /><br />

    </p>

<p style="margin-top:10px;">
    </p>
          
       </td></tr></tbody></table>
<p></p>

<h4> Comparison with <a href="https://phillipi.github.io/pix2pix/">Pix-to-Pix</a></h4>
<center><img src="./pix-to-pix.png" itemprop="image" width="800" alt="Comparison" /></center>

<h4> Multiple Outputs</h4>
<center><img src="./mult_1.jpg" itemprop="image" width="800" alt="Multiple" /></center>
<center><img src="./mult_2.jpg" itemprop="image" width="800" alt="Multiple" /></center>
<center><img src="./multiple_outputs.png" itemprop="image" width="800" alt="Multiple" /></center>


<h4> Edges-to-Faces</h4>
<center><img src="./fig_edges2face.png" itemprop="image" width="800" alt="Edges2Faces" /></center>

<h4> Normals-to-Faces</h4>
<center><img src="./fig_normals2face.png" itemprop="image" width="800" alt="Normals2Faces" /></center>

<h4> Edges-to-Cats-&amp;-Dogs</h4>
<center><img src="./fig_edges2catsanddogs.png" itemprop="image" width="800" alt="Edges2CatsAndDogs" /></center>

<h4> Normals-to-Cats-&amp;-Dogs</h4>
<center><img src="./fig_normals2catsanddogs.png" itemprop="image" width="800" alt="Normals2CatsAndDogs" /></center>

<h3> Example Frequency Analysis <!-- & Presentation--></h3>
<br />
We did frequency analysis via FFT to understand the frequency content in the output of our images.
<br />
<center><img src="./freq_analysis.png" itemprop="image" width="800" alt="FreqAnalysis" /></center>

<h3> </h3>
<p style="padding-left: 10px; padding-right: 10px;">
Comments, questions to <a href="http://www.cs.cmu.edu/~aayushb/" target="_blank">Aayush Bansal</a>. </p>
</div>



</body></html>