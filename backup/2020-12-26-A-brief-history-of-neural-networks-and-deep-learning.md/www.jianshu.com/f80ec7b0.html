<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0" /><meta http-equiv="X-UA-Compatible" content="ie=edge,chrome=1" /><meta http-equiv="Cache-Control" content="no-siteapp" /><meta http-equiv="Cache-Control" content="no-transform" /><meta name="applicable-device" content="pc,mobile" /><meta name="MobileOptimized" content="width" /><meta name="HandheldFriendly" content="true" /><meta name="theme-color" content="#ec7259" /><meta name="renderer" content="webkit" /><meta name="force-rendering" content="webkit" /><meta name="google" value="notranslate" /><meta property="wb:webmaster" content="294ec9de89e7fadb" /><meta property="qc:admins" content="104102651453316562112116375" /><meta property="qc:admins" content="11635613706305617" /><meta property="qc:admins" content="1163561616621163056375" /><meta name="360-site-verification" content="604a14b53c6b871206001285921e81d8" /><meta name="google-site-verification" content="cV4-qkUJZR6gmFeajx_UyPe47GW9vY6cnCrYtCHYNh4" /><meta name="google-site-verification" content="HF7lfF8YEGs1qtCE-kPml8Z469e2RHhGajy6JPVy5XI" /><meta name="tencent-site-verification" content="da26ce22cfed7aba6a96d8409f9b53a6" /><meta name="apple-mobile-web-app-title" content="简书" /><link href="data:image/vnd.microsoft.icon;base64,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" rel="shortcut icon" type="image/x-icon" /><link rel="dns-prefetch" href="//cdn2.jianshu.io" /><link rel="dns-prefetch" href="//upload-images.jianshu.io" /><meta http-equiv="mobile-agent" content="format=html5; url=https://www.jianshu.com/p/e1bac195f06d" /><meta name="apple-itunes-app" content="app-id=888237539, app-argument=jianshu://notes/18136088" /><meta property="al:ios:url" content="jianshu://notes/18136088" /><meta property="al:ios:app_store_id" content="888237539" /><meta property="al:ios:app_name" content="简书" /><meta property="al:android:url" content="jianshu://notes/18136088" /><meta property="al:android:package" content="com.jianshu.haruki" /><meta property="al:android:app_name" content="简书" /><title>深度 | 神经网络和深度学习简史第四部分：深度学习终迎伟大复兴  - 简书</title><meta name="robots" content="index,follow" /><meta name="googlebot" content="index,follow" /><meta name="description" content="来自Andrey KurenkovA 'Brief' History of Neural Nets and Deep Learning, Part4深度 | 神经网络和深度学..." /><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@jianshu.com" /><meta property="fb:app_id" content="865829053512461" /><meta property="og:url" content="https://www.jianshu.com/p/e1bac195f06d" /><meta property="og:type" content="article" /><meta property="og:title" content="深度 | 神经网络和深度学习简史第四部分：深度学习终迎伟大复兴 " /><meta property="og:description" content="来自Andrey KurenkovA 'Brief' History of Neural Nets and Deep Learning, Part4深度 | 神经网络和深度学..." /><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1496926-3b81b0bae7b4864c" /><meta property="og:site_name" content="简书" /><meta name="next-head-count" content="45" /><link rel="preload" href="https://cdn2.jianshu.io/shakespeare/_next/static/mJJ-kJwXxgtj1jdtJSnxx/pages/p/%5Bslug%5D.js" as="script" /><link rel="preload" href="https://cdn2.jianshu.io/shakespeare/_next/static/mJJ-kJwXxgtj1jdtJSnxx/pages/_app.js" as="script" /><link rel="preload" href="https://cdn2.jianshu.io/shakespeare/_next/static/runtime/webpack-69a7d3bdb55520eaee8f.js" as="script" /><link rel="preload" href="https://cdn2.jianshu.io/shakespeare/_next/static/chunks/commons.46a9d5f73f088e7dfd5a.js" as="script" /><link rel="preload" href="https://cdn2.jianshu.io/shakespeare/_next/static/chunks/styles.e6b2d8c62a1ec682db37.js" as="script" /><link rel="preload" href="https://cdn2.jianshu.io/shakespeare/_next/static/runtime/main-8f9d517354ec2be84ab8.js" as="script" /><link rel="preload" href="https://cdn2.jianshu.io/shakespeare/_next/static/css/commons.7df24f11.chunk.css" as="style" /><link rel="stylesheet" href="https://cdn2.jianshu.io/shakespeare/_next/static/css/commons.7df24f11.chunk.css" /><link rel="preload" href="https://cdn2.jianshu.io/shakespeare/_next/static/css/styles.2f727936.chunk.css" as="style" /><link rel="stylesheet" href="https://cdn2.jianshu.io/shakespeare/_next/static/css/styles.2f727936.chunk.css" /><link rel="stylesheet" type="text/css" href="https://cdn2.jianshu.io/shakespeare/_next/static/css/12.2f97d03e.chunk.css" /><script type="text/javascript" async="" src="https://pos.baidu.com/auto_dup?psi=b6039f1cb51296d7a9906224a66b9f58&amp;di=0&amp;dri=0&amp;dis=0&amp;dai=0&amp;ps=0x0&amp;enu=encoding&amp;exps=110011&amp;ant=0&amp;dcb=___baidu_union_callback&amp;dtm=AUTO_JSONP&amp;dvi=0.0&amp;dci=-1&amp;dpt=none&amp;tsr=0&amp;tpr=1608949919084&amp;ti=%E6%B7%B1%E5%BA%A6%20%7C%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8F%B2%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%88%E8%BF%8E%E4%BC%9F%E5%A4%A7%E5%A4%8D%E5%85%B4%20-%20%E7%AE%80%E4%B9%A6&amp;ari=2&amp;ver=1224&amp;dbv=2&amp;drs=3&amp;pcs=912x872&amp;pss=1032x14169&amp;cfv=0&amp;cpl=3&amp;chi=2&amp;cce=true&amp;cec=UTF-8&amp;tlm=1608949919&amp;prot=2&amp;rw=889&amp;ltu=https%3A%2F%2Fwww.jianshu.com%2Fp%2Fe1bac195f06d&amp;ecd=1&amp;uc=1920x1040&amp;pis=-1x-1&amp;sr=1920x1080&amp;tcn=1608949919&amp;dc=4"></script><script charset="utf-8" src="https://cdn2.jianshu.io/shakespeare/_next/static/chunks/12.7448138930bf90c350c7.js"></script><link rel="stylesheet" type="text/css" href="https://cdn2.jianshu.io/shakespeare/_next/static/css/13.8c3b56fa.chunk.css" /><script charset="utf-8" src="https://cdn2.jianshu.io/shakespeare/_next/static/chunks/13.58a30044fc4352d53762.js"></script><script charset="utf-8" src="https://cdn2.jianshu.io/shakespeare/_next/static/chunks/11.75b56346ed2db2849ee4.js"></script><link rel="stylesheet" type="text/css" href="https://cdn2.jianshu.io/shakespeare/_next/static/css/10.f8bf8f2f.chunk.css" /><script charset="utf-8" src="https://cdn2.jianshu.io/shakespeare/_next/static/chunks/10.9e56024fbbf8ece43854.js"></script><script charset="utf-8" src="https://cdn2.jianshu.io/shakespeare/_next/static/chunks/15.68f3d52cebcd122a7753.js"></script></head><body class=""><div id="BAIDU_DUP_fp_wrapper" style="position: absolute; left: -1px; bottom: -1px; z-index: 0; width: 0px; height: 0px; overflow: hidden; visibility: hidden; display: none;"><iframe id="BAIDU_DUP_fp_iframe" src="https://pos.baidu.com/wh/o.htm?ltr=" style="width: 0px; height: 0px; visibility: hidden; display: none;"></iframe></div><svg xmlns="http://www.w3.org/2000/svg" class="wCYvWN" style="display:none;width:0;height:0" width="0" height="0" focusable="false" aria-hidden="true"><symbol id="ic-icon_requests" viewBox="0 0 1024 1024"><path d="M934.4 627.2A38.4 38.4 0 0 1 972.8 665.6v128a140.8 140.8 0 0 1-140.8 140.8H192A140.8 140.8 0 0 1 51.2 793.6V665.6a38.4 38.4 0 1 1 76.8 0v128c0 35.328 28.672 64 64 64h640c35.328 0 64-28.672 64-64V665.6a38.4 38.4 0 0 1 38.4-38.4zM587.4688 91.392l281.8048 244.224a64 64 0 0 1-41.8816 112.384h-148.992V665.6a89.6 89.6 0 0 1-89.6 89.6h-153.6A89.6 89.6 0 0 1 345.6 665.6V448H196.608a64 64 0 0 1-41.8816-112.384l281.8048-244.224a115.2 115.2 0 0 1 150.9376 0zM486.8608 149.4016L230.912 371.2H384a38.4 38.4 0 0 1 38.4 38.4v256c0 7.0656 5.7344 12.8 12.8 12.8h153.6a12.8 12.8 0 0 0 12.8-12.8V409.6a38.4 38.4 0 0 1 38.4-38.4h153.088L537.088 149.4016a38.4 38.4 0 0 0-50.2784 0z"/></symbol><symbol id="ic-icon_others" viewBox="0 0 1024 1024"><path d="M512 435.2a76.8 76.8 0 1 1 0 153.6 76.8 76.8 0 0 1 0-153.6z m-307.2 0a76.8 76.8 0 1 1 0 153.6 76.8 76.8 0 0 1 0-153.6z m614.4 0a76.8 76.8 0 1 1 0 153.6 76.8 76.8 0 0 1 0-153.6z"/></symbol><symbol id="ic-icon_money" viewBox="0 0 1024 1024"><path d="M512 51.2a460.8 460.8 0 1 1 0 921.6 460.8 460.8 0 0 1 0-921.6z m0 76.8a384 384 0 1 0 0 768 384 384 0 0 0 0-768zM435.0976 311.3984L510.1568 450.56l75.264-139.52a37.7344 37.7344 0 0 1 49.664-16.0256 33.9456 33.9456 0 0 1 14.7968 47.104l-76.288 135.7824h102.4a29.0304 29.0304 0 1 1 0 58.0608h-127.0272v55.0912h126.976a29.0304 29.0304 0 1 1 0 58.0608h-126.976v49.152a38.8096 38.8096 0 1 1-77.6192 0v-49.152H345.2416a29.0304 29.0304 0 1 1 0-58.0608h126.1056v-55.0912H345.2416a29.0304 29.0304 0 0 1 0-58.0608h101.4272L369.0496 342.784a34.304 34.304 0 0 1 15.0016-48.0768 38.9632 38.9632 0 0 1 51.0464 16.6912z"/></symbol><symbol id="ic-icon_follows" viewBox="0 0 1024 1024"><path d="M844.8 614.4a38.4 38.4 0 0 1 38.4 38.4v102.4h102.4a38.4 38.4 0 1 1 0 76.8h-102.4512l0.0512 102.4a38.4 38.4 0 1 1-76.8 0l-0.0512-102.4H704a38.4 38.4 0 1 1 0-76.8h102.4v-102.4a38.4 38.4 0 0 1 38.4-38.4z m-370.176-89.6c80.5888 0 158.3104 16.896 227.4816 48.64a38.4 38.4 0 1 1-32.0512 69.7856 468.2752 468.2752 0 0 0-195.3792-41.6256c-175.616 0-327.7312 93.184-378.112 223.9488-13.4144 38.5536-4.7616 57.5488 7.8336 57.5488H665.6a38.4 38.4 0 1 1 0 76.8H104.448c-74.24 0-109.1584-76.8-80.0256-160.768 62.8736-163.1232 244.3264-274.3296 450.2528-274.3296zM460.8 12.8a243.2 243.2 0 1 1 0 486.4 243.2 243.2 0 0 1 0-486.4z m0 76.8a166.4 166.4 0 1 0 0 332.8 166.4 166.4 0 0 0 0-332.8z"/></symbol><symbol id="ic-icon_comments" viewBox="0 0 1024 1024"><path d="M537.6 51.2a435.2 435.2 0 1 1 0 870.4h-51.2a433.2544 433.2544 0 0 1-209.2544-53.504l-126.976 94.8224a51.2 51.2 0 0 1-81.8688-41.0624v-314.2144A435.2 435.2 0 0 1 486.4 51.2h51.2z m0 76.8h-51.2a358.4 358.4 0 0 0-344.32 458.24c2.048 6.9632 3.072 14.1824 3.072 21.4016v263.168l86.016-64.256a76.8 76.8 0 0 1 82.944-5.7856c52.3264 28.7744 111.104 44.032 172.288 44.032h51.2a358.4 358.4 0 0 0 0-716.8z m89.6 435.2a38.4 38.4 0 1 1 0 76.8h-230.4a38.4 38.4 0 1 1 0-76.8h230.4z m0-204.8a38.4 38.4 0 1 1 0 76.8h-230.4a38.4 38.4 0 0 1 0-76.8h230.4z"/></symbol><symbol id="ic-icon_chat" viewBox="0 0 1024 1024"><path d="M870.4 153.6a102.4 102.4 0 0 1 102.4 102.4v512a102.4 102.4 0 0 1-102.4 102.4H153.6a102.4 102.4 0 0 1-102.4-102.4V256a102.4 102.4 0 0 1 102.4-102.4h716.8z m0 76.8H153.6a25.6 25.6 0 0 0-25.6 25.6v512a25.6 25.6 0 0 0 25.6 25.6h716.8a25.6 25.6 0 0 0 25.6-25.6V256a25.6 25.6 0 0 0-25.6-25.6z m-113.7664 97.3312a38.4 38.4 0 0 1 47.1552 60.5696l-274.5856 213.9136a38.4 38.4 0 0 1-47.2064 0L208.384 389.12a38.4 38.4 0 1 1 47.2064-60.6208l250.0096 194.7648z"/></symbol><symbol id="ic-icon_collection" viewBox="0 0 1024 1024"><path d="M819.2 51.2a102.4 102.4 0 0 1 102.4 102.4v707.6864a76.8 76.8 0 0 1-104.192 71.7824L512 816.4864l-305.408 116.5824A76.8 76.8 0 0 1 102.4 861.2864V153.6a102.4 102.4 0 0 1 102.4-102.4h614.4z m0 76.8H204.8a25.6 25.6 0 0 0-25.6 25.6v707.6864l305.408-116.5312a76.8 76.8 0 0 1 54.784 0l305.408 116.5312V153.6a25.6 25.6 0 0 0-25.6-25.6z m-293.6832 105.984a25.6 25.6 0 0 1 8.192 8.192l63.744 102.2464 116.9408 28.9792a25.6 25.6 0 0 1 13.4144 41.3184L650.24 506.9312l8.5504 120.1664a25.6 25.6 0 0 1-35.1232 25.5488L512 607.3856l-111.616 45.2608a25.6 25.6 0 0 1-35.1744-25.5488L373.76 506.9312l-77.568-92.16a25.6 25.6 0 0 1 13.4144-41.3696l116.9408-28.9792L490.2912 242.176a25.6 25.6 0 0 1 35.2256-8.192zM512 352.512l-20.2752 32.512a76.8 76.8 0 0 1-46.6944 33.9456l-37.1712 9.216 24.6784 29.2864a76.8 76.8 0 0 1 17.8176 54.9376l-2.7136 38.1952 35.4816-14.3872a76.8 76.8 0 0 1 57.7536 0l35.4816 14.336-2.7136-38.144a76.8 76.8 0 0 1 17.8176-54.9376l24.6784-29.2864-37.1712-9.216a76.8 76.8 0 0 1-46.6944-33.9456L512 352.512z"/></symbol><symbol id="ic-icon_help" viewBox="0 0 1024 1024"><path d="M512 51.2a460.8 460.8 0 1 1 0 921.6 460.8 460.8 0 0 1 0-921.6z m0 76.8a384 384 0 1 0 0 768 384 384 0 0 0 0-768zM512 716.8a51.2 51.2 0 1 1 0 102.4 51.2 51.2 0 0 1 0-102.4z m0-481.8944a166.4 166.4 0 0 1 166.4 166.4c0 46.848-26.7264 78.336-79.8208 116.992l-23.552 16.7936a299.008 299.008 0 0 0-18.0736 13.7216c-4.9664 4.1984-6.4 5.9392-6.5536 2.2528v89.088a38.4 38.4 0 1 1-76.8 0V547.84c0.512-23.1936 12.7488-39.936 33.9456-57.7536 6.2464-5.2736 13.1584-10.496 22.7328-17.408 1.2288-0.8704 18.2272-12.9024 23.1424-16.4864 33.9968-24.7296 48.1792-41.472 48.1792-54.8864a89.6 89.6 0 0 0-179.2 0 38.4 38.4 0 0 1-76.8 0A166.4 166.4 0 0 1 512 234.9056z"/></symbol><symbol id="ic-icon_like" viewBox="0 0 1024 1024"><path d="M921.344 180.5312a269.3632 269.3632 0 0 1 0 377.2928l-372.8896 378.0096a51.2 51.2 0 0 1-72.9088 0l-372.8896-378.0096a269.3632 269.3632 0 0 1 0-377.2928 260.608 260.608 0 0 1 372.1216 0l37.2224 37.7344 37.2224-37.7344a260.608 260.608 0 0 1 372.1216 0zM157.3376 234.496a192.5632 192.5632 0 0 0 0 269.4144L512 863.4368l354.6624-359.5776a192.5632 192.5632 0 0 0 0-269.4144 183.808 183.808 0 0 0-262.7584 0L515.4304 324.096 400.128 443.5968a38.4 38.4 0 0 1-55.2448-53.3504L458.0864 272.896l-37.9904-38.5024a183.808 183.808 0 0 0-262.7584 0z"/></symbol><symbol id="ic-icon_purchased" viewBox="0 0 1024 1024"><path d="M819.2 51.2a102.4 102.4 0 0 1 102.4 102.4v716.8a102.4 102.4 0 0 1-102.4 102.4H204.8a102.4 102.4 0 0 1-102.4-102.4V153.6a102.4 102.4 0 0 1 102.4-102.4h614.4z m0 76.8H204.8a25.6 25.6 0 0 0-25.6 25.6v716.8a25.6 25.6 0 0 0 25.6 25.6h614.4a25.6 25.6 0 0 0 25.6-25.6V153.6a25.6 25.6 0 0 0-25.6-25.6z m-67.1744 341.0944a38.4 38.4 0 0 1 10.3424 53.3504l-186.112 275.8656a38.4 38.4 0 0 1-53.2992 10.3936l-148.5824-100.1984a38.4 38.4 0 1 1 43.008-63.6928l116.736 78.7456 164.5568-244.0704a38.4 38.4 0 0 1 53.3504-10.3936zM473.6 435.2a38.4 38.4 0 0 1 0 76.8h-179.2a38.4 38.4 0 0 1 0-76.8h179.2z m102.4-179.2a38.4 38.4 0 1 1 0 76.8h-281.6a38.4 38.4 0 0 1 0-76.8h281.6z"/></symbol><symbol id="ic-icon_logout" viewBox="0 0 1024 1024"><path d="M832 64c70.8096 0 128 58.0608 128 129.4336v45.5168a38.4 38.4 0 1 1-76.8 0v-45.568c0-29.184-23.04-52.5824-51.2-52.5824H243.2c-28.16 0-51.2 23.4496-51.2 52.6336v637.1328c0 29.184 23.04 52.6336 51.2 52.6336h588.8c28.16 0 51.2-23.4496 51.2-52.6336v-45.5168a38.4 38.4 0 0 1 76.8 0v45.568c0 71.3216-57.1904 129.3824-128 129.3824H243.2c-70.8096 0-128-58.0608-128-129.4336V193.4336c0-71.3728 57.1904-129.4336 128-129.4336z m-52.6336 248.064l160.8192 160.8192a38.4 38.4 0 0 1 2.1504 56.32l-162.9184 162.9184a38.4 38.4 0 0 1-54.272-54.272l97.3312-97.3824h-413.696c-13.9264 0-25.4464-14.2336-27.392-32.768l-0.3072-5.632c0-19.3024 10.24-35.2768 23.552-37.9904l4.096-0.4608 413.6448 0.0512-97.28-97.28a38.4 38.4 0 0 1 54.272-54.3232z"/></symbol><symbol id="ic-icon_wallet" viewBox="0 0 1024 1024"><path d="M870.4 128a102.4 102.4 0 0 1 102.4 102.4v563.2a102.4 102.4 0 0 1-102.4 102.4H153.6a102.4 102.4 0 0 1-102.4-102.4v-563.2a102.4 102.4 0 0 1 102.4-102.4h716.8zM870.4 204.8H153.6a25.6 25.6 0 0 0-25.6 25.6V307.2H358.4a204.8 204.8 0 1 1 0 409.6H128v76.8a25.6 25.6 0 0 0 25.6 25.6h716.8a25.6 25.6 0 0 0 25.6-25.6v-563.2A25.6 25.6 0 0 0 870.4 204.8zM358.4 384H128v256H358.4a128 128 0 1 0 0-256z m-25.6 51.2a76.8 76.8 0 1 1 0 153.6 76.8 76.8 0 0 1 0-153.6z"/></symbol><symbol id="ic-icon_mine" viewBox="0 0 1024 1024"><path d="M512.3072 549.4272h6.5024-7.168a588.0832 588.0832 0 0 1 28.16 0.6144l6.3488 0.4096c3.6352 0.1536 7.2704 0.4096 10.9056 0.6656l4.864 0.512c62.8736 5.2224 123.2384 20.6336 178.176 45.1584 1.536 0.7168 3.072 1.536 4.608 2.4064l1.8944 0.8704c100.352 46.6432 178.8416 122.6752 215.9616 216.32 29.184 81.664-5.8368 156.416-80.128 156.416l-178.176-0.0512-0.768 0.0512H141.568C67.328 972.8 32.256 898.048 61.44 816.384c37.12-93.6448 115.5584-169.6768 215.9616-216.32l1.9456-0.8704a39.168 39.168 0 0 1 4.608-2.4064c54.8864-24.576 115.2512-39.936 178.1248-45.2608l4.864-0.3584c3.584-0.3072 7.2704-0.5632 10.9056-0.768l6.3488-0.3072a671.5904 671.5904 0 0 1 28.1088-0.6656h-7.1168 6.5024z m370.176 348.672c12.5952 0 21.248-18.4832 7.8336-56.0128-47.0528-118.6304-182.7328-205.568-343.4496-216.7296a229.9392 229.9392 0 0 0-7.9872-0.512l7.9872 0.512a507.6992 507.6992 0 0 0-69.7344 0l7.9872-0.512c-164.1984 8.8576-303.616 96.6144-351.4368 217.2416-13.4144 37.5296-4.7616 55.9616 7.8848 55.9616h740.864zM498.3808 51.2c134.5024 0 243.5584 105.984 243.5584 236.6464 0 130.7136-109.056 236.6464-243.5584 236.6464S254.8736 418.56 254.8736 287.8464 363.9296 51.2 498.432 51.2z m0 74.752c-92.0064 0-166.656 72.4992-166.656 161.8944 0 89.4464 74.6496 161.9456 166.656 161.9456s166.6048-72.4992 166.6048-161.9456c0-89.3952-74.5984-161.8944-166.6048-161.8944z"/></symbol><symbol id="ic-icon_setting" viewBox="0 0 1024 1024"><path d="M708.864 68.608a102.4 102.4 0 0 1 88.6784 51.2L994.4576 460.8a102.4 102.4 0 0 1 0 102.4l-196.9152 340.992a102.4 102.4 0 0 1-88.6784 51.2h-393.728a102.4 102.4 0 0 1-88.6784-51.2L29.5424 563.2a102.4 102.4 0 0 1 0-102.4l196.9152-340.992a102.4 102.4 0 0 1 88.6784-51.2h393.728z m0 76.8h-393.728a25.6 25.6 0 0 0-22.1696 12.8L96.0512 499.2a25.6 25.6 0 0 0 0 25.6l196.9152 340.992a25.6 25.6 0 0 0 22.1696 12.8h393.728a25.6 25.6 0 0 0 22.1696-12.8l196.9152-340.992a25.6 25.6 0 0 0 0-25.6l-196.9152-340.992a25.6 25.6 0 0 0-22.1696-12.8zM512 345.6a166.4 166.4 0 1 1 0 332.8 166.4 166.4 0 0 1 0-332.8z m0 76.8a89.6 89.6 0 1 0 0 179.2 89.6 89.6 0 0 0 0-179.2z"/></symbol><symbol id="ic-spinner" viewBox="0 0 1024 1024"><path d="M300.571429 817.90476233q0 30.285714-21.428572 51.714285T227.428571 891.04761933q-29.714286 0-51.428571-21.714286t-21.714286-51.428571q0-30.285714 21.428572-51.714286T227.428571 744.76190433t51.714286 21.428572T300.571429 817.90476233z m284.571428 117.714285q0 30.285714-21.428571 51.714286T512 1008.76190433t-51.714286-21.428571T438.857143 935.61904733t21.428571-51.714285T512 862.47619033t51.714286 21.428572 21.428571 51.714285zM182.857143 533.33333333q0 30.285714-21.428572 51.714286T109.714286 606.47619033t-51.714286-21.428571T36.571429 533.33333333t21.428571-51.714286T109.714286 460.19047633t51.714285 21.428571T182.857143 533.33333333z m686.857143 284.571429q0 29.714286-21.714286 51.428571t-51.428571 21.714286q-30.285714 0-51.714286-21.428572T723.428571 817.90476233t21.428572-51.714286 51.714286-21.428572 51.714285 21.428572 21.428572 51.714286zM318.857143 248.76190433q0 37.714286-26.857143 64.571429t-64.571429 26.857143-64.571428-26.857143-26.857143-64.571429 26.857143-64.571428 64.571428-26.857143 64.571429 26.857143 26.857143 64.571428z m668.571428 284.571429q0 30.285714-21.428571 51.714286T914.285714 606.47619033t-51.714285-21.428571T841.142857 533.33333333t21.428572-51.714286T914.285714 460.19047633t51.714286 21.428571T987.428571 533.33333333z m-365.714285-402.285714q0 45.714286-32 77.714285t-77.714286 32-77.714286-32-32-77.714285 32-77.714286T512 21.33333333t77.714286 32 32 77.714286z m302.857143 117.714285q0 53.142857-37.714286 90.571429T796.571429 376.76190433q-53.142857 0-90.571429-37.428571T668.571429 248.76190433q0-52.571429 37.428571-90.285714t90.571429-37.714286q52.571429 0 90.285714 37.714286t37.714286 90.285714z"/></symbol><symbol id="ic-alipay" viewBox="0 0 1024 1024"><path d="M1023.795 853.64v6.348a163.807 163.807 0 0 1-163.807 163.807h-696.18A163.807 163.807 0 0 1 0 859.988v-696.18A163.807 163.807 0 0 1 163.807 0h696.181a163.807 163.807 0 0 1 163.807 163.807V853.64z" fill="#009FE9"/><path d="M844.836 648.267c-40.952-14.333-95.623-34.809-156.846-57.128a949.058 949.058 0 0 0 90.094-222.573H573.325V307.14h245.711v-43.41l-245.71 2.458V143.33H472.173c-18.223 0-21.704 20.476-21.704 20.476v102.38H204.759v40.952h245.71v61.427H245.712v40.952h409.518a805.522 805.522 0 0 1-64.909 148.246c-128.384-42.795-266.186-77.604-354.233-55.08a213.564 213.564 0 0 0-112.003 63.27c-95.418 116.917-26.21 294.034 175.274 294.034 119.989 0 236.087-67.366 325.771-177.73 134.322 65.932 398.666 176.297 398.666 176.297V701.3s-32.352-4.095-178.96-53.033z m-563.702 144.97c-158.893 0-204.759-124.699-126.336-194.112a191.86 191.86 0 0 1 90.913-46.276c93.575-10.238 189.811 35.629 293.624 86.614-74.941 94.598-166.674 153.774-258.2 153.774z" fill="#FFFFFF"/></symbol><symbol id="ic-wechat-pay" viewBox="0 0 1076 1024"><path d="M410.493712 644.226288c-64.448471 36.97706-74.006881-20.759958-74.006881-20.759958l-80.772173-193.983933c-31.078562-92.178305 26.897497-41.56191 26.897498-41.561909s49.746372 38.732181 87.50193 62.333712c37.732946 23.602608 80.745253 6.927882 80.745254 6.927882l528.043743-250.842313C881.479874 81.578667 720.547129 0 538.352656 0 241.013636 0 0 217.098768 0 484.919453c0 154.046856 79.806318 291.154103 204.11518 380.019214L181.698086 997.56551s-10.92805 38.720336 26.945952 20.759958c25.808892-12.243853 91.603314-56.122953 130.768353-82.82771 61.570288 22.083298 128.651441 34.345455 198.970414 34.345455 297.315331 0 538.378498-217.098768 538.378499-484.924837 0-77.573115-20.313102-150.8338-56.295235-215.861568-168.236416 104.176656-559.545472 346.282128-609.973434 375.167327z" fill="#00cc22"/></symbol><symbol id="ic-emoji" viewBox="0 0 1024 1024"><path d="M32 512C32 246.90332 246.90332 32 512 32 777.09668 32 992 246.90332 992 512 992 777.09668 777.09668 992 512 992 246.90332 992 32 777.09668 32 512ZM920 512C920 286.66782219 737.33217875 104 512 104 286.66782219 104 104 286.66782219 104 512 104 737.33217875 286.66782219 920 512 920 737.33217875 920 920 737.33217875 920 512ZM667.59732781 584C661.80988156 664.88360937 594.35802594 728.70452469 512 728.70452469 429.64197219 728.70452469 362.19011938 664.88360937 356.40267313 584 401.27447563 599.63467063 454.6818125 608.70452469 512 608.70452469 569.3181875 608.70452469 622.72552156 599.63467063 667.59732781 584L667.59732781 584ZM368 440C394.50966781 440 416 418.50966781 416 392 416 365.49033219 394.50966781 344 368 344 341.49033219 344 320 365.49033219 320 392 320 418.50966781 341.49033219 440 368 440ZM656 440C682.50966594 440 704 418.50966781 704 392 704 365.49033219 682.50966594 344 656 344 629.49033406 344 608 365.49033219 608 392 608 418.50966781 629.49033406 440 656 440Z"/></symbol><symbol id="ic-dislike" viewBox="0 0 1137 1024"><path d="M771.413333 668.728889c-18.773333 3.015111-25.031111 20.878222-28.16 29.866667v217.884444c0 59.733333-49.948444 107.52-112.412444 107.52a115.427556 115.427556 0 0 1-112.412445-92.558222c-31.857778-190.919111-146.830222-263.850667-230.627555-290.133334a27.420444 27.420444 0 0 1-19.228445-26.168888V37.944889C268.572444 17.066667 285.582222 0 306.631111 0h567.864889c59.335111 11.946667 99.953778 32.824889 128 89.543111l128.113778 429.909333c24.974222 77.653333-15.644444 152.291556-106.211556 149.276445H771.413333z m-605.866666-32.824889H81.180444C37.546667 635.904 0 600.064 0 558.250667V80.611556C0 35.84 34.360889 0 81.180444 0H165.546667c29.297778 0 53.077333 23.779556 53.077333 53.077333v529.749334a53.077333 53.077333 0 0 1-53.077333 53.077333z"/></symbol><symbol id="ic-close" viewBox="0 0 1024 1024"><path d="M511.99967832 371.66626953L792.60221182 91.06373692a99.19122714 99.19122714 0 0 1 140.33340878 140.33340878L652.33308711 511.99967832l280.60253349 280.6025335a99.19122714 99.19122714 0 1 1-140.33340878 140.33340878L511.99967832 652.33308711l-280.60253262 280.60253349a99.19122714 99.19122714 0 1 1-140.33340878-140.33340878L371.66626953 511.99967832 91.06373692 231.3971457A99.19122714 99.19122714 0 1 1 231.3971457 91.06373692L511.99967832 371.66626953z"/></symbol><symbol id="ic-right" viewBox="0 0 1024 1024"><path d="M570.461091 506.693818L241.384727 177.524364A93.090909 93.090909 0 1 1 373.015273 45.893818L768 440.878545a93.090909 93.090909 0 0 1 0 131.630546l-394.984727 394.891636A93.090909 93.090909 0 0 1 241.384727 835.770182l329.076364-329.076364z"/></symbol><symbol id="ic-more" viewBox="0 0 4096 1024"><path d="M3495.04988446 991.9952a481.91759063 481.91759063 0 0 1-483.83758031-479.9976c0-265.19867437 216.71891625-479.9976 483.83758031-479.9976S3978.64746665 246.79892562 3978.64746665 511.9976c0 264.95867531-216.47891719 479.9976-483.59758219 479.9976M2065.61703196 991.9952a481.91759063 481.91759063 0 0 1-483.35758312-479.9976c0-265.19867437 216.47891719-479.9976 483.35758312-479.9976 267.11866406 0 483.59758219 214.79892563 483.59758219 479.9976 0 264.95867531-216.47891719 479.9976-483.59758219 479.9976M622.26424884 991.9952A481.91759063 481.91759063 0 0 1 138.66666665 511.9976C138.66666665 246.79892562 355.14558384 32 622.26424884 32S1105.86183102 246.79892562 1105.86183102 511.9976c0 264.95867531-216.47891719 479.9976-483.59758218 479.9976"/></symbol><symbol id="ic-toggle" viewBox="0 0 1024 1024"><path d="M350.366755 1023.926863a50.025855 50.025855 0 0 0 50.025855-50.025855V174.803795a73.137215 73.137215 0 0 0-124.47954-52.146834L36.096141 359.109577a50.830365 50.830365 0 0 0-0.731372 71.674471 50.537816 50.537816 0 0 0 71.601334 0.877647l193.301659-189.425388v731.737838c0 27.645867 22.453125 50.025855 50.025855 50.025855z"/><path d="M720.953024 0.005851a47.246641 47.246641 0 0 0-47.246641 47.246641v799.389761a73.137215 73.137215 0 0 0 124.9915 51.634874l190.522446-191.619503a51.415462 51.415462 0 0 0 1.31647-71.089374 45.783897 45.783897 0 0 0-67.066827-0.292548l-155.343445 165.802066V47.252492A47.246641 47.246641 0 0 0 720.953024 0.005851z"/></symbol><symbol id="ic-notebook" viewBox="0 0 1024 1024"><path d="M178.390055 120.591045C111.268624 120.591045 56.888889 174.401955 56.888889 240.556383V903.97778C56.888889 970.302855 111.097977 1024 178.390055 1024h545.731364c67.121431 0 121.558049-53.81091 121.558049-120.02222V240.613265c0-66.268192-54.209088-120.02222-121.558049-120.02222H178.390055z m455.117432 301.136319H269.06087a30.147761 30.147761 0 0 1 0-60.238641h364.503499a30.147761 30.147761 0 0 1 0 60.238641z m303.18409 301.136318a30.318409 30.318409 0 0 1-30.375291-30.318409V180.317742c0-66.268192-53.81091-120.02222-121.330519-120.022219H329.697688a30.147761 30.147761 0 0 1 0-60.23864l454.946784 0.056882C885.326618 0.113765 967.009987 80.887013 967.009987 180.602155v511.943118a30.318409 30.318409 0 0 1-30.31841 30.318409z m-303.18409-120.47728H269.06087a30.147761 30.147761 0 1 1 0-60.238641h364.503499a30.147761 30.147761 0 0 1 0 60.238641z"/></symbol><symbol id="ic-check" viewBox="0 0 1433 1024"><path d="M586.88355555 719.072L1197.82755555 108.128a96 96 0 0 1 135.744 135.744L654.75555555 922.688a96 96 0 0 1-135.744 0L111.68355555 515.456A96 96 0 0 1 247.42755555 379.52l339.456 339.456z"/></symbol><symbol id="ic-plus" viewBox="0 0 1024 1024"><path d="M437.00000029 437.00000029V136.99999971a74.99999971 74.99999971 0 1 1 149.99999942 0v300.00000058h300.00000058a74.99999971 74.99999971 0 0 1 0 149.99999942H586.99999971v300.00000058a74.99999971 74.99999971 0 0 1-149.99999942 0V586.99999971H136.99999971a74.99999971 74.99999971 0 1 1 0-149.99999942h300.00000058z"/></symbol><symbol id="ic-pencil" viewBox="0 0 1092 1024"><path d="M597.469867 198.724267L81.92 718.574933 0 1024l309.4528-74.410667 512-521.762133-223.982933-229.102933z m400.861866-37.888L858.5216 17.885867a58.845867 58.845867 0 0 0-84.309333 0L657.2032 137.6256l223.914667 229.1712 117.1456-119.808a61.576533 61.576533 0 0 0 0-86.152533h0.068266zM546.133333 930.884267h546.133334V1024H546.133333v-93.115733z m273.066667-186.1632h273.066667v93.115733h-273.066667v-93.115733z"/></symbol><symbol id="ic-sugar" viewBox="0 0 1024 1024"><path d="M562.362182 689.338182L352.581818 422.632727a182.737455 182.737455 0 0 1 109.056-85.876363l209.780364 266.612363a182.737455 182.737455 0 0 1-109.056 85.876364z m-28.811637 5.725091a183.296 183.296 0 0 1-193.349818-245.76l193.349818 245.76z m150.248728-118.225455l-193.349818-245.76a183.249455 183.249455 0 0 1 193.349818 245.76zM558.545455 282.065455l-40.215273-150.109091 151.738182-40.680728-13.544728-50.548363-202.333091 54.178909L503.156364 277.550545a235.659636 235.659636 0 0 0-199.493819 345.553455L120.925091 672.069818l13.544727 50.548364 200.704-53.76A236.683636 236.683636 0 0 0 465.454545 744.075636l40.215273 150.155637-151.738182 40.634182 13.544728 50.594909 202.333091-54.225455-48.965819-182.690909a235.613091 235.613091 0 0 0 199.493819-345.553455l182.690909-48.919272-13.544728-50.594909-200.704 53.76A236.683636 236.683636 0 0 0 558.545455 282.065455z m126.882909-32.256l126.464-33.885091-13.544728-50.594909-101.189818 27.136-27.089454-101.189819-50.594909 13.544728 40.680727 151.738181 25.274182-6.74909zM338.618182 776.378182l-126.464 33.885091 13.544727 50.548363 101.189818-27.089454 27.089455 101.143273 50.594909-13.498182-40.680727-151.738182-25.274182 6.749091z m-153.6-67.304727l-50.548364 13.544727 40.680727 151.738182 50.548364-13.498182-40.634182-151.738182z m653.963636-392.052364l50.548364-13.544727-40.680727-151.738182-50.548364 13.544727 40.634182 151.738182z"/></symbol><symbol id="ic-like" viewBox="0 0 1084 1024"><path d="M728.064 343.943529c-17.648941-2.891294-23.552-20.239059-26.503529-28.912941V104.026353C701.560471 46.200471 654.396235 0 595.425882 0c-53.007059 0-97.28 40.478118-106.134588 89.569882-29.997176 184.862118-138.541176 255.457882-217.630118 280.937412a26.142118 26.142118 0 0 0-18.130823 24.877177v560.067764c0 19.817412 16.022588 35.84 35.84 35.84h535.973647c56.018824-11.565176 94.328471-31.804235 120.892235-86.738823l120.832-416.105412c23.552-75.173647-14.757647-147.395765-100.231529-144.564706h-238.772706z m-571.813647 31.744H76.619294C35.358118 375.687529 0 410.383059 0 450.861176v462.426353c0 43.369412 32.406588 78.004706 76.619294 78.004706h79.631059c27.708235 0 50.115765-22.407529 50.115765-50.115764V425.863529a50.115765 50.115765 0 0 0-50.115765-50.115764z"/></symbol><symbol id="ic-reply" viewBox="0 0 1092 1024"><path d="M173.24799969 781.568C95.168 781.568 32 723.07200031 32 650.94399969V162.56C32 90.49599969 95.04000031 32 172.99200031 32h742.01599969C992.96 32 1056.00000031 90.49599969 1056.00000031 162.62400031v488.31999938c0 72.06400031-63.36 130.62400031-141.12 130.62400031h-343.68l-287.55200062 196.224a12.79999969 12.79999969 0 0 1-19.968-10.87999969l5.63200031-185.34400031H173.24799969z"/></symbol><symbol id="ic-shang" viewBox="0 0 1024 1024"><path d="M827.512471 177.88486233c35.056941 0 61.982118 8.914824 79.028705 26.684236 17.950118 17.769412 26.985412 44.453647 26.985412 80.052706 0 17.769412-3.614118 43.610353-10.842353 77.402353-3.553882 17.769412-14.336 65.776941-50.236235 65.776941a48.549647 48.549647 0 0 1-35.056941-15.058824 39.273412 39.273412 0 0 1-3.614118-48.971294c9.938824-30.238118 15.299765-56.922353 15.299765-77.402353 0-26.684235-7.228235-26.684235-13.492706-26.684235H189.861647c-0.843294 0-1.807059 0-3.553882 1.807059-1.807059 1.807059-1.807059 2.650353-1.807059 4.457411v108.483765c0 17.829647-11.685647 39.152941-43.971765 39.152941-31.442824 0-43.128471-21.323294-43.12847-39.152941v-110.230588c0-29.394824 8.071529-50.718118 25.118117-65.837177 16.203294-14.275765 38.610824-21.383529 70.053647-21.383529h54.814118a267.685647 267.685647 0 0 0-27.828706-33.731765c-19.757176-22.287059-14.396235-40.96-9.878588-50.778353 8.071529-15.058824 21.504-23.973647 36.743529-23.973647a43.369412 43.369412 0 0 1 30.59953 13.312c3.614118 3.614118 8.071529 8.914824 13.43247 16.022589 5.421176 7.107765 11.685647 16.022588 18.913883 26.684235 11.625412 17.769412 22.407529 35.538824 32.286117 53.36847h128.421647V75.60533333c0-17.769412 10.842353-39.152941 43.128471-39.152941 32.346353 0 43.971765 21.383529 43.971765 39.152941v102.279529h136.553411c7.168-10.661647 15.239529-21.323294 22.467765-32.88847 9.818353-15.119059 19.696941-32.045176 29.635765-48.971294a45.537882 45.537882 0 0 1 22.407529-23.973647 40.357647 40.357647 0 0 1 30.539294-0.90353c7.228235 2.650353 13.492706 7.107765 18.853647 12.468706a35.659294 35.659294 0 0 1 11.685647 25.780706c0 11.565176-4.457412 27.587765-34.093176 68.487529h32.286118z m-532.540236 341.534118a46.622118 46.622118 0 0 1-30.539294-16.022588 57.825882 57.825882 0 0 1-16.143059-31.081412 169.381647 169.381647 0 0 1-3.614117-40.056471V388.76862733c0-15.119059 1.807059-27.587765 3.614117-37.345882 2.650353-12.468706 8.071529-22.287059 15.23953-30.298353 7.228235-8.854588 17.950118-14.215529 31.442823-15.962353 9.878588-0.903529 19.757176-1.807059 32.346353-1.807059h367.314824c30.539294 0 52.043294 8.011294 65.536 23.130353 11.685647 14.215529 17.950118 35.538824 17.950117 63.126588v46.260706c0 29.394824-6.264471 50.718118-20.660705 64.933647-13.432471 13.372235-34.093176 20.48-62.825412 20.48H326.415059c-12.589176 0-22.467765 0-31.442824-1.807059z m32.346353-129.867294h-0.903529v43.610353c0 5.360941 0.903529 7.107765 1.807059 8.011294h366.411294c-0.903529 0 0-2.650353 0-7.107765v-49.814588H327.318588v5.300706z m577.41553 527.480471c15.299765 11.565176 23.371294 24.877176 19.757176 39.152941 0 16.865882-9.878588 31.984941-26.925176 38.189176a36.743529 36.743529 0 0 1-17.046589 3.614118 64.451765 64.451765 0 0 1-31.442823-9.818353 2255.149176 2255.149176 0 0 0-139.14353-74.691765 2297.976471 2297.976471 0 0 0-155.407058-71.198117c-8.071529-4.397176-33.249882-15.058824-33.249883-39.152942 0-0.843294 0.903529-1.746824 0.90353-3.493647-5.421176 13.312-12.589176 26.684235-19.757177 39.152941-30.539294 53.308235-91.557647 93.364706-180.525176 119.145412-84.389647 24.937412-144.564706 37.345882-184.982588 37.345883-34.153412 0-45.778824-22.226824-45.778824-41.803295 0-8.854588 3.614118-36.442353 44.875294-44.453647 86.196706-11.565176 152.696471-26.684235 199.378824-43.610353 43.971765-16.865882 76.318118-39.152941 96.075294-66.68047 19.757176-27.587765 30.539294-71.137882 31.442823-129.867294 0-19.576471 9.035294-33.792 24.274824-39.996236H272.504471c-6.264471 0-8.975059 0-8.975059 14.21553v176.128c0 24.877176-17.046588 41.803294-43.068236 41.803294-25.178353 0-42.224941-16.022588-42.224941-41.803294v-184.139294c0-29.334588 8.071529-51.621647 23.311059-66.740706 15.299765-15.058824 37.767529-22.226824 68.306824-22.226824h489.411764c33.249882 0 55.717647 7.107765 70.053647 21.38353 13.492706 14.215529 20.660706 36.442353 20.660706 67.584v180.525176c0 24.937412-17.046588 41.803294-43.12847 41.803294-25.118118 0-42.164706-15.962353-42.164706-41.803294v-170.767059a31.503059 31.503059 0 0 0-1.807059-13.312c-1.807059-0.903529-3.614118-0.903529-7.228235-0.903529H524.890353c15.239529 6.204235 25.118118 20.48 25.118118 40.056471 0 37.345882-6.264471 72.884706-17.950118 107.580235 0.903529-0.843294 0.903529-1.807059 1.807059-2.650353 4.457412-8.914824 16.143059-15.119059 29.635764-15.119059 6.264471 0 15.239529 1.807059 54.814118 18.672941l98.785882 46.260706c20.600471 9.758118 40.357647 19.576471 60.114824 28.431059 20.660706 8.914824 40.417882 18.672941 59.271529 28.491294 19.757176 9.758118 35.056941 17.769412 45.778824 23.130353 9.938824 5.300706 16.263529 8.914824 18.913882 9.758118l3.614118 1.807059z"/></symbol><symbol id="ic-diamond" viewBox="0 0 1026 1024"><path d="M751.144277 307.2l-123.016533-238.933333h159.778133a81.92 81.92 0 0 1 59.1872 25.258666l160.256 167.492267A27.306667 27.306667 0 0 1 987.620011 307.2h-236.475734z m270.506667 111.547733L640.927744 946.039467a27.306667 27.306667 0 0 1-48.128-24.234667L766.504277 375.466667h-56.388266l-170.5984 590.165333a27.306667 27.306667 0 0 1-52.462934 0.034133L315.500544 375.466667H259.112277l174.523734 545.5872a27.306667 27.306667 0 0 1-48.128 24.302933L5.160277 418.747733A27.306667 27.306667 0 0 1 27.346944 375.466667H999.464277a27.306667 27.306667 0 0 1 22.152534 43.281066zM18.301611 261.0176L178.557611 93.525333A81.92 81.92 0 0 1 237.744811 68.266667h159.744L274.506411 307.2H38.030677a27.306667 27.306667 0 0 1-19.729066-46.1824zM453.877077 68.266667h117.896534l122.9824 238.933333H330.894677l122.9824-238.933333z"/></symbol><symbol id="ic-money" viewBox="0 0 1024 1024"><path d="M884.363636 512C884.363636 306.349242 717.650758 139.636364 512 139.636364 306.349242 139.636364 139.636364 306.349242 139.636364 512 139.636364 717.650758 306.349242 884.363636 512 884.363636 717.650758 884.363636 884.363636 717.650758 884.363636 512ZM46.545455 512C46.545455 254.936553 254.936553 46.545455 512 46.545455 769.063447 46.545455 977.454545 254.936553 977.454545 512 977.454545 769.063447 769.063447 977.454545 512 977.454545 254.936553 977.454545 46.545455 769.063447 46.545455 512ZM470.626262 520.334527 346.368469 520.334527C335.022727 520.334527 325.818182 511.082366 325.818182 499.669243L325.818182 478.939206C325.818182 467.431177 335.018859 458.273923 346.368469 458.273923L459.41918 458.273923 362.752701 343.071299C355.558764 334.497899 356.572921 321.312715 365.315879 313.976502L381.196011 300.65149C390.011671 293.254272 402.96653 294.447111 410.293884 303.179511L512.303444 424.749773 614.313007 303.179511C621.64036 294.447111 634.595221 293.254272 643.410879 300.65149L659.29101 313.976502C668.033969 321.312715 669.048129 334.497899 661.854189 343.071299L565.18771 458.273923 677.63153 458.273923C688.977273 458.273923 698.181818 467.526083 698.181818 478.939206L698.181818 499.669243C698.181818 511.177272 688.981141 520.334527 677.63153 520.334527L553.373738 520.334527 553.373738 582.395136 677.63153 582.395136C688.977273 582.395136 698.181818 591.647297 698.181818 603.060419L698.181818 623.790457C698.181818 635.298486 688.981141 644.455741 677.63153 644.455741L553.373738 644.455741 553.373738 737.562275C553.373738 748.871415 544.025139 758.175739 532.493056 758.175739L491.506944 758.175739C479.797853 758.175739 470.626262 748.946776 470.626262 737.562275L470.626262 644.455741 346.368469 644.455741C335.022727 644.455741 325.818182 635.203579 325.818182 623.790457L325.818182 603.060419C325.818182 591.552391 335.018859 582.395136 346.368469 582.395136L470.626262 582.395136 470.626262 520.334527Z"/></symbol><symbol id="ic-others" viewBox="0 0 1024 1024"><path d="M232.727273 579.87878833C271.28679 579.87878833 302.545455 548.62012233 302.545455 510.06060633 302.545455 471.50108933 271.28679 440.24242433 232.727273 440.24242433 194.167756 440.24242433 162.909091 471.50108933 162.909091 510.06060633 162.909091 548.62012233 194.167756 579.87878833 232.727273 579.87878833ZM512 579.87878833C550.559516 579.87878833 581.818182 548.62012233 581.818182 510.06060633 581.818182 471.50108933 550.559516 440.24242433 512 440.24242433 473.440484 440.24242433 442.181818 471.50108933 442.181818 510.06060633 442.181818 548.62012233 473.440484 579.87878833 512 579.87878833ZM791.272727 579.87878833C829.832243 579.87878833 861.090909 548.62012233 861.090909 510.06060633 861.090909 471.50108933 829.832243 440.24242433 791.272727 440.24242433 752.713211 440.24242433 721.454545 471.50108933 721.454545 510.06060633 721.454545 548.62012233 752.713211 579.87878833 791.272727 579.87878833Z"/></symbol><symbol id="ic-requests" viewBox="0 0 1024 1024"><path d="M418.909091 372.363636 418.909091 698.181818 605.090909 698.181818 605.090909 372.363636 696.24367 372.363638 511.582813 151.061383 326.921959 372.363638 418.909091 372.363636ZM325.818182 791.272727 325.818182 465.454545 139.636364 465.454545 512 10.219745 884.363636 465.454545 698.181818 465.454545 698.181818 791.272727 325.818182 791.272727ZM791.549193 930.909091C842.809195 930.909091 884.363636 889.589313 884.363636 837.818182L884.363636 744.727273 977.454545 744.727273 977.454545 884.363636C977.454545 961.482668 914.929478 1024 838.088048 1024L185.911951 1024C108.94196 1024 46.545455 961.32575 46.545455 884.363636L46.545455 744.727273 139.636364 744.727273 139.636364 837.818182C139.636364 889.230871 181.271913 930.909091 232.450806 930.909091L791.549193 930.909091Z"/></symbol><symbol id="ic-follows" viewBox="0 0 1024 1024"><path d="M742.39037 893.84038533C732.980703 903.25005233 718.025272 903.19522133 708.767223 893.93717233L565.458115 753.71907433C556.207048 744.46800633 556.2434 729.40742933 565.554902 720.09592233L605.034543 682.83743133C614.444209 673.42776433 629.399645 673.48259533 638.657694 682.74064433L728.38723 766.49557833 941.491782 540.05939833C950.503205 531.04797933 965.609342 531.03876833 974.920848 540.35027433L1010.521307 576.49106033C1019.930973 585.90072733 1019.963043 600.76926333 1010.812183 609.92012233L742.39037 893.84038533ZM139.636364 859.15151533C139.636364 859.15151533 140.67734 830.50210333 143.721266 818.93653533 149.029303 798.76835333 158.415813 778.50853133 172.872966 759.57276033 185.195819 743.43246833 200.763606 728.89474933 220.104544 716.30744833 248.816792 697.62118933 341.214879 652.42733033 332.078782 657.40617733 390.138425 625.76575033 412.212292 582.79679733 388.559923 518.84814133 384.958515 509.11105133 363.507092 452.78421333 358.209139 437.93037933 342.817466 394.77685333 333.272371 357.97333033 324.783144 304.62989533 312.248334 225.86538933 369.770254 160.96969733 466.26749 160.96969733 571.044971 160.96969733 629.910798 218.91014833 619.986167 306.71758733 613.807085 361.38656133 604.889014 392.41438433 587.2357 428.78513033 587.680116 427.86951433 552.225592 496.14089933 539.29385 524.72497933L624.108763 563.09615833C635.7853 537.28656933 670.418623 470.59648633 670.983024 469.43366333 693.277635 423.50053933 705.13172 382.25771733 712.48809 317.17282433 728.985321 171.21479033 623.998925 67.87878833 466.26749 67.87878833 313.335459 67.87878833 211.18413 183.12496033 232.849136 319.26053733 242.258212 378.38399033 253.244797 420.74554033 270.528472 469.20365633 276.225846 485.17734633 297.956064 542.23624033 301.249676 551.14114433 307.065226 566.86458533 308.380666 564.30394833 287.532892 575.66524233 282.015993 578.67175233 275.594213 581.88642833 265.97112 586.50462633 270.601051 584.28268633 245.145458 596.43114733 238.070951 599.89143433 212.184458 612.55306833 190.997437 624.18127233 169.326586 638.28490833 140.984774 656.73008533 117.593202 678.57391433 98.881863 703.08175233 60.377931 753.51364533 47.027784 804.23828133 46.549412 840.92027533L46.545455 947.98348733 587.235705 947.98345933 512 859.15151533 139.636364 859.15151533Z"/></symbol><symbol id="ic-chats" viewBox="0 0 1024 1024"><path d="M124.121212 139.636364C88.436364 139.636364 47.010909 181.527273 46.545455 217.212121L46.545455 799.030305C46.545455 842.472727 88.436364 884.363636 124.121212 876.606059L899.878786 876.606059C935.563636 884.363636 977.454545 842.472727 977.454545 799.030305L977.454545 217.212121C977.454545 181.527273 935.563636 139.636364 899.878786 139.636364L124.121212 139.636364ZM512 473.016869 139.636364 232.727273 884.363636 232.727273 512 473.016869ZM139.636364 791.272727 139.636364 331.612007 512 578.515503 884.363636 331.612007 884.363636 791.272727 139.636364 791.272727Z"/></symbol><symbol id="ic-comments" viewBox="0 0 1024 1024"><path d="M977.454545 164.91403167C977.454545 113.60050467 935.731963 71.75757567 884.264546 71.75757567L139.735452 71.75757567C88.181004 71.75757567 46.545455 113.46514167 46.545455 164.91403167L46.545455 676.78293967C46.545455 728.09646367 88.22568 769.93939367 139.640844 769.93939367L186.181818 769.93939367 186.181818 956.12121167 512 769.93939367 884.524167 769.93939367C935.858506 769.93939367 977.454545 728.23182767 977.454545 676.78293967L977.454545 164.91403167ZM884.363636 164.84848467L884.363636 676.84848467 502.393986 676.84848467 279.272727 769.93939367 279.272727 676.84848467 139.636364 676.84848467 139.636364 164.84848467 884.363636 164.84848467Z"/></symbol><symbol id="ic-likes" viewBox="0 0 1024 1024"><path d="M511.646501 852.318427C513.3925 850.741015 516.884503 847.586202 516.884503 847.586202 738.668074 646.043071 808.239081 574.380446 853.64177 489.88787 874.584837 450.913673 884.363636 415.390578 884.363636 379.345455 884.363636 287.398144 813.401856 216.436364 721.454545 216.436364 669.217853 216.436364 616.89613 241.028421 582.874945 280.979901L512 364.209197 441.125053 280.979901C407.103871 241.028421 354.782148 216.436364 302.545455 216.436364 210.598144 216.436364 139.636364 287.398144 139.636364 379.345455 139.636364 415.547805 149.501383 451.227391 170.635978 490.39044 216.182926 574.790321 286.220326 646.813794 507.042118 847.054141 507.042118 847.054141 490.96233 871.005342 511.646501 852.318427ZM512 220.625455 578.083025 164.351628C620.609936 138.33686 670.384463 123.345455 721.454545 123.345455 864.814545 123.345455 977.454545 235.985455 977.454545 379.345455 977.454545 555.287273 819.2 698.647273 579.490909 916.48L512 977.454545 444.509091 916.014545C204.8 698.647273 46.545455 555.287273 46.545455 379.345455 46.545455 235.985455 159.185455 123.345455 302.545455 123.345455 353.615536 123.345455 403.390064 138.33686 445.916975 164.351628L512 220.625455Z"/></symbol><symbol id="ic-nav-mode" viewBox="0 0 1024 1024"><path d="M194.56 597.333333l-64.853333 166.4c-2.858667 9.088-3.413333 15.786667-1.706667 20.053334 1.706667 4.266667 10.24 6.954667 25.6 8.106666l56.32 4.266667v24.746667a521.557333 521.557333 0 0 0-34.56-1.28c-17.365333-0.298667-40.832-0.426667-70.4-0.426667s-53.034667 0.128-70.4 0.426667c-17.365333 0.298667-28.885333 0.725333-34.56 1.28v-24.746667l53.76-4.266667c18.773333-1.706667 31.872-12.245333 39.253333-31.573333L292.693333 256h41.813334l188.586666 498.346667c3.968 9.685333 7.68 16.213333 11.093334 19.626666 3.413333 3.413333 9.685333 6.528 18.773333 9.386667l46.933333 14.506667v23.893333a1946.538667 1946.538667 0 0 0-99.84-2.56h-71.68c-45.525333 0-82.773333 0.853333-111.786666 2.56v-23.893333l53.76-8.533334c16.512-2.858667 25.898667-6.272 28.16-10.24 2.261333-3.968 1.152-11.648-3.413334-23.04L336.213333 597.333333H194.56z m127.146667-39.253333L271.36 398.506667l-61.44 159.573333h111.786667z m427.52 273.92c-23.338667 0-43.392-4.138667-60.16-12.373333a110.805333 110.805333 0 0 1-40.96-33.706667 146.816 146.816 0 0 1-23.466667-50.346667c-5.12-19.328-7.68-40.106667-7.68-62.293333 0-30.165333 4.565333-56.874667 13.653333-80.213333 9.088-23.338667 21.333333-42.965333 36.693334-58.88a153.6 153.6 0 0 1 53.76-36.266667 171.946667 171.946667 0 0 1 64.853333-12.373333c15.914667 0 29.866667 1.152 41.813333 3.413333 11.946667 2.261333 21.888 4.821333 29.866667 7.68 9.685333 2.858667 17.621333 6.272 23.893333 10.24l56.32-31.573333 7.68 2.56v262.826666c0 22.741333 9.685333 33.578667 29.013334 32.426667 4.565333 0 8.106667-0.725333 10.666666-2.133333s4.992-2.688 7.253334-3.84c2.261333-1.706667 4.266667-3.413333 5.973333-5.12l5.12 11.946666c-4.565333 10.794667-10.538667 20.48-17.92 29.013334-6.272 7.381333-14.634667 14.08-25.173333 20.053333-10.538667 5.973333-23.466667 8.96-38.826667 8.96-25.045333 0-42.794667-7.552-53.333333-22.613333-10.538667-15.061333-15.786667-33.408-15.786667-55.04v-20.48a181.888 181.888 0 0 1-12.373333 36.266666 129.92 129.92 0 0 1-20.053334 31.146667 92.373333 92.373333 0 0 1-29.44 22.186667 93.44 93.44 0 0 1-41.386666 8.533333z m103.253333-196.266667c0-11.946667-0.554667-24.448-1.706667-37.546666a143.872 143.872 0 0 0-7.68-36.266667 66.858667 66.858667 0 0 0-18.346666-27.733333c-8.234667-7.381333-19.498667-11.093333-33.706667-11.093334-7.978667 0-16.341333 2.133333-25.173333 6.4-8.832 4.266667-16.938667 11.221333-24.32 20.906667-7.381333 9.685333-13.525333 22.912-18.346667 39.68-4.821333 16.768-7.253333 37.418667-7.253333 61.866667 0 17.066667 1.28 33.28 3.84 48.64 2.56 15.36 6.698667 28.714667 12.373333 40.106666 5.674667 11.392 12.672 20.352 20.906667 26.88 8.234667 6.528 18.048 9.813333 29.44 9.813334 11.392 0 21.333333-3.712 29.866666-11.093334 8.533333-7.381333 15.786667-17.194667 21.76-29.44 5.973333-12.245333 10.538667-26.325333 13.653334-42.24 3.114667-15.914667 4.693333-32.426667 4.693333-49.493333v-9.386667z"/></symbol><symbol id="ic-write" viewBox="0 0 1024 1024"><path d="M151.007 942.26766666c-22.993 21.261-64.594 64.683-43.381-35.512 37.218-163.524 447.875-792.517 794.003-852.861 97.053 0-180.459 232.311-180.459 232.311 0 0 95.338 11.846 157.962-54.296-37.533 195.818-214.91 195.149-260.719 210.729 53.59 7.89 93.441 31.594 176.379 7.368-20.222 55.519-100.075 123.839-387.175 191.454-126.92 45.617-233.617 279.547-256.61 300.808z"/></symbol><symbol id="ic-night" viewBox="0 0 1024 1024"><path d="M386.573124 0C340.147815 173.910685 384.73085 366.612567 521.059141 502.940859 657.387432 639.26915 850.089314 683.852184 1024 637.426876 1001.155802 722.908399 956.572765 804.336916 889.51398 871.395701 686.126909 1074.782772 355.991373 1074.782772 152.6043 871.395701-50.782772 668.00863-50.782772 337.87309 152.6043 134.486017 219.663081 67.427236 301.091602 22.8442 386.573124 0Z"/></symbol><symbol id="ic-mark" viewBox="0 0 1024 1024"><path d="M268.190476 113.777778C214.552381 113.777778 171.154286 155.291291 171.154286 206.03003L170.666667 967.111111 512 828.732735 853.333333 967.111111 853.333333 206.03003C853.333333 155.291291 809.447617 113.777778 755.809525 113.777778L268.190476 113.777778Z"/></symbol><symbol id="ic-user" viewBox="0 0 1081 1024"><path d="M803.557507 705.46529267C921.627438 734.68811367 1009.013675 841.17086467 1009.013675 968.09948167L1009.013675 999.65574867 113.777778 999.65574867 113.777778 968.09948167C113.777778 845.98765767 194.87339 742.62628167 306.138642 709.26557267 386.042902 689.94662967 446.772745 624.07875167 455.097973 555.06783567 413.092784 526.09210167 380.671395 482.61667367 369.546671 434.01441967L331.247877 266.69273867C301.999138 138.90939267 384.447284 35.55555567 515.334601 35.55555567L604.020878 35.55555567C735.179184 35.55555567 819.313243 139.39951167 792.544825 267.49752267L757.903491 433.27071867C747.227432 484.36012067 713.120956 529.80140967 668.826761 558.82664367 675.211167 624.51970267 729.457687 683.91321067 803.557507 705.46529267Z"/></symbol><symbol id="ic-setting" viewBox="0 0 1024 1024"><path d="M846.327484 515.083444C846.327484 499.504829 844.9529 484.842604 843.120122 470.180378L939.799171 394.578276C948.504866 387.705358 950.795839 375.334105 945.297505 365.253825L853.658596 206.718508C848.160262 196.638228 835.78901 192.972672 825.708726 196.638228L711.618281 242.457684C687.792168 224.129902 662.133271 209.009481 634.183401 197.554617L616.772011 76.13306C615.397427 65.136391 605.775343 56.888889 594.320475 56.888889L411.042655 56.888889C399.587791 56.888889 389.965705 65.136391 388.591122 76.13306L371.179729 197.554617C343.22986 209.009481 317.570966 224.588096 293.744849 242.457684L179.654404 196.638228C169.11593 192.514478 157.202871 196.638228 151.704536 206.718508L60.065625 365.253825C54.109096 375.334105 56.858263 387.705358 65.56396 394.578276L162.243011 470.180378C160.410233 484.842604 159.035649 499.963024 159.035649 515.083444 159.035649 530.203865 160.410233 545.324285 162.243011 559.986511L65.56396 635.588614C56.858263 642.461531 54.567291 654.832782 60.065625 664.913067L151.704536 823.448383C157.202871 833.528661 169.574124 837.194217 179.654404 833.528661L293.744849 787.709207C317.570966 806.036986 343.22986 821.15741 371.179729 832.612272L388.591131 947.866943C389.965715 958.86361 399.587801 967.111111 411.042665 967.111111L594.320486 967.111111C605.775349 967.111111 615.397439 958.86361 616.772022 947.866943L634.183401 832.612272C662.133271 821.15741 687.792168 805.578792 711.618281 787.709207L825.708726 833.528661C836.247205 837.652412 848.160262 833.528661 853.658596 823.448383L945.297505 664.913067C950.795839 654.832782 948.504866 642.461531 939.799171 635.588614L843.120122 559.986511C844.9529 545.324285 846.327484 530.662059 846.327484 515.083444ZM502.681566 675.45154C414.250017 675.45154 342.313471 603.514994 342.313471 515.083444 342.313471 426.651895 414.250017 354.71535 502.681566 354.71535 591.113114 354.71535 663.04966 426.651895 663.04966 515.083444 663.04966 603.514994 591.113114 675.45154 502.681566 675.45154L502.681566 675.45154Z"/></symbol><symbol id="ic-wallet" viewBox="0 0 1081 1024"><path d="M78.590372 320.640733C79.612385 263.784132 108.255224 220.849417 161.254046 208.613675L811.774629 58.429164C843.73446 51.050656 875.603143 70.885317 882.986092 102.864395L930.36408 264.731771C930.36408 264.731771 432.674221 267.670638 193.893052 264.731771 191.508185 264.702418 137.002048 273.909722 137.002047 320.464627L888.060649 320.640726C942.735462 320.640726 988.818119 385.402493 988.818119 439.966797L988.818119 835.991046C988.818119 890.695509 928.709194 947.609862 874.118087 947.609862L193.893052 947.609862C139.218237 947.609862 78.590372 890.555352 78.590372 835.991046L78.590372 320.640733ZM870.350433 600.907349C870.350433 562.701243 839.378261 531.729073 801.172156 531.729073 762.96605 531.729073 731.993879 562.701243 731.993879 600.907349 731.993879 639.113461 762.96605 670.085626 801.172156 670.085626 839.378261 670.085626 870.350433 639.113461 870.350433 600.907349Z"/></symbol><symbol id="ic-like-filled" viewBox="0 0 1024 1024"><path d="M455.286273 907.310814C215.315919 689.706627 56.888889 546.190377 56.888889 370.056797 56.888889 226.540546 169.651657 113.777778 313.167908 113.777778 394.24527 113.777778 472.0609 151.520688 522.850741 211.163805 573.640585 151.520688 651.45621 113.777778 732.533578 113.777778 876.049829 113.777778 988.812595 226.540546 988.812595 370.056797 988.812595 546.190377 830.385567 689.706627 590.415212 907.776774L522.850741 968.817778 455.286273 907.310814Z"/></symbol><symbol id="ic-feedback" viewBox="0 0 1024 1024"><path d="M158.037561 937.474543C158.037561 937.474543 428.526853 971.883105 479.47676 768.097638L795.921232 768.097638C859.047925 768.097638 910.222222 716.754256 910.222222 654.123816L910.222222 227.7516C910.222222 164.805596 859.18947 113.777778 796.11115 113.777778L227.888848 113.777778C164.867044 113.777778 113.777778 165.12116 113.777778 227.7516L113.777778 654.123816C113.777778 717.069824 164.241867 768.097638 227.072055 768.097638L285.060607 768.097638C254.825383 906.665296 158.037561 937.474543 158.037561 937.474543ZM284.444444 512 625.777778 512 625.777778 597.333333 284.444444 597.333333 284.444444 512 284.444444 512ZM284.473374 312.888889 739.584484 312.888889 739.584484 398.222222 284.473374 398.222222 284.473374 312.888889 284.473374 312.888889Z"/></symbol><symbol id="ic-signout" viewBox="0 0 1024 1024"><path d="M376.88888867 568.888889L746.66666667 568.888889 632.88888867 682.666667 703.99999967 755.238935 945.77777767 540.444444 945.77777767 483.555556 703.99999967 270.947385 632.88888867 341.333333 746.66666667 455.111111 376.88888867 455.111111 376.88888867 568.888889ZM489.07269967 113.777778L489.07269967 227.888848 206.20427767 227.888848 206.20427767 796.111167 489.07269967 796.111167 489.07269967 910.222222 206.20427967 910.222222C143.37645767 910.222222 92.44444467 859.18947 92.44444467 796.11115L92.44444467 227.888848C92.44444467 164.867044 143.37274867 113.777778 206.20427967 113.777778L489.07269967 113.777778Z"/></symbol><symbol id="ic-nav-discover" viewBox="0 0 1024 1024"><path d="M13.3565216 512C13.3565216 236.60633067 236.60633067 13.3565216 512 13.3565216S1010.6434784 236.60633067 1010.6434784 512 787.39366933 1010.6434784 512 1010.6434784 13.3565216 787.39366933 13.3565216 512z m926.05217387 0a427.40869547 427.40869547 0 1 0-854.81739094 0 427.40869547 427.40869547 0 0 0 854.81739094 0z m-321.3638496-253.548336c120.29180267-82.53736853 173.6704-39.4165792 119.4369856 95.9295072l-104.9525792 261.6691008-217.9784352 148.8332064c-120.434272 82.1574496-175.90242347 37.70694507-124.13848107-98.6364288l93.93493333-247.42214507 233.65008747-160.37324053zM512 583.23478293a71.23478293 71.23478293 0 1 0 0-142.46956586 71.23478293 71.23478293 0 0 0 0 142.46956586z"/></symbol><symbol id="ic-nav-notification" viewBox="0 0 1024 1024"><path d="M513.024 1001.828174A111.88313 111.88313 0 0 0 625.39687 890.434783H400.695652a111.88313 111.88313 0 0 0 112.328348 111.393391z m-320.823652-489.293913v5.030956c0 43.408696-13.445565 84.591304-36.062609 122.301218-8.013913 13.312-16.562087 25.154783-25.065739 35.350261-4.897391 5.921391-8.592696 9.794783-10.329043 11.486608l-3.250087 3.650783C57.433043 768.934957 96.478609 845.913043 196.118261 845.913043h634.434782c99.906783 0 135.43513-75.063652 74.306783-153.11026l-1.736348-2.181566-2.048-1.869913a180.535652 180.535652 0 0 1-11.53113-11.976347 298.206609 298.206609 0 0 1-27.603478-36.151653c-24.709565-38.021565-39.357217-79.070609-39.357218-121.366261 0-2.31513 0-4.630261 0.089044-6.945391l-0.044522-76.755478c0-132.096-84.591304-258.626783-189.350957-283.202783C624.951652 91.447652 571.65913 44.521739 507.458783 44.521739 437.426087 44.521739 395.931826 92.16 382.085565 151.863652 268.332522 200.214261 192.333913 310.761739 192.333913 435.556174v63.933217c-0.133565 3.116522-0.133565 6.233043-0.089043 13.089392z m65.80313-72.525913c0-105.427478 67.31687-198.210783 166.199652-232.848696l21.99374-7.657739v-29.028174c0-33.124174 27.38087-60.237913 61.261913-60.237913 33.836522 0 61.217391 27.113739 61.217391 60.237913v29.028174l21.993739 7.702261C689.508174 241.797565 756.869565 334.625391 756.869565 440.05287l0.089044 77.06713-0.089044 7.568696c0 56.765217 18.788174 109.968696 49.775305 158.230261 18.432 28.672 36.953043 49.597217 49.997913 61.618086l-3.739826-4.096c28.093217 36.285217 22.216348 48.88487-22.394435 48.88487H196.118261c-45.456696 0-53.426087-15.894261-26.178783-52.001391l-3.250087 3.695304c12.02087-11.798261 29.072696-32.411826 45.946435-60.816696 28.226783-47.549217 45.278609-100.396522 45.278609-157.250782v-5.431652l0.044522-11.842783V440.008348z"/></symbol><symbol id="ic-nav-follow" viewBox="0 0 1024 1024"><path d="M725.25913 780.14701467L721.474783 780.05797067H868.173913c24.620522 0 44.521739-19.945739 44.521739-44.744347V134.71536267A44.521739 44.521739 0 0 0 868.08487 89.97101467H155.91513C131.33913 89.97101467 111.304348 110.00579667 111.304348 134.71536267v600.598261A44.432696 44.432696 0 0 0 155.737043 780.05797067h146.832696l-3.784348 0.089044c69.053217-3.873391 154.757565 43.186087 213.036522 106.184348 51.778783-60.594087 125.373217-111.170783 213.437217-106.184348zM44.521739 735.31362367V134.71536267A111.482435 111.482435 0 0 1 155.91513 23.18840567h712.16974A111.304348 111.304348 0 0 1 979.478261 134.71536267v600.598261a111.34887 111.34887 0 0 1-111.304348 111.526956h-146.69913c-66.960696-3.784348-172.78887 74.885565-201.861566 145.67513-4.541217 11.130435-10.685217 10.596174-15.449043-0.756869-29.606957-70.611478-134.90087-148.702609-201.594435-144.918261H155.737043A111.215304 111.215304 0 0 1 44.521739 735.31362367zM489.73913 179.01449267h44.52174V735.53623167h-44.52174V179.01449267z m333.913044 178.086957V290.31884067H623.304348v66.782609h200.347826zM623.304348 512.92753667h200.347826v-66.782609H623.304348V512.92753667zM200.347826 357.10144967H400.695652V290.31884067H200.347826v66.782609z m0 155.826087H400.695652v-66.782609H200.347826V512.92753667z"/></symbol><symbol id="ic-nav-download" viewBox="0 0 1024 1024"><path d="M222.608696 133.342609C222.608696 84.279652 262.41113 44.521739 311.785739 44.521739h400.428522C761.455304 44.521739 801.391304 84.680348 801.391304 133.342609v757.314782A88.909913 88.909913 0 0 1 712.214261 979.478261H311.785739A89.266087 89.266087 0 0 1 222.608696 890.657391V133.342609zM445.217391 912.695652c0-12.288 9.616696-22.26087 22.038261-22.260869h89.488696c12.154435 0 22.038261 10.329043 22.038261 22.260869 0 12.288-9.616696 22.26087-22.038261 22.26087h-89.488696a22.394435 22.394435 0 0 1-22.038261-22.26087zM289.391304 845.913043h445.217392V111.304348h-445.217392V845.913043zM356.173913 489.73913l155.826087 184.141913L667.826087 489.73913h-89.043478V356.173913h-133.565218v133.565217H356.173913z"/></symbol><symbol id="ic-paid" viewBox="0 0 1024 1024"><path d="M511.850132 0C229.23293 0 0.000088 229.132871 0.000088 511.850044s229.132871 511.850044 511.850044 511.850044 511.850044-229.232842 511.850044-511.850044-229.132871-511.850044-511.850044-511.850044z m226.833545 576.031241c29.191448 0 52.984477 23.693059 52.984477 52.884506 0 14.195841-5.398418 27.491946-15.195548 37.289076-9.997071 9.997071-23.293176 15.39549-37.588988 15.395489H564.034843v173.649127c0 29.091477-23.593088 52.584594-52.484623 52.584594-29.091477 0-52.884506-23.693059-52.984478-52.684565V681.800254H285.016587c-29.191448 0-52.884506-23.693059-52.884507-52.884507s23.693059-52.884506 52.884507-52.884506h173.449185V454.766768H284.816646c-29.191448 0-52.884506-23.593088-52.884507-52.584595s23.693059-52.584594 52.884507-52.584594H474.161173L330.703202 205.939666c-10.097042-9.997071-15.595431-23.193205-15.595431-37.289075 0-14.09587 5.498389-27.491946 15.595431-37.489017 9.997071-9.997071 23.093234-15.39549 37.189105-15.39549 14.09587 0 27.292004 5.498389 37.289075 15.595431l116.46588 116.465879L637.813229 131.261544c9.997071-10.097042 23.193205-15.595431 37.389046-15.595431 14.09587 0 27.192034 5.498389 37.189104 15.39549 20.493996 20.593967 20.593967 54.184126 0 74.678122L569.23332 349.597579h169.350386c29.191448 0 52.984477 23.593088 52.984477 52.584594 0 27.591916-21.493703 50.285268-48.685736 52.484624h-178.947574V576.131212h174.748804v-0.099971z"/></symbol></svg><div id="__next"><header style="width:100%"><div class="_1CSgtu"><div class="_2oDcyf"><a class="_1AawTM _1OhGeD" href="/" aria-label="简书" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" class="wCYvWN" style="width:60px;height:30px" width="60" height="30" focusable="false" aria-hidden="true" viewBox="0 0 106 50" version="1.1"><g><path d="M79.6542664,49.2735656 L75.6602511,49.6932377 L75.6602511,27.3313525 L59.1137321,27.3313525 C58.6314725,27.3313525 57.9655336,26.8821721 57.8498237,26.1776639 L57.5346557,23.1870902 L75.6602511,23.1887295 L75.6602511,12.1260246 L62.1759992,12.1260246 C61.6180832,12.0858607 61.0229458,11.7788934 60.8894344,10.9870902 L60.5819534,7.93790984 L75.6602511,7.93790984 L75.6602511,0.409631148 L81.2074496,0.409631148 L81.2074496,7.93790984 L97.4727855,7.93790984 L97.4727855,23.1887295 L103.836831,23.1887295 L103.836831,38.1235656 C103.836831,42.2026639 100.70174,44.4715164 97.7187702,44.4715164 L92.357274,44.4715164 C91.6217473,44.4715164 91.0290374,43.9440574 90.9659229,43.3719262 L90.6681519,40.5223361 L92.1274725,40.5223361 L95.7933733,40.5223361 C97.0864115,40.5223361 98.3321137,39.6739754 98.3321137,38.1235656 L98.3321137,27.3313525 L81.2074496,27.3313525 L81.2074496,47.4452869 C81.2074496,48.5985656 80.4148771,49.1264344 79.6542664,49.2735656 L79.6542664,49.2735656 Z M81.260045,22.917418 L91.9745412,22.917418 L91.9745412,12.0514344 L81.260045,12.0514344 L81.260045,22.917418 Z M104.57519,13.9920082 L100.167289,13.9920082 C99.5871214,9.9170082 97.5274038,5.26987705 95.0355947,1.96209016 L100.167289,1.96209016 C102.583037,4.95553279 104.693327,8.85922131 105.720556,12.5608607 C105.867015,13.1711066 105.472144,13.9920082 104.57519,13.9920082 L104.57519,13.9920082 Z"/><path d="M4.98236412,12.0515574 L9.99834885,12.0515574 C11.3953641,13.8056557 12.3169977,15.077377 13.7310053,18.5515574 C13.8337687,18.8339344 13.9130664,20.2007377 12.5900893,20.2007377 L8.57624962,20.2007377 C7.43047863,15.8630328 6.43521145,14.337623 4.97831832,12.0540164 C3.51980687,13.0105738 1.86507405,13.8138525 -4.04580153e-05,14.3802459 L-4.04580153e-05,10.3880328 C2.35987557,9.24172131 4.08662366,7.6892623 5.34608168,5.85278689 C6.43399771,4.2654918 7.18206641,2.47901639 7.58300534,0.409754098 L11.8043947,0.409754098 C12.772555,0.409754098 13.2313489,1.06877049 13.0962191,1.56139344 C12.9129443,2.1392623 12.5342573,2.99377049 12.1640664,3.78270492 L25.2676084,3.78270492 L25.0038221,6.30278689 C24.9370664,6.85483607 24.5227763,7.57532787 23.6181351,7.57532787 L17.5377,7.57532787 C18.2930511,9.24336066 18.7571046,10.7601639 18.9112496,11.3568852 C19.0508298,11.9036066 18.7004634,12.7810656 17.765074,12.7810656 L13.9850817,12.7810656 C13.7127992,10.9581148 13.4073412,9.70811475 12.633784,7.57532787 L9.82963893,7.57532787 C8.54266947,9.20852459 6.93891374,10.7679508 4.98236412,12.0515574 L4.98236412,12.0515574 Z M14.1651198,43.4847541 L14.1651198,22.1413115 L34.8367382,22.1413115 L34.8367382,36.8896721 C34.9253412,41.1093443 31.729158,43.4847541 28.4929214,43.4847541 L14.1651198,43.4847541 Z M35.3776618,49.3056557 C34.6793565,49.3056557 34.1008069,48.7921311 34.0028985,48.0851639 L33.669929,45.2757377 L38.4682496,45.2372131 C39.5310817,45.2372131 40.5696389,44.5277869 40.5696389,42.9769672 L40.5696389,18.4761475 L16.1390664,18.4761475 C15.6288908,18.4761475 15.0078603,18.2769672 14.819326,17.2298361 L14.5478527,14.3802459 L45.940845,14.3802459 L45.940845,43.4048361 C45.940845,45.4257377 44.1275168,49.2339344 39.6552878,49.3056557 L35.3776618,49.3056557 Z M6.84505115,49.2904918 L3.0626313,49.6933607 L3.0626313,20.2007377 L8.42250916,20.2007377 L8.42250916,47.3646721 C8.42250916,48.0146721 8.09399008,49.0794262 6.84505115,49.2904918 L6.84505115,49.2904918 Z M41.1040893,12.617541 L37.1335397,12.617541 C36.8025931,10.8986885 36.5436618,9.44581967 35.7462344,7.47942623 L32.2174863,7.47942623 C31.3132496,8.52245902 30.2645779,9.69581967 28.8404557,10.8204098 C27.3799214,11.9736885 25.5018603,13.1154918 22.9704023,13.9917213 L22.9704023,9.96672131 C27.0331962,7.6904918 29.0237305,4.6007377 29.9906771,0.409754098 L34.2080206,0.409754098 C35.3258756,0.409754098 35.6547992,1.21385246 35.5848069,1.4892623 C35.3121198,2.35811475 34.9075397,2.9892623 34.6032954,3.70360656 L48.620784,3.70360656 L48.3602344,6.23434426 C48.2724405,6.99745902 47.669616,7.47942623 47.0243107,7.47942623 L40.7173107,7.47942623 C41.4892496,9.14459016 41.945616,10.3318852 42.1398145,11.2503279 C42.223158,11.6478689 41.9775779,12.5761475 41.1040893,12.617541 L41.1040893,12.617541 Z M19.1422649,39.6040164 L27.579784,39.6040164 C29.230471,39.6040164 29.861616,38.5290164 29.861616,37.3347541 L29.861616,34.5634426 L19.1693718,34.5589344 L19.1422649,39.6040164 Z M19.1422649,30.6786066 L29.8620206,30.6786066 L29.8620206,26.0220492 L19.1422649,26.0220492 L19.1422649,30.6786066 Z"/></g></svg></a><div class="_7hb9O4"><i aria-label="ic-nav-mode" tabindex="-1" class="anticon _1nZg8v"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-nav-mode"/></svg></i><span class="_1jKNin" aria-label="简书钻"><svg xmlns="http://www.w3.org/2000/svg" class="wCYvWN" style="width:54px;height:24px" width="54" height="24" focusable="false" aria-hidden="true" viewBox="0 0 50 22" version="1.1"><g transform="translate(1.000000, 4.000000)" fill="#EA6F5A" stroke="#EA6F5A" stroke-width="0.5"><path d="M6.97355121,0.977899795 L5.71487604,5.02704222 L14.0382941,5.02704222 L12.7812515,0.977899795 L6.97355121,0.977899795 Z M5.94914309,0.977899795 L5.39898286,0.977899795 C5.34645865,0.977985274 5.29616908,0.99894266 5.25940216,1.03606786 L1.33563333,5.02704222 L4.69046792,5.02704222 L5.94914309,0.977899795 Z M8.80687445,13.8613144 L4.56068234,5.99650966 L1.40664806,5.99650966 L8.80687445,13.8613144 Z M5.67079793,5.99650966 L9.99290234,14.0010793 L14.0962487,5.99650966 L5.67079793,5.99650966 Z M6.61929374,0.00843234872 L14.581108,0.00843234872 C14.8986337,0.00843234872 15.2014667,0.135271015 15.4234898,0.359864308 L19.6476429,4.6562209 C20.0841892,5.10085541 20.0917287,5.80620568 19.6647844,6.25988166 L10.8491606,15.6281688 C10.4068293,16.0970986 9.66444122,16.1231211 9.18970108,15.6863369 C9.1594994,15.6580607 9.1594994,15.6580607 9.13093026,15.6281688 L0.315306487,6.25988166 C-0.111412478,5.80599166 -0.10351395,5.10063881 0.333264249,4.6562209 L4.55741731,0.359864308 C4.7786002,0.135221626 5.08206407,0.00849721138 5.39898286,0.00843234872 L6.62011001,0.00843234872 L6.61929374,0.00843234872 Z M13.8056596,0.977899178 L15.0643348,5.02704222 L18.6452738,5.02704222 L14.721505,1.03606786 C14.6845399,0.998742398 14.6339153,0.977768057 14.581108,0.977899795 L13.8056596,0.977899178 Z M11.1487286,13.8879747 L18.5742591,5.99650966 L15.1949366,5.99650966 L11.1487286,13.8879747 Z"/></g><g transform="translate(19.000000, 5.000000)"><path d="M5.16425826,0.266666667 C3.98605079,0.266666667 3.03092493,1.22179253 3.03092493,2.4 L3.03092493,8.73301908 L2.96783132,8.90526634 L1.31769477,10.8555055 C1.236217,10.9518012 1.19150756,11.0738592 1.19150756,11.2 C1.19150756,11.4945519 1.43028903,11.7333333 1.72484089,11.7333333 L26.5448138,11.7333333 C27.7230213,11.7333333 28.6781471,10.7782075 28.6781471,9.6 L28.6781471,2.4 C28.6781471,1.22179253 27.7230213,0.266666667 26.5448138,0.266666667 L5.16425826,0.266666667 Z" stroke="#EA6F5A" stroke-width="0.533333333" fill="#EA6F5A"/><path d="M6.79203604,2 L7.57631592,2 L7.57631592,4.96927224 C7.96845586,4.26091644 8.52725527,3.9115903 9.25271416,3.9115903 C9.96836954,3.9115903 10.5369725,4.17358491 10.9487194,4.71698113 C11.3212523,5.20215633 11.5173223,5.80377358 11.5173223,6.54123989 C11.5173223,7.29811321 11.3212523,7.91913747 10.9487194,8.40431267 C10.527169,8.92830189 9.94876255,9.2 9.21350016,9.2 C8.42922028,9.2 7.87042087,8.87978437 7.52729842,8.2393531 L7.52729842,9.06415094 L6.79203604,9.06415094 L6.79203604,2 Z M9.08605468,4.55202156 C8.62529025,4.55202156 8.25275731,4.72668464 7.97825935,5.09541779 C7.6841544,5.45444744 7.54690542,5.92991914 7.54690542,6.51212938 L7.54690542,6.60916442 C7.54690542,7.17196765 7.6743509,7.62803235 7.92924186,7.97735849 C8.20373982,8.36549865 8.60568326,8.55956873 9.11546518,8.55956873 C9.66446109,8.55956873 10.076208,8.35579515 10.350706,7.96765499 C10.5859899,7.61832884 10.7134354,7.14285714 10.7134354,6.54123989 C10.7134354,5.93962264 10.5859899,5.47385445 10.331099,5.13423181 C10.0467975,4.74609164 9.6350506,4.55202156 9.08605468,4.55202156 Z M14.6250313,3.9115903 C15.4387217,3.9115903 16.0563421,4.18328841 16.468089,4.72668464 C16.840622,5.21185984 17.0366919,5.90080863 17.0562989,6.77412399 L13.0564716,6.77412399 C13.0956856,7.33692722 13.242738,7.77358491 13.517236,8.08409704 C13.7917339,8.39460916 14.1740704,8.54986523 14.6544418,8.54986523 C15.0661888,8.54986523 15.4093112,8.44312668 15.6642022,8.2393531 C15.8798791,8.06469003 16.0465386,7.80269542 16.1739841,7.45336927 L16.958264,7.45336927 C16.840622,7.93854447 16.6151415,8.32668464 16.2720191,8.63719677 C15.8504686,9.00592992 15.3112762,9.2 14.6544418,9.2 C13.9289829,9.2 13.340773,8.9574124 12.9094191,8.4916442 C12.4584582,8.006469 12.2427812,7.36603774 12.2427812,6.5509434 C12.2427812,5.81347709 12.4486547,5.19245283 12.8800086,4.69757412 C13.3113625,4.17358491 13.8897689,3.9115903 14.6250313,3.9115903 Z M14.6446383,4.56172507 C14.1936774,4.56172507 13.8309479,4.70727763 13.55645,4.99838275 C13.281952,5.28948787 13.1250961,5.67762803 13.0760786,6.17250674 L16.2426086,6.17250674 C16.1445736,5.09541779 15.6053812,4.56172507 14.6446383,4.56172507 Z M19.3307106,2.42695418 L19.3307106,4.04743935 L20.5855584,4.04743935 L20.5855584,4.69757412 L19.3307106,4.69757412 L19.3307106,7.89002695 C19.3307106,8.07439353 19.3601211,8.21024259 19.4385491,8.28787062 C19.5071736,8.36549865 19.634619,8.41401617 19.811082,8.41401617 L20.4581129,8.41401617 L20.4581129,9.06415094 L19.69344,9.06415094 C19.2816931,9.06415094 18.9777846,8.9574124 18.8013217,8.74393531 C18.6346622,8.54986523 18.5562342,8.26846361 18.5562342,7.89002695 L18.5562342,4.69757412 L17.5366704,4.69757412 L17.5366704,4.04743935 L18.5562342,4.04743935 L18.5562342,2.74716981 L19.3307106,2.42695418 Z M23.6638569,3.9115903 C24.3893158,3.9115903 24.9187047,4.09595687 25.2716307,4.47439353 C25.5657356,4.80431267 25.7225916,5.26037736 25.7225916,5.8425876 L25.7225916,9.06415094 L24.9873292,9.06415094 L24.9873292,8.21994609 C24.7912592,8.4916442 24.5265648,8.7245283 24.2030493,8.89919137 C23.8305164,9.09326146 23.408966,9.2 22.9482015,9.2 C22.4384196,9.2 22.0266727,9.06415094 21.7325677,8.81185984 C21.4188558,8.54986523 21.2619998,8.20053908 21.2619998,7.7541779 C21.2619998,7.11374663 21.5168907,6.64797844 22.0364762,6.35687332 C22.4482231,6.11428571 23.016826,5.98814016 23.7226779,5.98814016 L24.9383117,5.97843666 L24.9383117,5.81347709 C24.9383117,4.96927224 24.4971543,4.55202156 23.6148394,4.55202156 C23.212896,4.55202156 22.8893805,4.6296496 22.6540966,4.80431267 C22.3992056,4.97897574 22.2423496,5.23126685 22.1835286,5.58059299 L21.4090523,5.58059299 C21.4874803,5.00808625 21.7325677,4.58113208 22.1541181,4.29973046 C22.5266511,4.03773585 23.0266295,3.9115903 23.6638569,3.9115903 Z M24.9383117,6.58975741 L23.7716954,6.59946092 C22.6344896,6.59946092 22.0658867,6.98760108 22.0658867,7.74447439 C22.0658867,7.98706199 22.1541181,8.19083558 22.3403846,8.34609164 C22.5266511,8.4916442 22.7815421,8.56927224 23.114861,8.56927224 C23.6246429,8.56927224 24.0559969,8.41401617 24.4187263,8.10350404 C24.7618487,7.79299191 24.9383117,7.43396226 24.9383117,7.02641509 L24.9383117,6.58975741 Z" fill="#FFFFFF"/></g></svg></span><a href="/sign_in" target="_blank" class="_2MpoKb _1OyPqC _1AT95S _2WY0RL" role="button" tabindex="-1"><span>登录</span></a><a href="/sign_up" target="_blank" class="_2MpoKb _1OyPqC _3Mi9q9 _2WY0RL" role="button" tabindex="-1"><span>注册</span></a><a href="/writer" target="_blank" class="_1OyPqC _3Mi9q9 _2WY0RL _1YbC5u" role="button" tabindex="-1"><i aria-label="ic-write" style="margin-right:2px" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-write"/></svg></i><span>写文章</span></a></div><div class="_1YyUun"><div class="_2RZATq"><nav class="_3JYrtj"><a class="hM7XFL _1OhGeD" href="/">首页</a><a class="hM7XFL _1OhGeD" href="/apps?utm_medium=desktop&amp;utm_source=navbar-apps">下载APP</a></nav><div class="MoRCpo"><div class="_31TNvD"><input type="search" class="_2q13cl G1b3UE" value="" placeholder="搜索" aria-label="搜索专题" /><span class="x6-7Eb" role="button" tabindex="-1" aria-label="搜索"><i aria-label="icon: search" class="anticon anticon-search"><svg xmlns="http://www.w3.org/2000/svg" viewBox="64 64 896 896" focusable="false" class="" data-icon="search" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M909.6 854.5L649.9 594.8C690.2 542.7 712 479 712 412c0-80.2-31.3-155.4-87.9-212.1-56.6-56.7-132-87.9-212.1-87.9s-155.5 31.3-212.1 87.9C143.2 256.5 112 331.8 112 412c0 80.1 31.3 155.5 87.9 212.1C256.5 680.8 331.8 712 412 712c67 0 130.6-21.8 182.7-62l259.7 259.6a8.2 8.2 0 0 0 11.6 0l43.6-43.5a8.2 8.2 0 0 0 0-11.6zM570.4 570.4C528 612.7 471.8 636 412 636s-116-23.3-158.4-65.6C211.3 528 188 471.8 188 412s23.3-116.1 65.6-158.4C296 211.3 352.2 188 412 188s116.1 23.2 158.4 65.6S636 352.2 636 412s-23.3 116.1-65.6 158.4z"/></svg></i></span></div></div><div class="_1F7CTF" role="button" tabindex="0"><div class="_1o4qyK">抽奖</div><img class="_1YyGPQ" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMIAAABwCAMAAAB7PV99AAAC/VBMVEUAAADdcGT8zcX/z3H/z3DPPjrSOjLFHhzKHRn7vrbeT0XgZUXMExHzrKTJIiHJHhv/zXD3LyXdWT3lUVHPKijUMTHVKinKHBr3s5LfT0/kXUP8Tk7/0HLxUErzMCbyioD4TEr/TEv2OS/4UE3/0XH/0XP8S0r/z3H/0nL4Jyf/Skn3JCT5l0D9VlbsOzb/0XL+q0j/rkr6MjL9qEX8VFT/0HH8qkX1JCT9V1L/0XL1Li7nfyX8VFT7v2T/z3H/0HH1rmL/YF/+PT3/0XL6hkXmcCT/rEj4RUTvnFzzplv3jWD7X1//rVr/aWn3nDvumlX6sFbxUlL/z3H/VVX/yF3/WFj/V1f/Wlr/YWH+x27/XVz/ZGT/UlL/X171Gxr/SUn/Z2fwUFD0Fhb/amr2Hh3/c3PuTk73ISD/bW39UVD3JSL4Jyb/cHD9xVf8SEfzExPsS0vqR0f5qED6LCv/xlv5KSj/yGL+TEr9zV/+Tk76uUzlQUH6MC/9yVr8vlXkPj7/ZGH+wVj7VlTnQ0P8Ozn7RUL/yWj+1mP90mH8YF3/ZFz5tEj7MzL/2Gb8xFL8QD35u1TlnFT0slP8Nzb8Xlb+ulD6rkf/aGL9xFzjk1H7X0v6a0L3Nyj/yHn8tmv/mFvklVblmkj5q0P/yHL7vF/8vk39s038aUz6UEHahUD+1Wf/bmPno1f7dUT5WTz9wWr+0Gffj0P1QkD6YT34QCz9vnfwqlD8V07so03hj035nUL/gmD/bF7pqlv7ZVX/cV3+g0/5STngOTj7rGf3s2H/d2HyWVT9dk//r0r7UknumDvNbDT6OTD5Sy75umb5i2L7i0z0SEb6jkDvPj3Pbzb5VTP5oWfrtV/zrF3+jlfvd1b/0Yb/nmDqr1zxolr4wln9uVjyaFP3TUz6lkP3gUDUej7/pGH0blnpdkzlTTrRdDnsV1X6o07nNTT/eVzyf1vsm1fyYFXeg1D3UlDkTk7+rl76r1f4azL/yoLroF/ngTboYUv3lknoPz7ocTa0USzJAAAAUXRSTlMAAwTdzAYJDRIIGRAYDiUe/v4iNS1LPDIVQSzBiG1SH5bqfU415qagJtzWrotx/PPr18XCta+vloNsYTvX1XVV4+PZt6tSR/Dj1NOenfHi187tLc/wAAAOc0lEQVR42u3beVxUVRQHcIYSFTRR1FSUXNJyyzXX9n0v9SGyKWqZozHpNLGMOTLIFmMwjsyIiMCMMEAmyL6YCmqsKkhogiQo4J772vbpnHsfD2Gw/IOnQ5/5lUSDfj7n67nnvjvwnpkppphiiimmmGKKKab8PyIgMeuYEQjMzQXwAUP+Y9bhImiqn0V0PII51t+VC/xPR0MIBBTQnQ0iOtZQYA8ooB8JfGJh0alTx5ltAe0Bqf8pGktLKyurzsjo1CE2qSbBU2NthwiHjJo8eOywAb0sLS27dO4MCnAYP8RcQARjh3h7CyHe3p4i0du2I4ba9e7WpQs4IMYOIYR+IxZ4gsDFxUUoBIN/aKif34bUiaOHzujbGmKEDNKEwSIQzPGd6wuhBr8NG9at+wYzcvTUGX16dGEl6DA2BBKGiQjB3gECBkLwo4Sv2IycOHWoHUJQYVwIARC6jxKJvIW+9g7fQojBHwz3EL7GLIdMnNGtGyigE0ZkQEJ/f0IAASbMV1gsaiJg/RSAWbZs2ci+PQjCiAy4pU7jCD4+SMiihAPrSA9o/RTw+eef9+zzBCDAYFSEyaF0IbGEsKziBByGAwewC1RAAZhPR/Y1MgMSXgv1R0LYPYSE/HwgHGgiLOMAkNFPEoPRLCUgdA8N9ff39PZFQ/C3u8PCIrKQkA6EyJ49AdBSMP/TPmh41G3oZWfXjTPY+IFB5CkEQ9huTEREEjGkH4hEAwpo/UQwf/7UJ3s/AW14hIS+Q0cuh5qmWrAEOz8/aIOoOCuLEiKAkJR0JP9gejoSsAnNAMxzH7/77icvvvh4e+X1Zwa+/OwDl29hN3Xk11+DAKoZTQ8Y5mNDiaG4OAuK3x0P2ZyWBoaDTQYUUADJop4nThzbfyjZ0dp6dvtl+DNPP0D9ljNGw4CiAEqCUuwIQfBBdAIaEhKKgRBPCGg4ggYk4DqiACpY1PMKJTgCoT3z2PSu/1r/gKETN6zDnZ4TLBpNCe/V1ET4haKBEDZv3759MyLAAG0AAv5+DgCJ5IeArZguuO/6Gfoantu+IQJcRoQwn0yD4MOdNacLQ0PzExKS0qD27ZiGioqKhttHSBtIE1gAybI1q9auXLliycKFs9ojBQUnL5dFWVPE4/cZCsvX/OjJE5cRJ/jyy2H4xafc3H6qOV2Vn59PCFh/rj4OIpUq6pPISiICCsA/9lv7ElhIWRQxjHu5TYIte/DkBoEK0sbgF4e5ubltiz5dk5afRAn16pAQ99VeXqtDpApFRTp0gZJZAUtY0b4ETDVFDGxrDFBAl1ELQd4QXEdjgbBxW9np04VHktJAkBsSsjrwu+++xzBSRW46EJpbgNnbRJjV3ilzRMMzhgNhR5cRHYTIg0fSP50PtSTleS6wAcL7zs5A2Lbz9OmqNCDUh4QEfQehiB1KTW4kmkn9NIvW8EPAnIxquw/DuEFYHtmgU+g1uQcXEcGCaUCY7Oy8EQhguFmz+XZDHAhoULBjh1JX8TkIOMCypMI1q5CwhAcCjMQ+NLxkMM3cIETmqiUMw8g1t4+gYMEoILzKEn7aWXPzZoMmzosCApGABoXuIBCoYP7BvOjoav4IGDRYG1zmJjYNQoUaAOFKxl1XDgJILzMrZyRsBAIa6uJCoPjAoERtYGAgACDhunoqWJS+PTo6uurSQiCwGxIfKcC1NLx7K8JQIKDgoIaRq5hMrcRdXSESiYAw1szG2cfHmSXsjFbHeQVitKVegQBJ9NqxI0AhjgRCz7QqAOz80cFhZfOeykuuOrYxDgOq8jYnHFi+/LaakSdmp6jkclmuSARHO5GtYJCPj89GHyDsQYM0LogQakuzAwNTSjODvLw8ZOLby47kYf17nOD7A/Zr+SRgqnEptb7EjcqKL8yrKqxQSySy8+e1MqUyN8HfHw2W0+aBAVbSj0iokoYE0iQ2BmWXaoO8gBAuziULCOpHgT2PGxKbo7iztiIMXgDxjCiXStxl2ZUpKpWqJBUChhmTgYCIH8FwqUoREkSSrS3VlqaAAMKIxdGXgmn9EF9KWMIj4aq1YRv6LyCJ0EnC5aqUbFVmyd8/QwAx4lUfEmzDnnsIKedLS1MyoX4vBrrgROqnuc7vhkTbYDgNgiHUUCGVy5SqzGzt7+eee+45NLztPA/CtmHPTkUcEdSqalMqa1UqIKxe7S6upwCaJQ+BcBKP3mYtM4ISius1KCj5/dxvGEAcdp7nSgzBwWC4pJDCygnyUqYkllamaGtXY5Ti8mbA3Lk8b0g0uLG2ujYMowRRcUNJScnvf5/bu3fvuYaLGk1FPNSPhmAggOGiZrUXTWWiKluJAneFuJAtHzNnLRkFngllhpdoK09KEB0+nHQOAXsbpOISSLkzIbi6BhNEuUaK9cNffamW1O/uHiIWh0H18C8RzFnzMAhngPB86/M2R/j5rzUg+FNRopPGSXWFzrCSSIIxe3Qa99UktbVQP0YhzoPyaf0Q3jckGmvDYRhEtlUgpBLCWWm2UoZva36iBA6Rp1NQgjsbqTg3jNZPw/+GhMFhsDZvddRrIhxeuXbVmnPKTKVMJlNrpduccSU5uTo5OVHEnzqFe1NCQkKkYnE81s/l1MMh4GGvX+sLNAg8PU8RwqqzdSoQKFXX9G5uQECAgxMEKMG5Ol0cVo+JU4jFhQhozkPZkOiVofUZYxolnFqxYuWNG1fq6mJjj61fn31xsRtZSU4UgQkuF4s10jioXwqA3HgK4LL24RFan7htWMKSFStu3DgRC4mJuZZzmSM4NKewXtyUvLB7V5ELhE7zI+mCYAgIvE+dWrhwCUu4ey0mu2zpYiTcA7DHD7vz6nNz68sLAdBKwBEMBbzPgmAwCLyF12fNunDhQkzsodj9d4/FZO9cCm2gBPbyBb+4qwB+RsMZIs6e432aDXckGvNhQBAKfa8XAKH62P79+2/dUqk+4wgUwIVyaDgB5GROzh/8EwyuC5RgMYQQfAsKzpQdzcjIiNp34uLFxShoIrRQEIADhrzaZDibs2n9Wf6n2fDqjATzEZRQlnEccufOnS0Z+8rhBztEYNgFWr8TBL9IDC4s4Yc/eCfQM5KBwQZGwSU44/idjChIRsaWLVuSy3x9sUwCYBc9/rInAicuzYZrSLjCO4GeVA1/IjVI6L3n+PFDt25C/ZCojC1bk6OvX/elf832dGoRwQJcXedhaJvAgIQcJOyq5nkUroJguMCwC+Zdbd44vuXWrUNbaACw1TH54uFTQqi5efPEPiCBCpoNc9EQlrMpZv0Pv17hmcC+azMkmPcbPlujSYbatyIA4hgQEFueKhK2AYD6nSHwDVf4iAh7eyCcJE349ZcLvBKubsULW5u3vAw8rtclz7Z2dMTyUQAED/3mVJELByAGjuCGQURrwkleCUfZ/cjQ0G+ch67O2jogIDnmkCMkAOMhv5ia6tnSYI8CBCymIWdBJ1xKZ1lCNc/vd+gwGxKmz9brAkiuxQQEOAbsB4CHR3gRtEHYQmDv5AQ9QMBSEmJwcoKJvraJf0JBMvk2UpuE163FdVi0R8CmRmhFTuMx+JwJ11f8nFqc5QvLBP5pboIbAr7AUAOMtP0cMs27fuF1Ie3D7aifWZsZ5yEO9yDZdT42NrFxPwLC5Xpxen5CVlaWCwQRSMBlRAWffYYIcgxxsA+DJhBCwSyegoMAebltgfkOuZhhGCTEns9pTIxFgVwmU4jLj0ZF1ezJEqKBEHASoAlQP4YQ3ObNgxMsu47u8taEgn30ZyRtp/sOGRAg2Yna85WJ4R4MI5HLlFLZ7HC9RlNnvS+CGijBDQRIwJClBANtf2ZTDL+jcDVqNh2EttPVQ04JqkxtZWVjiooJR0FtgEYv9/CQaeSx8S6Qtgh0JTk5VMcQwi6+jheXHVHw/P3vtg5ndBIQhFeeTyzVZmpVEncQSD00APCA19WyuiyDLtA0ES5Twkl+CGdgEdGfFt43LzJSGYMGVWIlrCSJRK6UKuRSOb6WGQ4GplyIhja7gLNgv4kQqvk45BVcpgDr6Wb/kg8YiZ4haUxhlNgEqULBSPEFSWM4TkZdFhroOMOG1HIWXB3CNsXAhnS23d/vXD1Tts+RvQ/j328meUvOyNyxYHljJnwkBCkDjanVZqbgCxJ5vJAMA14XFnNtaNpU7XfjNOPtF8mO7Zits7mMe+k/HjoQTIDqmaZICEHNSBhJorYyMSUxEwiFQjINcGGg1wUMCuCbBK6wIcWwN5BYz+YjwwcaXNAM24Cj0JIgJYNQmqhVyiUALPQGAxDYaSCIpSAgB4y5l++igB/CuOdf7mr2AJkib03Q4HinZKZoGYw63lvITQMYEMEKXOEdw3vvvPPKK6+8+Fg75/HXB05/+kEfW7GaIGtJEIspSikhLxUVe3t7UwMe9OhRlTts2/bp0xfuLHyEd+VhnpqglnAGd7VOXFIkYbio8zy9iWEOGkgjICiYh2+fBwEBbizsbPFob++0mlKklkuIQK3X6D4a+oaeM8jGD7YdJaR9oOdV7n0nvuEZjE3oDU2wAMGjjOCtSUVFRXp9EWT8GDu7N6fo5QwljX/zSUifQYMnvzCEGsj7Z1f67h8F5O5OI7hPuNPMMZMmjJ8w6Y1BUNKAmTPHjNer1Wp90aSZT2B69wZHX4CMsH3VAUK/v/ECriJYRj2M4zZhQSeLLt269cBb4K2sLHsNmDlmyqQpY97s0Q0CL/dARm8bm/79+w8bNO192xdesB2BAAAbwV3CzQoBeaYFMIjAWNKnROAhC5YxYIANpD9NXwAQgVHd9k+CCFBA8LkK+NyCMiwtETYAYkPSG/IEts3oBARBO8KmyQHNwfSigRHpAevMuB69uF84B9scSzawzgDQEQQtGV3BQdOls/E9hvSfDPL8c6dOXbsCBdPJ+B4Ge7DHuDFQu9E+WPhAS6qjPTdsiimmmGKKKaaYYoopDyH/ALZE4T86YscYAAAAAElFTkSuQmCC" alt="reward" /></div></div></div></div><div class="_3t3lfz"><div class="FTZkZo"><div class="_16zCst"><h1 class="_2zeTMs" title="深度 | 神经网络和深度学习简史第四部分：深度学习终迎伟大复兴 ">深度 | 神经网络和深度学习简史第四部分：深度学习终迎伟大复兴 </h1></div><div class="_26qd_C"><a class="qzhJKO" href="/u/8ad7903302b3"><img class="_2JlnTn" src="https://upload.jianshu.io/users/upload_avatars/1496926/af15a9ba-97c7-4d07-91d2-c8a4c9a61418.png?imageMogr2/auto-orient/strip|imageView2/1/w/80/h/80/format/webp" alt="" /><span class="_22gUMi">jiandanjinxin</span></a><button data-locale="zh-CN" type="button" class="_1OyPqC _3Mi9q9"><span>关注</span></button><button type="button" class="_1OyPqC _3Mi9q9 _1YbC5u"><span>赞赏支持</span></button></div></div></div></div><div class="VYwngI"></div></header><div class="_21bLU4 _3kbg6I"><div class="_3VRLsv" role="main"><div class="_gp-ck"><section class="ouvJEz"><h1 class="_1RuRku">深度 | 神经网络和深度学习简史第四部分：深度学习终迎伟大复兴 </h1><div class="rEsl9f"><div class="_2mYfmT"><a class="_1qp91i _1OhGeD" href="/u/8ad7903302b3" target="_blank" rel="noopener noreferrer"><img class="_13D2Eh" src="https://upload.jianshu.io/users/upload_avatars/1496926/af15a9ba-97c7-4d07-91d2-c8a4c9a61418.png?imageMogr2/auto-orient/strip|imageView2/1/w/96/h/96/format/webp" alt="" /></a><div style="margin-left: 8px;"><div class="_3U4Smb"><span class="FxYr8x"><a class="_1OhGeD" href="/u/8ad7903302b3" target="_blank" rel="noopener noreferrer">jiandanjinxin</a></span><button data-locale="zh-CN" type="button" class="_3kba3h _1OyPqC _3Mi9q9 _34692-"><span>关注</span></button></div><div class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-diamond"/></svg></i><span>0.312</span></span><time datetime="2017-10-10T07:28:01.000Z">2017.10.10 15:28:01</time><span>字数 8,203</span><span>阅读 294</span></div></div></div></div><article class="_2rhmJa"><p>来自<a href="https://link.jianshu.com?t=http://www.andreykurenkov.com/" target="_blank" rel="nofollow">Andrey Kurenkov</a><br />
<a href="https://link.jianshu.com?t=http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/" target="_blank" rel="nofollow">A 'Brief' History of Neural Nets and Deep Learning, Part4</a><br />
<a href="https://link.jianshu.com?t=https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=402552632&amp;idx=1&amp;sn=694a4a327a79c4efeeb4db15b3ff4a28#rd" target="_blank" rel="nofollow">深度 | 神经网络和深度学习简史第四部分：深度学习终迎伟大复兴 </a></p>
<p>导读：这是《神经网络和深度学习简史》第四部分。前三部分的链接分别是：</p>
<p><a href="https://link.jianshu.com?t=http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=402032673&amp;idx=1&amp;sn=d7e636b6d033cbcf8a74dfaf710e9ccf&amp;scene=21#wechat_redirect" target="_blank" rel="nofollow">神经网络和深度学习简史（一）</a>：从感知机到BP算法<br />
<a href="https://link.jianshu.com?t=http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=402115604&amp;idx=2&amp;sn=740b0378af1e754b1790a432b4cad5a6&amp;scene=21#wechat_redirect" target="_blank" rel="nofollow">神经网络和深度学习简史（二）</a>：BP算法之后的又一突破——信念网络<br />
<a href="https://link.jianshu.com?t=http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=402228099&amp;idx=1&amp;sn=a8e664d332f7d28250fbbf357c773f62&amp;scene=21#wechat_redirect" target="_blank" rel="nofollow">神经网络和深度学习简史（三）</a>：90年代的兴衰——强化学习与递归神经网络</p>
<p>我们终于来到简史的最后一部分。这一部分，我们会来到故事的尾声并一睹神经网络如何在上世纪九十年代末摆脱颓势并找回自己，也会看到自此以后它获得的惊人先进成果。</p>
<p>「试问机器学习领域的任何一人，是什么让神经网络研究进行下来，对方很可能提及这几个名字中的一个或全部: <a href="https://link.jianshu.com?t=http://www.cs.toronto.edu/~hinton/" target="_blank" rel="nofollow">Geoffrey Hinton</a>，加拿大同事<a href="https://link.jianshu.com?t=http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html" target="_blank" rel="nofollow">Yoshua Bengio</a> 以及脸书和纽约大学的<a href="https://link.jianshu.com?t=http://yann.lecun.com/" target="_blank" rel="nofollow">Yann LeCun</a>。」<br />
<strong>深度学习的密谋</strong></p>
<p>当你希望有一场革命的时候，那么，从密谋开始吧。随着支持向量机的上升和反向传播的失败，对于神经网络研究来说，上世纪早期是一段黑暗的时间。Lecun与Hinton各自提到过，那时他们以及他们学生的论文被拒成了家常便饭，因为论文主题是神经网络。上面的引文可能夸张了——当然机器学习与AI的研究仍然十分活跃，其他人，例如<a href="https://link.jianshu.com?t=http://people.idsia.ch/~juergen/" target="_blank" rel="nofollow">Juergen Schmidhuber</a>也正在研究神经网络——但这段时间的引用次数也清楚表明兴奋期已经平缓下来，尽管还没有完全消失。在研究领域之外，他们找到了一个强有力的同盟：加拿大政府。CIFAR的资助鼓励还没有直接应用的基础研究，这项资助首先鼓励Hinton于1987年搬到加拿大，然后一直资助他的研究直到九十年代中期。…Hinton 没有放弃并改变他的方向，而是继续研究神经网络，并努力从CIFAR那里获得更多资助，正如这篇例文（<a href="https://link.jianshu.com?t=http://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html" target="_blank" rel="nofollow">http://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html</a>）清楚道明的：</p>
<p>「但是，在2004年，Hinton要求领导一项新的有关神经计算的项目。主流机器学习社区对神经网络兴趣寡然。」</p>
<p>「那是最不可能的时候」Bengio是蒙特利尔大学的教授，也是去年重新上马的CIFAR项目联合主管，「其他每个人都在做着不同的事。莫名其妙地，Geoff说服了他们。」</p>
<p>「我们应该为了他们的那场豪赌大力赞许CIFAR。」</p>
<p>CIFAR「对于深度学习的社区形成有着巨大的影响。」LeCun补充道，他是CIFAR项目的另一个联合主管。「我们像是广大机器学习社区的弃儿：无法发表任何文章。这个项目给了我们交流思想的天地。」</p>
<p>资助不算丰厚，但足够让研究员小组继续下去。Hinton和这个小组孕育了一场密谋：用「深度学习」来「重新命名」让人闻之色变的神经网络领域。接下来，每位研究人员肯定都梦想过的事情真的发生了：2006年，Hinton、Simon Osindero与Yee-Whye Teh发表了一篇论文，这被视为一次重要突破，足以重燃人们对神经网络的兴趣：A fast learning algorithm for deep belief nets（论文参见：<a href="https://link.jianshu.com?t=https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf" target="_blank" rel="nofollow">https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf</a>）。</p>
<p>正如我们将要看到的，尽管这个想法所包含的东西都已经很古老了，「深度学习」的运动完全可以说是由这篇文章所开始。但是比起名称，更重要的是如果权重能够以一种更灵活而非随机的方式进行初始化，有着多层的神经网络就可以得以更好地训练。</p>
<p>「历史上的第一次，神经网络没有好处且不可训练的信念被克服了，并且这是个非常强烈的信念。我的一个朋友在ICML（机器学习国际会议）发表了一篇文章，而就在这不久之前，选稿编辑还说过ICML不应该接受这种文章，因为它是关于神经网络，并不适合ICML。实际上如果你看一下去年的ICML，没有一篇文章的标题有『神经网络』四个字，因此ICML不应该接受神经网络的文章。那还仅仅只是几年前。IEEE期刊真的有『不接收你的文章』的官方准则。所以，这种信念其实非常强烈。」</p>
<p></p><div class="image-package">
<div class="image-container" style="max-width: 203px; max-height: 112px;">
<div class="image-container-fill" style="padding-bottom: 55.169999999999995%;"></div>
<div class="image-view" data-width="203" data-height="112"><img data-original-src="//upload-images.jianshu.io/upload_images/1496926-4d6b2da090e95ad2" data-original-width="203" data-original-height="112" data-original-format="image/png" data-original-filesize="6377" data-image-index="0" style="cursor: zoom-in;" class="image-loading" /></div>
</div>
<div class="image-caption"></div>
</div><br />
<em>受限的玻尔兹曼机器</em><p></p>
<p>那么什么叫做初始化权重的灵活方法呢？实际上，这个主意基本就是利用非监督式训练方式去一个一个训练神经层，比起一开始随机分配值的方法要更好些，之后以监督式学习作为结束。每一层都以受限波尔兹曼机器（RBM）开始，就像上图所显示的隐藏单元和可见单元之间并没有连接的玻尔兹曼机器（如同亥姆霍兹机器），并以非监督模式进行数据生成模式的训练。事实证明这种形式的玻尔兹曼机器能够有效采用2002年Hinton引进的方式「最小化对比发散专家训练产品（Training Products of Experts by Minimizing Contrastive Divergence）」进行训练。</p>
<p>基本上，除去单元生成训练数据的可能，这个算法最大化了某些东西，保证更优拟合，事实证明它做的很好。因此，利用这个方法，这个算法如以下：</p>
<p>利用对比发散训练数据训练RBM。这是信念网络（belief net）的第一层。</p>
<p>生成训练后RBM数据的隐藏值，模拟这些隐藏值训练另一个RBM，这是第二层——将之「堆栈」在第一层之上，仅在一个方向上保持权重直至形成一个信念网络。</p>
<p>根据信念网络需求在多层基础上重复步骤2。</p>
<p>如果需要进行分类，就添加一套隐藏单元，对应分类标志，并改变唤醒-休眠算法「微调」权重。这样非监督式与监督式的组合也经常叫做半监督式学习。</p>
<p></p><div class="image-package">
<div class="image-container" style="max-width: 466px; max-height: 246px;">
<div class="image-container-fill" style="padding-bottom: 52.790000000000006%;"></div>
<div class="image-view" data-width="466" data-height="246"><img data-original-src="//upload-images.jianshu.io/upload_images/1496926-319fedf1a3c8fbe3" data-original-width="466" data-original-height="246" data-original-format="image/png" data-original-filesize="9637" data-image-index="1" style="cursor: zoom-in;" class="image-loading" /></div>
</div>
<div class="image-caption"></div>
</div><br />
<em>Hinton引入的层式预训练</em><p></p>
<p>这篇论文展示了深度信念网络（DBNs）对于标准化MNIST字符识别数据库有着完美的表现，超越了仅有几层的普通神经网络。Yoshua Bengio等在这项工作后于2007年提出了「深层网络冗余式逐层训练（ “Greedy Layer-Wise Training of Deep Networks）」，其中他们表达了一个强有力的论点，深度机器学习方法（也就是有着多重处理步骤的方法，或者有着数据等级排列特征显示）在复杂问题上比浅显方法更加有效（双层ANNs或向量支持机器）。</p>
<p></p><div class="image-package">
<div class="image-container" style="max-width: 212px; max-height: 473px;">
<div class="image-container-fill" style="padding-bottom: 223.11%;"></div>
<div class="image-view" data-width="212" data-height="473"><img data-original-src="//upload-images.jianshu.io/upload_images/1496926-9197a5a2839b4498" data-original-width="212" data-original-height="473" data-original-format="image/jpeg" data-original-filesize="20209" data-image-index="2" style="cursor: zoom-in;" class="image-loading" /></div>
</div>
<div class="image-caption"></div>
</div><br />
<em>关于非监督式预训练的另一种看法，利用自动代码取代RBM。</em><p></p>
<p>他们还提出了为什么附加非监督式预训练，并总结这不仅仅以更优化的方式初始权重，而且更加重要的是导致了更有用的可学习数据显示，让算法可以有更加普遍化的模型。实际上，利用RBM并不是那么重要——普通神经网络层的非监督式预训练利用简单的自动代码层反向传播证明了其有效性。同样的，与此同时，另一种叫做分散编码的方法也表明，非监督式特征学习对于改进监督式学习的性能非常有力。</p>
<p>因此，关键在于有着足够多的显示层，这样优良的高层数据显示能够被学习——与传统的手动设计一些特征提取步骤并以提取到的特征进行机器学习方式完全不同。Hinton与Bengio的工作有着实践上的证明，但是更重要的是，展示了深层神经网络并不能被训练好的假设是错误的。LeCun已经在整个九十年代证明了CNN，但是大部分研究团体却拒绝接受。Bengio与Yann LeCun一起，在「实现AI的算法（Scaling Algorithms Towards AI）」研究之上证明了他们自己：</p>
<p>「直至最近，许多人相信训练深层架构是一个太过困难的优化问题。然而，至少有两个不同的方法对此都很有效：应用于卷积神经网络的简单梯度下降[LeCun et al., 1989, LeCun et al., 1998]（适用于信号和图像），以及近期的逐层非监督式学习之后的梯度下降[Hinton et al., 2006, Bengio et al., 2007, Ranzato et al., 2006]。深层架构的研究仍然处于雏形之中，更好的学习算法还有待发现。从更广泛的观点来看待以发现能够引出AI的学习准则为目标这事已经成为指导性观念。我们希望能够激发他人去寻找实现AI的机器学习方法。」</p>
<p>他们的确做到了。或者至少，他们开始了。尽管深度学习还没有达到今天山呼海应的效果，它已经如冰面下的潜流，不容忽视地开始了涌动。那个时候的成果还不那么引人注意——大部分论文中证明的表现都限于MNIST数据库，一个经典的机器学习任务，成为了十年间算法的标准化基准。Hinton在2006年发布的论文展现出惊人的错误率，在测试集上仅有1.25%的错误率，但SVMs已经达到了仅1.4%的错误率，甚至简单的算法在个位数上也能达到较低的错误率，正如在论文中所提到的，LeCun已经在1998年利用CNNs表现出0.95%的错误率。</p>
<p>因此，在MNIST上做得很好并不是什么大事。意识到这一点，并自信这就是深度学习踏上舞台的时刻的Hinton与他的两个研究生，Abdel-rahman Mohamed和George Dahl，展现了他们在一个更具有挑战性的任务上的努力：语音识别（ Speech Recognition）。</p>
<p>利用DBN，这两个学生与Hinton做到了一件事，那就是改善了十年间都没有进步的标准语音识别数据集。这是一个了不起的成就，但是现在回首来看，那只是暗示着即将到来的未来——简而言之，就是打破更多的记录。<br />
<strong>蛮力的重要性</strong></p>
<p>上面所描述的算法对于深度学习的出现有着不容置疑的重要性，但是自上世纪九十年代开始，也有着其他重要组成部分陆续出现：纯粹的计算速度。随着摩尔定律，计算机比起九十年代快了数十倍，让大型数据集和多层的学习更加易于处理。但是甚至这也不够——CPU开始抵达速度增长的上限，计算机能力开始主要通过数个CPU并行计算增长。为了学习深度模型中常有的数百万个权重值，脆弱的CPU并行限制需要被抛弃，并被具有大型并行计算能力的GPUs所代替。意识到这一点也是Abdel-rahman Mohamed，George Dahl与Geoff Hinton做到打破语音识别性能记录的部分原因：</p>
<p>「由Hinton的深度神经网络课堂之一所激发，Mohamed开始将它们应用于语音——但是深度神经网络需要巨大的计算能力，传统计算机显然达不到——因此Hinton与Mohamed招募了Dahl。Dahl是Hinton实验室的学生，他发现了如何利用相同的高端显卡（让栩栩如生的计算机游戏能够显示在私人计算机上）有效训练并模拟神经网络。」</p>
<p>「他们用相同的方法去解决时长过短的语音中片段的音素识别问题，」Hinton说道，「对比于之前标准化三小时基准的方法，他们有了更好的成果。」</p>
<p>在这个案例中利用GPU而不是CPU到底能变得有多快很难说清楚，但是同年《Large-scale Deep Unsupervised Learning using Graphics Processors》这篇论文给出了一个数字：70倍。是的，70倍，这使得数以周记的工作可以被压缩到几天就完成，甚至是一天。之前研发了分散式代码的作者中包括高产的机器学习研究者吴恩达，他逐渐意识到利用大量训练数据与快速计算的能力在之前被赞同学习算法演变愈烈的研究员们低估了。这个想法在2010年的《Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition》（作者之一J. Schimidhuber正是递归LTSM网络（recurrent LTSM networks）的投资者）中也得到了大力支持，展示了MNIST数据库能够达到令人惊叹的0.35%错误率，并且除去大型神经网络、输入的多个变量、以及有效的反向传播GPU实现以外没有任何特殊的地方。这些想法已经存在了数十年，因此尽管可以说算法的改进并不那么重要，但是结果确实强烈表明大型训练数据集与快速腭化计算的蛮力方法是一个关键。</p>
<p>Dahl与Mohamed利用GPU打破记录是一个早期且相对有限的成功，但是它足以激励人们，并且对这两人来说也为他们带来了在微软研究室实习的机会。在这里，他们可以享受到那时已经出现的计算领域内另一个趋势所带来的益处：大数据。这个词语定义宽松，在机器学习的环境下则很容易理解——大量训练数据。大量的训练数据非常重要，因为没有它神经网络仍然不能做到很好——它们有些过拟合了（完美适用于训练数据，但无法推广到新的测试数据）。这说得通——大型神经网络能够计算的复杂度需要许多数据来使它们避免学习训练集中那些不重要的方面——这也是过去研究者面对的主要难题。因此现在，大型公司的计算与数据集合能力证明了其不可替代性。这两个学生在三个月的实习期中轻易地证明了深度学习的能力，微软研究室也自此成为了深度学习语音识别研究的前沿地带。</p>
<p>微软不是唯一一个意识到深度学习力量的大公司（尽管起初它很灵巧）。Navdeep Jaitly是Hinton的另一个学生，2011年曾在谷歌当过暑假实习生。他致力于谷歌的语音识别项目，通过结合深度学习能够让他们现存的设备大大提高。修正后的方法不久就加强了安卓的语音识别技术，替代了许多之前的解决方案。</p>
<p>除了博士实习生给大公司的产品带来的深刻影响之外，这里最著名的是两家公司都在用相同的方法——这方法对所有使用它的人都是开放的。实际上，微软和谷歌的工作成果，以及IBM和Hinton实验室的工作成果，在2012 年发布了令人印象深刻的名为「深层神经网络语音识别的声学建模：分享四个研究小组的观点」的文章。</p>
<p>这四个研究小组——有三个是来自企业，确定能从伤脑筋的深度学习这一新兴技术专利中获益，而大学研究小组推广了技术——共同努力并将他们的成果发布给更广泛的研究社区。如果有什么理想的场景让行业接受研究中的观念，似乎就是这一刻了。</p>
<p>这并不是说公司这么做是为了慈善。这是他们所有人探索如何把技术商业化的开始，其中最为突出的是谷歌。但是也许并非Hinton，而是吴恩达造成了这一切，他促使公司成为世界最大的商业化采用者和技术用户者。在2011年，吴恩达在巡视公司时偶遇到了传说中的谷歌人Jeff Dean，聊了一些他用谷歌的计算资源来训练神经网络所做的努力。</p>
<p>这使Dean着迷，于是与吴恩达一起创建了谷歌大脑（Google Brain）——努力构建真正巨大的神经网络并且探索它们能做什么。这项工作引发了一个规模前所未有的无监督式神经网络学习——16000个CPU核，驱动高达10亿权重的学习（作为比较，Hinton在2006年突破性的DBN大约有100万权重）。神经网络在YouTube视频上被训练，完全无标记，并且学着在这些视频中去辨认最平常的物体——而神经网络对于猫的发现，引起了互联网的集体欢乐。</p>
<p></p><div class="image-package">
<div class="image-container" style="max-width: 640px; max-height: 556px;">
<div class="image-container-fill" style="padding-bottom: 86.88%;"></div>
<div class="image-view" data-width="640" data-height="556"><img data-original-src="//upload-images.jianshu.io/upload_images/1496926-3b81b0bae7b4864c" data-original-width="640" data-original-height="556" data-original-format="image/jpeg" data-original-filesize="17404" data-image-index="3" style="cursor: zoom-in;" class="image-loading" /></div>
</div>
<div class="image-caption"></div>
</div><em>谷歌最著名的神经网络学习猫。这是输入到一个神经元中最佳的一张。</em><p></p>
<p>它很可爱，也很有用。正如他们常规发表的一篇论文中所报道的，由模型学习的特征能用来记录标准的计算机视觉基准的设置性能。</p>
<p>这样一来，谷歌训练大规模的神经网络的内部工具诞生了，自此他们仅需继续发展它。深度学习研究的浪潮始于2006年，现在已经确定进入行业使用。<br />
<strong>深度学习的上升</strong></p>
<p>当深度学习进入行业使用时，研究社区很难保持平静。有效的利用GPU和计算能力的发现是如此重要，它让人们检查长久存疑的假设并且问一些也许很久之前被提及过的问题——也就是，反向传播到底为何没什么用呢？为什么旧的方法不起作用，而不是新的方法能奏效，这样的问题观点让Xavier Glort 和 Yoshua Bengio在2010年写了「理解训练深度前馈神经网络的难点」（Understanding the difficulty of training deep feedforward neural networks）一文。</p>
<p>在文中，他们讨论了两个有重大意义的发现：</p>
<p>为神经网络中神经元选取的特定非线性激活函数，对性能有巨大影响，而默认使用的函数不是最好的选择。</p>
<p>相对于随机选取权重，不考虑神经层的权重就随机选取权重的问题要大得多。以往消失的梯度问题重现，根本上，由于反向传播引入一系列乘法，不可避免地导致给前面的神经层带来细微的偏差。就是这样，除非依据所在的神经层不同分别选取不同的权重 ——否则很小的变化会引起结果巨大变化。</p>
<p></p><div class="image-package">
<div class="image-container" style="max-width: 560px; max-height: 420px;">
<div class="image-container-fill" style="padding-bottom: 75.0%;"></div>
<div class="image-view" data-width="560" data-height="420"><img data-original-src="//upload-images.jianshu.io/upload_images/1496926-540672fd868ac270" data-original-width="560" data-original-height="420" data-original-format="image/png" data-original-filesize="3841" data-image-index="4" style="cursor: zoom-in;" class="image-loading" /></div>
</div>
<div class="image-caption"></div>
</div><em>不同的激活函数。ReLU是</em><em>修正线性单元</em>**<p></p>
<p>第二点的结论已经很清楚了，但是第一点提出了这样的问题：『然而，什么是最好的激活函数？』有三个不同的团队研究了这个问题：LeCun所在的团队，他们研究的是「针对对象识别最好的多级结构是什么？」；另一组是Hinton所在的团队，研究「修正的线性单元改善受限玻尔兹曼机器」；第三组是Bengio所在的团队——「深度稀缺的修正神经网络」。他们都发现惊人的相似结论：近乎不可微的、十分简单的函数f(x)=max(0,x)似乎是最好的。令人吃惊的是，这个函数有点古怪——它不是严格可微的，确切地说，在零点不可微，因此 就 数学而言论文看起来很糟糕。但是，清楚的是零点是很小的数学问题——更严重的问题是为什么这样一个零点两侧导数都是常数的简单函数，这么好用。答案还未揭晓，但一些想法看起来已经成型：</p>
<p>修正的激活导致了表征稀疏，这意味着在给定输入时，很多神经元实际上最终需要输出非零值。这些年的结论是，稀疏对深度学习十分有利，一方面是由于它用更具鲁棒性的方式表征信息，另一方面由于它带来极高的计算效率（如果大多数的神经元在输出零，实际上就可以忽略它们，计算也就更快）。顺便提一句，计算神经科学的研究者首次在大脑视觉系统中引入稀疏计算，比机器学习的研究早了10年。</p>
<p>相比指数函数或者三角函数，简单的函数及其导数，使它能非常快地工作。当使用GPU时，这就不仅仅是一个很小的改善，而是十分重要，因为这能规模化神经网络以很好地完成极具挑战的问题。</p>
<p>后来吴恩达联合发表的「修正的非线性改善神经网络的语音模型 」（Rectifier Nonlinearities Improve Neural Network Acoustic Models）一文，也证明了ReLU导数为常数0或1对学习并无害处。实际上，它有助于避免梯度消失的问题，而这正是反向传播的祸根。此外，除了生成更稀疏的表征，它还能生成更发散的表征——这样就可以结合多个神经元的多重值，而不局限于从单个神经元中获取有意义的结论。</p>
<p>目前，结合2006年以来的这些发现，很清楚的是非监督预训练对深度学习来说不是必要的。虽然，它的确有帮助，但是在某些情况下也表明，纯粹的监督学习（有正确的初始权重规模和激活函数）能超越含非监督训练的学习方式。那么，到底为什么基于反向传播的纯监督学习在过去表现不佳？Geoffrey Hinton总结了目前发现的四个方面问题：</p>
<div class="_2Uzcx_"><button class="VJbwyy" type="button" aria-label="复制代码"><i aria-label="icon: copy" class="anticon anticon-copy"><svg xmlns="http://www.w3.org/2000/svg" viewBox="64 64 896 896" focusable="false" class="" data-icon="copy" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M832 64H296c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h496v688c0 4.4 3.6 8 8 8h56c4.4 0 8-3.6 8-8V96c0-17.7-14.3-32-32-32zM704 192H192c-17.7 0-32 14.3-32 32v530.7c0 8.5 3.4 16.6 9.4 22.6l173.3 173.3c2.2 2.2 4.7 4 7.4 5.5v1.9h4.2c3.5 1.3 7.2 2 11 2H704c17.7 0 32-14.3 32-32V224c0-17.7-14.3-32-32-32zM350 856.2L263.9 770H350v86.2zM664 888H414V746c0-22.1-17.9-40-40-40H232V264h432v624z"/></svg></i></button><pre class="line-numbers  language-undefined"><code class="  language-undefined">带标签的数据集很小，只有现在的千分之一.

计算性能很慢，只有现在的百万分之一.

权重的初始化方式笨拙.

使用了错误的非线性模型。
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div>
<p>好了，就到这里了。深度学习。数十年研究的积累，总结成一个公式就是：</p>
<p>深度学习=许多训练数据+并行计算+规模化、灵巧的的算法</p>
<div class="image-package">
<div class="image-container" style="max-width: 640px; max-height: 145px;">
<div class="image-container-fill" style="padding-bottom: 22.66%;"></div>
<div class="image-view" data-width="640" data-height="145"><img data-original-src="//upload-images.jianshu.io/upload_images/1496926-088afe4fb9bd4415.jpeg" data-original-width="640" data-original-height="145" data-original-format="image/jpeg" data-original-filesize="18981" data-image-index="5" style="cursor: zoom-in;" class="image-loading" /></div>
</div>
<div class="image-caption">640.jpeg</div>
</div>
<p>我希望我是第一个提出这个赏心悦目的方程的，但是看起来有人走在我前面了。</p>
<p>更不要说这里就是希望弄清楚这点。差远了！被想通的东西刚好是相反的：人们的直觉经常出错，尤其是一些看似没有问题的决定及假设通常都是没有根据的。问简单的问题，尝试简单的东西——这些对于改善最新的技术有很大的帮助。其实这一直都在发生，我们看到更多的想法及方法在深度学习领域中被发掘、被分享。例如 G. E. Hinton等的「透过预防特征检测器的互相适应改善神经网络」（ Improving neural networks by preventing co-adaptation of feature detectors）。</p>
<p>其构思很简单：为了避免过度拟合，我们可以随机假装在训练当中有些神经元并不在那儿。想法虽然非常简单——被称为丢弃法（dropout）——但对于实施非常强大的集成学习方法又非常有效，这意味着我们可以在训练数据中实行多种不同的学习方法。随机森林——一种在当今机器学习领域中占主导地位的方法——主要就是得益于集成学习而非常有效。训练多个不同的神经网络是可能的，但它在计算上过于昂贵，而这个简单的想法在本质上也可取得相同的结果，而且性能也可有显著提高。</p>
<p>然而，自2006年以来的所有这些研究发现都不是促使计算机视觉及其他研究机构再次尊重神经网络的原因。这个原因远没有看来的高尚：在现代竞争的基准上完全摧毁其他非深度学习的方法。Geoffrey Hinton召集与他共同写丢弃法的两位作家，Alex Krizhevsky 与 Ilya Sutskever，将他们所发现的想法在ILSVRC-2012计算机视觉比赛中创建了一个条目。</p>
<p>对于我来说，了解他们的工作是非常惊人的，他们的「使用深度卷积神经网络在ImageNet上分类」（ImageNet Classification with deep convolutional neural networks）一文其实就是将一些很旧的概念（例如卷积神经网络的池化及卷积层，输入数据的变化）与一些新的关键观点（例如十分高性能的GPU、ReLU神经元、丢弃法等）重新组合，而这点，正是这一点，就是现代深度网络的所有深意了。但他们如何做到的呢？</p>
<p>远比下一个最近的条目好：它们的误差率是15.3%，第二个最近的是26.2%。在这点上——第一个及唯一一个在比赛中的CNN条目——对于CNNs及深度学习整体来说是一个无可争议的标志，对于计算机视觉，它应该被认真对待。如今，几乎所有的比赛条目都是CNNs——这就是Yann LeCun自1989年以来在上面花费大量心血的神经网络模型。还记得上世纪90年代由Sepp Hochreiter 及 Jürgen Schmidhuber为了解决反向传播问题而开发的LSTM循环神经网络吗？这些在现在也是最新的连续任务比如语音处理的处理方法。</p>
<p>这就是转折点。一波对于其可能发展的狂欢在其无可否认的成绩中达到了高潮，这远远超过了其他已知方法所能处理的。这就是我们在第一部分开头所描写的山呼海应比喻的起点，而且它到如今还一直在增长，强化。深度学习就在这儿，我们看不到寒冬。</p>
<p></p><div class="image-package">
<div class="image-container" style="max-width: 490px; max-height: 323px;">
<div class="image-container-fill" style="padding-bottom: 65.92%;"></div>
<div class="image-view" data-width="490" data-height="323"><img data-original-src="//upload-images.jianshu.io/upload_images/1496926-8d610fda7cec0db6" data-original-width="490" data-original-height="323" data-original-format="image/png" data-original-filesize="41614" data-image-index="6" style="cursor: zoom-in;" class="image-loading" /></div>
</div>
<div class="image-caption"></div>
</div><br />
<em>我们列举了对深度学习的发展做出重要贡献的人物。我相信我不需要再指出自从2012年以来其飞涨的趋势了。</em><br />
<strong>后记：现状</strong><p></p>
<p>如果这是一部电影，2012年ImageNet比赛将是其高潮，而现在在电影结束的时候，我们将会出现这几个字：「他们如今在哪里」。Yann Lecun：Facebook； Geoffrey Hinton： 谷歌； 吴恩达： Coursera、谷歌、百度； Bengi、Schmidhuber 及 Hochreiter 依然还留在学术界——但我们可以很容易推测，这个领域将会有更多的引用及毕业生。</p>
<p>虽然深度学习的理念及成绩令人振奋，但当我在写这几篇文章的时候，我也不由自主地被他们所感动，他们在一个几乎被人遗弃的领域里深耕数十年，他们现在富裕、成功，但重要的是他们如今更确信自己的研究。这些人的思想依然保持开放，而这些大公司也一直在开源他们的深度学习模型，犹如一个由工业界领导研究界的理想国。多美好的故事啊啊。</p>
<p>我愚蠢的以为我可以在这一部分写一个过去几年让人印象深刻的成果总结，但在此，我清楚知道我已经没有足够的空间来写这些。可能有一天我会继续写第五部分，那就可以完成这个故事了。但现在，让我提供以下一个简短的清单：</p>
<p>1.LTSM RNNs的死灰复燃以及分布式表征的代表</p>
<p></p><div class="image-package">
<div class="image-container" style="max-width: 640px; max-height: 260px;">
<div class="image-container-fill" style="padding-bottom: 40.63%;"></div>
<div class="image-view" data-width="640" data-height="260"><img data-original-src="//upload-images.jianshu.io/upload_images/1496926-62eb4790d03ce2cf" data-original-width="640" data-original-height="260" data-original-format="image/jpeg" data-original-filesize="35188" data-image-index="7" style="cursor: zoom-in;" class="image-loading" /></div>
</div>
<div class="image-caption"></div>
</div><em>去年的结果。看看吧！</em><p></p>
<p>2.利用深度学习来加强学习</p>
<p>3.附加外部可读写存储</p>
<p>参考文献：<br />
Kate Allen. How a Toronto professor’s research revolutionized artificial intelligence Science and Technology reporter, Apr 17 2015 <a href="https://link.jianshu.com?t=http://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html" target="_blank" rel="nofollow">http://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html</a><br />
Hinton, G. E., Osindero, S., &amp; Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural computation, 18(7), 1527-1554.<br />
Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural computation, 14(8), 1771-1800.<br />
Bengio, Y., Lamblin, P., Popovici, D., &amp; Larochelle, H. (2007). Greedy layer-wise training of deep networks. Advances in neural information processing systems, 19, 153.<br />
Bengio, Y., &amp; LeCun, Y. (2007). Scaling learning algorithms towards AI. Large-scale kernel machines, 34(5).<br />
Mohamed, A. R., Sainath, T. N., Dahl, G., Ramabhadran, B., Hinton, G. E., &amp; Picheny, M. (2011, May). Deep belief networks using discriminative features for phone recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on (pp. 5060-5063). IEEE.<br />
November 26, 2012. Leading breakthroughs in speech recognition software at Microsoft, Google, IBM Source: <a href="https://link.jianshu.com?t=http://news.utoronto.ca/leading-breakthroughs-speech-recognition-software-microsoft-google-ibm" target="_blank" rel="nofollow">http://news.utoronto.ca/leading-breakthroughs-speech-recognition-software-microsoft-google-ibm</a><br />
Raina, R., Madhavan, A., &amp; Ng, A. Y. (2009, June). Large-scale deep unsupervised learning using graphics processors. In Proceedings of the 26th annual international conference on machine learning (pp. 873-880). ACM.<br />
Claudiu Ciresan, D., Meier, U., Gambardella, L. M., &amp; Schmidhuber, J. (2010). Deep big simple neural nets excel on handwritten digit recognition. arXiv preprint arXiv:1003.0358.<br />
Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A. R., Jaitly, N., … &amp; Kingsbury, B. (2012). Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6), 82-97.<br />
Le, Q. V. (2013, May). Building high-level features using large scale unsupervised learning. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on (pp. 8595-8598). IEEE. ↩<br />
Glorot, X., &amp; Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In International conference on artificial intelligence and statistics (pp. 249-256).<br />
Jarrett, K., Kavukcuoglu, K., Ranzato, M. A., &amp; LeCun, Y. (2009, September). What is the best multi-stage architecture for object recognition?. In Computer Vision, 2009 IEEE 12th International Conference on (pp. 2146-2153). IEEE.<br />
Nair, V., &amp; Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10) (pp. 807-814).<br />
Glorot, X., Bordes, A., &amp; Bengio, Y. (2011). Deep sparse rectifier neural networks. In International Conference on Artificial Intelligence and Statistics (pp. 315-323).<br />
Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013, June). Rectifier nonlinearities improve neural network acoustic models. In Proc. ICML (Vol. 30).<br />
Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.<br />
Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105).<br />
<a href="https://link.jianshu.com?t=http://www.technologyreview.com/news/524026/is-google-cornering-the-market-on-deep-learning/" target="_blank" rel="nofollow">http://www.technologyreview.com/news/524026/is-google-cornering-the-market-on-deep-learning/</a></p>
</article><div></div><div class="_1kCBjS"><div class="_18vaTa"><div class="_3BUZPB"><div class="_2Bo4Th" role="button" tabindex="-1" aria-label="给文章点赞"><i aria-label="ic-like" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-like"/></svg></i></div><span class="_1LOh_5" role="button" tabindex="-1" aria-label="查看点赞列表">6人点赞<i aria-label="icon: right" class="anticon anticon-right"><svg xmlns="http://www.w3.org/2000/svg" viewBox="64 64 896 896" focusable="false" class="" data-icon="right" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M765.7 486.8L314.9 134.7A7.97 7.97 0 0 0 302 141v77.3c0 4.9 2.3 9.6 6.1 12.6l360 281.1-360 281.1c-3.9 3-6.1 7.7-6.1 12.6V883c0 6.7 7.7 10.4 12.9 6.3l450.8-352.1a31.96 31.96 0 0 0 0-50.4z"/></svg></i></span></div><div class="_3BUZPB"><div class="_2Bo4Th" role="button" tabindex="-1"><i aria-label="ic-dislike" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-dislike"/></svg></i></div></div></div><div class="_18vaTa"><a class="_3BUZPB _1x1ok9 _1OhGeD" href="/nb/3059498" target="_blank" rel="noopener noreferrer"><i aria-label="ic-notebook" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-notebook"/></svg></i><span>日记本</span></a><div class="_3BUZPB ant-dropdown-trigger"><div class="_2Bo4Th"><i aria-label="ic-others" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-others"/></svg></i></div></div></div></div><div class="_19DgIp" style="margin-top:24px;margin-bottom:24px"></div><div class="_13lIbp"><div class="_16AzcO">更多精彩内容下载简书APP</div><div class="_6S_NkV"><canvas height="110" width="110" style="height: 110px; width: 110px;"></canvas></div><div class="_191KSt">"小礼物走一走，来简书关注我"</div><button type="button" class="_1OyPqC _3Mi9q9 _2WY0RL _1YbC5u"><span>赞赏支持</span></button><span class="_3zdmIj">还没有人赞赏，支持一下</span></div><div class="d0hShY"><a class="_1OhGeD" href="/u/8ad7903302b3" target="_blank" rel="noopener noreferrer"><img class="_27NmgV" src="https://upload.jianshu.io/users/upload_avatars/1496926/af15a9ba-97c7-4d07-91d2-c8a4c9a61418.png?imageMogr2/auto-orient/strip|imageView2/1/w/100/h/100/format/webp" alt="  " /></a><div class="Uz-vZq"><div class="Cqpr1X"><a class="HC3FFO _1OhGeD" href="/u/8ad7903302b3" title="jiandanjinxin" target="_blank" rel="noopener noreferrer">jiandanjinxin</a><span class="_2WEj6j" title="敏于行而慎于言,讷于言而敏于行&#10;&#10;http://blog.csdn.net/jiandanji...">敏于行而慎于言,讷于言而敏于行

http://blog.csdn.net/jiandanji...</span></div><div class="lJvI3S"><span>总资产12 (约0.66元)</span><span>共写了18.4W字</span><span>获得479个赞</span><span>共447个粉丝</span></div></div><button data-locale="zh-CN" type="button" class="_1OyPqC _3Mi9q9"><span>关注</span></button></div></section><section class="-umr26" aria-label="baidu-ad"><div id="_u6201831_dlpaosdhlmq"><iframe id="iframeu6201831_0" name="iframeu6201831_0" src="https://pos.baidu.com/vcfm?conwid=728&amp;conhei=90&amp;rdid=6201831&amp;dc=3&amp;exps=110261,110252,110011&amp;psi=b6039f1cb51296d7a9906224a66b9f58&amp;di=u6201831&amp;dri=0&amp;dis=0&amp;dai=1&amp;ps=13247x16&amp;enu=encoding&amp;ant=0&amp;dcb=___adblockplus_&amp;dtm=HTML_POST&amp;dvi=0.0&amp;dci=-1&amp;dpt=none&amp;tsr=0&amp;tpr=1608949919084&amp;ti=%E6%B7%B1%E5%BA%A6%20%7C%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8F%B2%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%88%E8%BF%8E%E4%BC%9F%E5%A4%A7%E5%A4%8D%E5%85%B4%20-%20%E7%AE%80%E4%B9%A6&amp;ari=2&amp;ver=1224&amp;dbv=2&amp;drs=3&amp;pcs=912x872&amp;pss=1032x14169&amp;cfv=0&amp;cpl=3&amp;chi=2&amp;cce=true&amp;cec=UTF-8&amp;tlm=1608949919&amp;prot=2&amp;rw=889&amp;ltu=https%3A%2F%2Fwww.jianshu.com%2Fp%2Fe1bac195f06d&amp;ecd=1&amp;uc=1920x1040&amp;pis=-1x-1&amp;sr=1920x1080&amp;tcn=1608949919&amp;qn=4c9492b7df6d8a22&amp;tt=1608949919076.9.10.11" width="728" height="90" align="center,center" vspace="0" hspace="0" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" style="border:0;margin:0;width:728px;height:90px" allowtransparency="true"></iframe></div><script type="text/javascript" src="//cpro.baidustatic.com/cpro/ui/cm.js" async="" defer=""></script></section><div id="note-page-comment"><div class="lazyload-placeholder"></div></div><section class="ouvJEz"><h3 class="QxT4hD"><span>被以下专题收入，发现更多相似内容</span></h3><div class="_2Nttfz"><a class="_3s5t0Q _1OhGeD" href="/c/7847442e0728" target="_blank" rel="noopener noreferrer"><img class="_2vEwGY" src="https://upload.jianshu.io/collections/images/589145/CodeHeader-1920x1200.jpg?imageMogr2/auto-orient/strip|imageView2/1/w/48/h/48/format/webp" alt="" /><span class="_2-Djqu">我爱编程</span></a><a class="_3s5t0Q _1OhGeD" href="/c/c4e9f3b2259a" target="_blank" rel="noopener noreferrer"><img class="_2vEwGY" src="https://upload.jianshu.io/collections/images/368425/4.jpg?imageMogr2/auto-orient/strip|imageView2/1/w/48/h/48/format/webp" alt="" /><span class="_2-Djqu">深度学习</span></a></div><div class="_19DgIp" style="margin-top: 32px; margin-bottom: 32px;"></div><h3 class="QxT4hD"><span>推荐阅读</span><a class="_29KFEa _1OhGeD" href="/" target="_blank" rel="noopener noreferrer">更多精彩内容<i aria-label="ic-right" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-right"/></svg></i></a></h3><ul class="_1iTR78"><li class="_11jppn"><div class="JB6qEE"><div class="em6wEs" title="深度 | 神经网络和深度学习简史（第三部分）：90年代的兴衰——强化学习与递归神经网络" role="heading" aria-level="4"><a class="_2voXH8 _1OhGeD" href="/p/5db8170d4bcb" target="_blank" rel="noopener noreferrer">深度 | 神经网络和深度学习简史（第三部分）：90年代的兴衰——强化学习与递归神经网络</a></div><div class="_3fvgn4">来自Andrey KurenkovA 'Brief' History of Neural Nets and Dee...</div><div class="_1pJt6F"><a class="_3IWz1q _1OhGeD" href="/u/8ad7903302b3" target="_blank" rel="noopener noreferrer"><img class="_34VC_H" src="https://upload.jianshu.io/users/upload_avatars/1496926/af15a9ba-97c7-4d07-91d2-c8a4c9a61418.png?imageMogr2/auto-orient/strip|imageView2/1/w/48/h/48/format/webp" alt="" /><span class="_3tPsL6">jiandanjinxin</span></a><span class="_31hjBO">阅读<!-- --> <!-- -->789</span><span class="_31hjBO">评论<!-- --> <!-- -->0</span><span class="_31hjBO">赞<!-- --> <!-- -->6</span></div></div><a class="_10MMAm _1OhGeD" href="/p/5db8170d4bcb" target="_blank" rel="noopener noreferrer"><img class="_3zGDUj" src="//upload-images.jianshu.io/upload_images/1496926-b5dd96d6bbcb1bd9?imageMogr2/auto-orient/strip|imageView2/1/w/300/h/240/format/webp" alt="" /></a></li><li class="_11jppn"><div class="JB6qEE"><div class="em6wEs" title="机器学习(Machine Learning)&amp;深度学习(Deep Learning)资料(Ch..." role="heading" aria-level="4"><a class="_2voXH8 _1OhGeD" href="/p/feb615cd9509" target="_blank" rel="noopener noreferrer">机器学习(Machine Learning)&amp;深度学习(Deep Learning)资料(Ch...</a></div><div class="_3fvgn4">机器学习(Machine Learning)&amp;深度学习(Deep Learning)资料(Chapter 1) 注...</div><div class="_1pJt6F"><a class="_3IWz1q _1OhGeD" href="/u/185a3c553fc6" target="_blank" rel="noopener noreferrer"><img class="_34VC_H" src="https://upload.jianshu.io/users/upload_avatars/1552893/8bd1f15a-ea45-49f3-bc0f-c36caf4c6b36.jpg?imageMogr2/auto-orient/strip|imageView2/1/w/48/h/48/format/webp" alt="" /><span class="_3tPsL6">Albert陈凯</span></a><span class="_31hjBO">阅读<!-- --> <!-- -->19,494</span><span class="_31hjBO">评论<!-- --> <!-- -->7</span><span class="_31hjBO">赞<!-- --> <!-- -->463</span></div></div></li><li class="_11jppn"><div class="JB6qEE"><div class="em6wEs" title="不知道写什么，那就来一份书单吧" role="heading" aria-level="4"><a class="_2voXH8 _1OhGeD" href="/p/ecba58e32427" target="_blank" rel="noopener noreferrer">不知道写什么，那就来一份书单吧</a></div><div class="_3fvgn4">最近有学弟/妹问我，找工作的话需要看那些书，想想距离上一次写书单已经两年了，索性再更新一版吧，希望里面的书能有所帮...</div><div class="_1pJt6F"><a class="_3IWz1q _1OhGeD" href="/u/7482cba3176c" target="_blank" rel="noopener noreferrer"><img class="_34VC_H" src="https://upload.jianshu.io/users/upload_avatars/325455/51d84f6291b2.jpeg?imageMogr2/auto-orient/strip|imageView2/1/w/48/h/48/format/webp" alt="" /><span class="_3tPsL6">Linsama</span></a><span class="_31hjBO">阅读<!-- --> <!-- -->510</span><span class="_31hjBO">评论<!-- --> <!-- -->0</span><span class="_31hjBO">赞<!-- --> <!-- -->2</span></div></div></li><li class="_11jppn"><div class="JB6qEE"><div class="em6wEs" title="2017-08-03" role="heading" aria-level="4"><a class="_2voXH8 _1OhGeD" href="/p/9d5de679d770" target="_blank" rel="noopener noreferrer">2017-08-03</a></div><div class="_3fvgn4">心中产生的感觉就是宇宙给我的回应，我觉察我的每一个想法所产生的感觉。我创照我的生活，我想要的生活。感恩所有。</div><div class="_1pJt6F"><a class="_3IWz1q _1OhGeD" href="/u/88050aa7dd97" target="_blank" rel="noopener noreferrer"><img class="_34VC_H" src="https://upload.jianshu.io/users/upload_avatars/5469350/757a4577-56ad-42cf-bd80-9d82e5337248?imageMogr2/auto-orient/strip|imageView2/1/w/48/h/48/format/webp" alt="" /><span class="_3tPsL6">殷晴_1115</span></a><span class="_31hjBO">阅读<!-- --> <!-- -->35</span><span class="_31hjBO">评论<!-- --> <!-- -->0</span><span class="_31hjBO">赞<!-- --> <!-- -->0</span></div></div></li><li class="_11jppn"><div class="JB6qEE"><div class="em6wEs" title="赞中国女排里约奥运夺冠" role="heading" aria-level="4"><a class="_2voXH8 _1OhGeD" href="/p/db983e329a1b" target="_blank" rel="noopener noreferrer">赞中国女排里约奥运夺冠</a></div><div class="_3fvgn4">沁园春■ 赞中国女排里约奥运夺冠 （作者  白定稳） 里约球场， 群雄逐鹿， 风倒巨浪。 望五洲排手， 再拼位次；...</div><div class="_1pJt6F"><a class="_3IWz1q _1OhGeD" href="/u/f5d1650cf198" target="_blank" rel="noopener noreferrer"><img class="_34VC_H" src="https://upload.jianshu.io/users/upload_avatars/2824000/fb638986b59d.jpeg?imageMogr2/auto-orient/strip|imageView2/1/w/48/h/48/format/webp" alt="" /><span class="_3tPsL6">白定稳</span></a><span class="_31hjBO">阅读<!-- --> <!-- -->241</span><span class="_31hjBO">评论<!-- --> <!-- -->2</span><span class="_31hjBO">赞<!-- --> <!-- -->2</span></div></div><a class="_10MMAm _1OhGeD" href="/p/db983e329a1b" target="_blank" rel="noopener noreferrer"><img class="_3zGDUj" src="//upload-images.jianshu.io/upload_images/2824000-775c2b0a668c93b8.jpg?imageMogr2/auto-orient/strip|imageView2/1/w/300/h/240/format/webp" alt="" /></a></li></ul></section></div><aside class="_2OwGUo"><section class="-umr26" aria-label="youdao-ad"><img class="QrLrqK" src="https://oimagec5.ydstatic.com/image?id=9206847990555258694&amp;product=adpublish&amp;format=JPEG&amp;w=520&amp;h=347" alt="" /><span class="_22kSId">广告</span></section><section class="_3Z3nHf"><div class="_3Oo-T1"><a class="_1b5rv9 _1OhGeD" href="/u/8ad7903302b3" target="_blank" rel="noopener noreferrer"><img class="_3T9iJQ" src="https://upload.jianshu.io/users/upload_avatars/1496926/af15a9ba-97c7-4d07-91d2-c8a4c9a61418.png?imageMogr2/auto-orient/strip|imageView2/1/w/90/h/90/format/webp" alt="" /></a><div class="_32ZTTG"><div class="_2O0T_w"><div class="_2v-h3G"><span class="_2vh4fr" title="jiandanjinxin"><a class="_1OhGeD" href="/u/8ad7903302b3" target="_blank" rel="noopener noreferrer">jiandanjinxin</a></span></div><button data-locale="zh-CN" type="button" class="tzrf9N _1OyPqC _3Mi9q9 _34692-"><span>关注</span></button></div><div class="_1pXc22">总资产12 (约0.66元)</div></div></div><div class="_19DgIp"></div><div class="_26Hhi2" role="listitem"><div class="_3TNGId" title="王敏捷 - 深度学习框架这十年！"><a class="_2ER8Tt _1OhGeD" href="/p/917bdbed4852" target="_blank" rel="noopener noreferrer">王敏捷 - 深度学习框架这十年！</a></div><div class="DfvGP9">阅读 12</div></div><div class="_26Hhi2" role="listitem"><div class="_3TNGId" title="ubuntu 安装两个Anaconda，并迁移虚拟环境"><a class="_2ER8Tt _1OhGeD" href="/p/7ade3b78220e" target="_blank" rel="noopener noreferrer">ubuntu 安装两个Anaconda，并迁移虚拟环境</a></div><div class="DfvGP9">阅读 401</div></div><div class="_26Hhi2" role="listitem"><div class="_3TNGId" title="CT的窗口技术"><a class="_2ER8Tt _1OhGeD" href="/p/e7aba7e75e27" target="_blank" rel="noopener noreferrer">CT的窗口技术</a></div><div class="DfvGP9">阅读 243</div></div></section><div><div class=""><section class="_3Z3nHf"><h3 class="QHRnq8 QxT4hD"><span>推荐阅读</span></h3><div class="cuOxAY" role="listitem"><div class="_3L5YSq" title="盘点组织诊断必备5个模型，它最受阿里欢迎！"><a class="_1-HJSV _1OhGeD" href="/p/e983b91d2982" target="_blank" rel="noopener noreferrer">盘点组织诊断必备5个模型，它最受阿里欢迎！</a></div><div class="_19haGh">阅读 143</div></div><div class="cuOxAY" role="listitem"><div class="_3L5YSq" title="如何上好一节公开课——从一次公开课得到的启示"><a class="_1-HJSV _1OhGeD" href="/p/d8e502ff6b8e" target="_blank" rel="noopener noreferrer">如何上好一节公开课——从一次公开课得到的启示</a></div><div class="_19haGh">阅读 83</div></div><div class="cuOxAY" role="listitem"><div class="_3L5YSq" title="麦肯锡内部团队的沟通管理方法"><a class="_1-HJSV _1OhGeD" href="/p/6b8928183b46" target="_blank" rel="noopener noreferrer">麦肯锡内部团队的沟通管理方法</a></div><div class="_19haGh">阅读 24</div></div><div class="cuOxAY" role="listitem"><div class="_3L5YSq" title="新晋管理层提升培训课程有哪些？"><a class="_1-HJSV _1OhGeD" href="/p/76d293cb25ed" target="_blank" rel="noopener noreferrer">新晋管理层提升培训课程有哪些？</a></div><div class="_19haGh">阅读 57</div></div><div class="cuOxAY" role="listitem"><div class="_3L5YSq" title="LI和L2范数在机器学习中的应用"><a class="_1-HJSV _1OhGeD" href="/p/618cd0d13098" target="_blank" rel="noopener noreferrer">LI和L2范数在机器学习中的应用</a></div><div class="_19haGh">阅读 86</div></div></section><section class="-umr26" aria-label="baidu-ad"><div id="_u6203499_c5btkif5cia"><iframe id="iframeu6203499_0" name="iframeu6203499_0" src="https://pos.baidu.com/vcfm?conwid=260&amp;conhei=210&amp;rdid=6203499&amp;dc=3&amp;exps=110261,110252,110011&amp;psi=b6039f1cb51296d7a9906224a66b9f58&amp;di=u6203499&amp;dri=0&amp;dis=0&amp;dai=2&amp;ps=153x756&amp;enu=encoding&amp;ant=0&amp;dcb=___adblockplus_&amp;dtm=HTML_POST&amp;dvi=0.0&amp;dci=-1&amp;dpt=none&amp;tsr=0&amp;tpr=1608949919084&amp;ti=%E6%B7%B1%E5%BA%A6%20%7C%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8F%B2%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%88%E8%BF%8E%E4%BC%9F%E5%A4%A7%E5%A4%8D%E5%85%B4%20-%20%E7%AE%80%E4%B9%A6&amp;ari=2&amp;ver=1224&amp;dbv=2&amp;drs=3&amp;pcs=912x872&amp;pss=1032x14264&amp;cfv=0&amp;cpl=3&amp;chi=2&amp;cce=true&amp;cec=UTF-8&amp;tlm=1608949919&amp;prot=2&amp;rw=889&amp;ltu=https%3A%2F%2Fwww.jianshu.com%2Fp%2Fe1bac195f06d&amp;ecd=1&amp;uc=1920x1040&amp;pis=-1x-1&amp;sr=1920x1080&amp;tcn=1608949919&amp;qn=4223c4d69edf4e12&amp;tt=1608949919076.22.22.22" width="260" height="210" align="center,center" vspace="0" hspace="0" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" style="border:0;margin:0;width:260px;height:210px" allowtransparency="true"></iframe></div><script type="text/javascript" src="//cpro.baidustatic.com/cpro/ui/cm.js" async="" defer=""></script></section></div></div></aside></div></div><footer style="width:100%"><div class="_2xr8G8"><div class="_1Jdfvb"><div class="TDvCqd"><textarea class="W2TSX_" placeholder="写下你的评论..."></textarea></div><div class="-pXE92"><div class="_3nj4GN" role="button" tabindex="0" aria-label="添加评论"><i aria-label="ic-reply" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-reply"/></svg></i><span>评论<!-- -->0</span></div><div class="_3nj4GN" role="button" tabindex="0" aria-label="给文章点赞"><i aria-label="ic-like" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-like"/></svg></i><span>赞<!-- -->6</span></div><div class="_3nj4GN ant-dropdown-trigger" role="button" tabindex="0" aria-label="更多操作"><i aria-label="ic-others" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-others"/></svg></i></div></div></div></div><div class="_1LI0En" style="height: 56px;"></div></footer><div class="_3Pnjry"><div class="_1pUUKr"><div class="_2VdqdF" role="button" tabindex="-1" aria-label="给文章点赞"><i aria-label="ic-like" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-like"/></svg></i></div><div class="P63n6G"><div class="_2LKTFF"><span class="_1GPnWJ" role="button" tabindex="-1" aria-label="查看点赞列表">6<!-- -->赞</span><span class="_1GPnWJ">7<!-- -->赞</span></div></div></div><div class="_1pUUKr"><div class="_2VdqdF" role="button" tabindex="-1" aria-label="赞赏作者"><i aria-label="ic-shang" class="anticon"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" aria-hidden="true" focusable="false" class=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#ic-shang"/></svg></i></div><div class="P63n6G" role="button" tabindex="-1" aria-label="查看赞赏列表">赞赏</div></div></div></div><script async="" src="https://hm.baidu.com/hm.js?0c0e9d9b1e7d617b3e6842e85b9fb068"></script><script async="" src="https://www.google-analytics.com/analytics.js"></script><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"isServer":true,"initialState":{"global":{"done":false,"artFromType":null,"fontType":"black","$modal":{"ContributeModal":false,"RewardListModal":false,"PayModal":false,"CollectionModal":false,"LikeListModal":false,"ReportModal":false,"QRCodeShareModal":false,"BookCatalogModal":false,"RewardModal":false},"$ua":{"value":"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36","isIE11":false,"earlyIE":null,"chrome":"77.0","firefox":null,"safari":null,"isMac":false},"$diamondRate":{"displayable":false,"rate":0},"readMode":"day","locale":"zh-CN","seoList":[{"comments_count":0,"public_abbr":"来自Andrey KurenkovA 'Brief' History of Neural Nets and Dee...","share_image_url":"http://upload-images.jianshu.io/upload_images/1496926-b5dd96d6bbcb1bd9","slug":"5db8170d4bcb","user":{"id":1496926,"nickname":"jiandanjinxin","slug":"8ad7903302b3","avatar":"https://upload.jianshu.io/users/upload_avatars/1496926/af15a9ba-97c7-4d07-91d2-c8a4c9a61418.png"},"likes_count":6,"title":"深度 | 神经网络和深度学习简史（第三部分）：90年代的兴衰——强化学习与递归神经网络","id":18135965,"views_count":789},{"comments_count":7,"public_abbr":"机器学习(Machine Learning)\u0026深度学习(Deep Learning)资料(Chapter 1) 注...","share_image_url":"","slug":"feb615cd9509","user":{"id":1552893,"nickname":"Albert陈凯","slug":"185a3c553fc6","avatar":"https://upload.jianshu.io/users/upload_avatars/1552893/8bd1f15a-ea45-49f3-bc0f-c36caf4c6b36.jpg"},"likes_count":463,"title":"机器学习(Machine Learning)\u0026深度学习(Deep Learning)资料(Ch...","id":12082542,"views_count":19494},{"comments_count":0,"public_abbr":"最近有学弟/妹问我，找工作的话需要看那些书，想想距离上一次写书单已经两年了，索性再更新一版吧，希望里面的书能有所帮...","share_image_url":"","slug":"ecba58e32427","user":{"id":325455,"nickname":"Linsama","slug":"7482cba3176c","avatar":"https://upload.jianshu.io/users/upload_avatars/325455/51d84f6291b2.jpeg"},"likes_count":2,"title":"不知道写什么，那就来一份书单吧","id":4823929,"views_count":510},{"comments_count":0,"public_abbr":"心中产生的感觉就是宇宙给我的回应，我觉察我的每一个想法所产生的感觉。我创照我的生活，我想要的生活。感恩所有。","share_image_url":"","slug":"9d5de679d770","user":{"id":5469350,"nickname":"殷晴_1115","slug":"88050aa7dd97","avatar":"https://upload.jianshu.io/users/upload_avatars/5469350/757a4577-56ad-42cf-bd80-9d82e5337248"},"likes_count":0,"title":"2017-08-03","id":15338468,"views_count":35},{"comments_count":2,"public_abbr":"沁园春■ 赞中国女排里约奥运夺冠 （作者  白定稳） 里约球场， 群雄逐鹿， 风倒巨浪。 望五洲排手， 再拼位次；...","share_image_url":"http://upload-images.jianshu.io/upload_images/2824000-775c2b0a668c93b8.jpg","slug":"db983e329a1b","user":{"id":2824000,"nickname":"白定稳","slug":"f5d1650cf198","avatar":"https://upload.jianshu.io/users/upload_avatars/2824000/fb638986b59d.jpeg"},"likes_count":2,"title":"赞中国女排里约奥运夺冠","id":5593568,"views_count":241}]},"note":{"data":{"is_author":false,"last_updated_at":1512929601,"public_title":"深度 | 神经网络和深度学习简史第四部分：深度学习终迎伟大复兴 ","purchased":false,"liked_note":false,"comments_count":0,"free_content":"\u003cp\u003e来自\u003ca href=\"https://link.jianshu.com?t=http://www.andreykurenkov.com/\" target=\"_blank\" rel=\"nofollow\"\u003eAndrey Kurenkov\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://link.jianshu.com?t=http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/\" target=\"_blank\" rel=\"nofollow\"\u003eA 'Brief' History of Neural Nets and Deep Learning, Part4\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://link.jianshu.com?t=https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==\u0026amp;mid=402552632\u0026amp;idx=1\u0026amp;sn=694a4a327a79c4efeeb4db15b3ff4a28#rd\" target=\"_blank\" rel=\"nofollow\"\u003e深度 | 神经网络和深度学习简史第四部分：深度学习终迎伟大复兴 \u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e导读：这是《神经网络和深度学习简史》第四部分。前三部分的链接分别是：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://link.jianshu.com?t=http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==\u0026amp;mid=402032673\u0026amp;idx=1\u0026amp;sn=d7e636b6d033cbcf8a74dfaf710e9ccf\u0026amp;scene=21#wechat_redirect\" target=\"_blank\" rel=\"nofollow\"\u003e神经网络和深度学习简史（一）\u003c/a\u003e：从感知机到BP算法\u003cbr\u003e\n\u003ca href=\"https://link.jianshu.com?t=http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==\u0026amp;mid=402115604\u0026amp;idx=2\u0026amp;sn=740b0378af1e754b1790a432b4cad5a6\u0026amp;scene=21#wechat_redirect\" target=\"_blank\" rel=\"nofollow\"\u003e神经网络和深度学习简史（二）\u003c/a\u003e：BP算法之后的又一突破——信念网络\u003cbr\u003e\n\u003ca href=\"https://link.jianshu.com?t=http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==\u0026amp;mid=402228099\u0026amp;idx=1\u0026amp;sn=a8e664d332f7d28250fbbf357c773f62\u0026amp;scene=21#wechat_redirect\" target=\"_blank\" rel=\"nofollow\"\u003e神经网络和深度学习简史（三）\u003c/a\u003e：90年代的兴衰——强化学习与递归神经网络\u003c/p\u003e\n\u003cp\u003e我们终于来到简史的最后一部分。这一部分，我们会来到故事的尾声并一睹神经网络如何在上世纪九十年代末摆脱颓势并找回自己，也会看到自此以后它获得的惊人先进成果。\u003c/p\u003e\n\u003cp\u003e「试问机器学习领域的任何一人，是什么让神经网络研究进行下来，对方很可能提及这几个名字中的一个或全部: \u003ca href=\"https://link.jianshu.com?t=http://www.cs.toronto.edu/~hinton/\" target=\"_blank\" rel=\"nofollow\"\u003eGeoffrey Hinton\u003c/a\u003e，加拿大同事\u003ca href=\"https://link.jianshu.com?t=http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html\" target=\"_blank\" rel=\"nofollow\"\u003eYoshua Bengio\u003c/a\u003e 以及脸书和纽约大学的\u003ca href=\"https://link.jianshu.com?t=http://yann.lecun.com/\" target=\"_blank\" rel=\"nofollow\"\u003eYann LeCun\u003c/a\u003e。」\u003cbr\u003e\n\u003cstrong\u003e深度学习的密谋\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e当你希望有一场革命的时候，那么，从密谋开始吧。随着支持向量机的上升和反向传播的失败，对于神经网络研究来说，上世纪早期是一段黑暗的时间。Lecun与Hinton各自提到过，那时他们以及他们学生的论文被拒成了家常便饭，因为论文主题是神经网络。上面的引文可能夸张了——当然机器学习与AI的研究仍然十分活跃，其他人，例如\u003ca href=\"https://link.jianshu.com?t=http://people.idsia.ch/~juergen/\" target=\"_blank\" rel=\"nofollow\"\u003eJuergen Schmidhuber\u003c/a\u003e也正在研究神经网络——但这段时间的引用次数也清楚表明兴奋期已经平缓下来，尽管还没有完全消失。在研究领域之外，他们找到了一个强有力的同盟：加拿大政府。CIFAR的资助鼓励还没有直接应用的基础研究，这项资助首先鼓励Hinton于1987年搬到加拿大，然后一直资助他的研究直到九十年代中期。…Hinton 没有放弃并改变他的方向，而是继续研究神经网络，并努力从CIFAR那里获得更多资助，正如这篇例文（\u003ca href=\"https://link.jianshu.com?t=http://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html\" target=\"_blank\" rel=\"nofollow\"\u003ehttp://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html\u003c/a\u003e）清楚道明的：\u003c/p\u003e\n\u003cp\u003e「但是，在2004年，Hinton要求领导一项新的有关神经计算的项目。主流机器学习社区对神经网络兴趣寡然。」\u003c/p\u003e\n\u003cp\u003e「那是最不可能的时候」Bengio是蒙特利尔大学的教授，也是去年重新上马的CIFAR项目联合主管，「其他每个人都在做着不同的事。莫名其妙地，Geoff说服了他们。」\u003c/p\u003e\n\u003cp\u003e「我们应该为了他们的那场豪赌大力赞许CIFAR。」\u003c/p\u003e\n\u003cp\u003eCIFAR「对于深度学习的社区形成有着巨大的影响。」LeCun补充道，他是CIFAR项目的另一个联合主管。「我们像是广大机器学习社区的弃儿：无法发表任何文章。这个项目给了我们交流思想的天地。」\u003c/p\u003e\n\u003cp\u003e资助不算丰厚，但足够让研究员小组继续下去。Hinton和这个小组孕育了一场密谋：用「深度学习」来「重新命名」让人闻之色变的神经网络领域。接下来，每位研究人员肯定都梦想过的事情真的发生了：2006年，Hinton、Simon Osindero与Yee-Whye Teh发表了一篇论文，这被视为一次重要突破，足以重燃人们对神经网络的兴趣：A fast learning algorithm for deep belief nets（论文参见：\u003ca href=\"https://link.jianshu.com?t=https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\" target=\"_blank\" rel=\"nofollow\"\u003ehttps://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\u003c/a\u003e）。\u003c/p\u003e\n\u003cp\u003e正如我们将要看到的，尽管这个想法所包含的东西都已经很古老了，「深度学习」的运动完全可以说是由这篇文章所开始。但是比起名称，更重要的是如果权重能够以一种更灵活而非随机的方式进行初始化，有着多层的神经网络就可以得以更好地训练。\u003c/p\u003e\n\u003cp\u003e「历史上的第一次，神经网络没有好处且不可训练的信念被克服了，并且这是个非常强烈的信念。我的一个朋友在ICML（机器学习国际会议）发表了一篇文章，而就在这不久之前，选稿编辑还说过ICML不应该接受这种文章，因为它是关于神经网络，并不适合ICML。实际上如果你看一下去年的ICML，没有一篇文章的标题有『神经网络』四个字，因此ICML不应该接受神经网络的文章。那还仅仅只是几年前。IEEE期刊真的有『不接收你的文章』的官方准则。所以，这种信念其实非常强烈。」\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 203px; max-height: 112px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 55.169999999999995%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"203\" data-height=\"112\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/1496926-4d6b2da090e95ad2\" data-original-width=\"203\" data-original-height=\"112\" data-original-format=\"image/png\" data-original-filesize=\"6377\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cem\u003e受限的玻尔兹曼机器\u003c/em\u003e\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e那么什么叫做初始化权重的灵活方法呢？实际上，这个主意基本就是利用非监督式训练方式去一个一个训练神经层，比起一开始随机分配值的方法要更好些，之后以监督式学习作为结束。每一层都以受限波尔兹曼机器（RBM）开始，就像上图所显示的隐藏单元和可见单元之间并没有连接的玻尔兹曼机器（如同亥姆霍兹机器），并以非监督模式进行数据生成模式的训练。事实证明这种形式的玻尔兹曼机器能够有效采用2002年Hinton引进的方式「最小化对比发散专家训练产品（Training Products of Experts by Minimizing Contrastive Divergence）」进行训练。\u003c/p\u003e\n\u003cp\u003e基本上，除去单元生成训练数据的可能，这个算法最大化了某些东西，保证更优拟合，事实证明它做的很好。因此，利用这个方法，这个算法如以下：\u003c/p\u003e\n\u003cp\u003e利用对比发散训练数据训练RBM。这是信念网络（belief net）的第一层。\u003c/p\u003e\n\u003cp\u003e生成训练后RBM数据的隐藏值，模拟这些隐藏值训练另一个RBM，这是第二层——将之「堆栈」在第一层之上，仅在一个方向上保持权重直至形成一个信念网络。\u003c/p\u003e\n\u003cp\u003e根据信念网络需求在多层基础上重复步骤2。\u003c/p\u003e\n\u003cp\u003e如果需要进行分类，就添加一套隐藏单元，对应分类标志，并改变唤醒-休眠算法「微调」权重。这样非监督式与监督式的组合也经常叫做半监督式学习。\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 466px; max-height: 246px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 52.790000000000006%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"466\" data-height=\"246\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/1496926-319fedf1a3c8fbe3\" data-original-width=\"466\" data-original-height=\"246\" data-original-format=\"image/png\" data-original-filesize=\"9637\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cem\u003eHinton引入的层式预训练\u003c/em\u003e\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e这篇论文展示了深度信念网络（DBNs）对于标准化MNIST字符识别数据库有着完美的表现，超越了仅有几层的普通神经网络。Yoshua Bengio等在这项工作后于2007年提出了「深层网络冗余式逐层训练（ “Greedy Layer-Wise Training of Deep Networks）」，其中他们表达了一个强有力的论点，深度机器学习方法（也就是有着多重处理步骤的方法，或者有着数据等级排列特征显示）在复杂问题上比浅显方法更加有效（双层ANNs或向量支持机器）。\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 212px; max-height: 473px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 223.11%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"212\" data-height=\"473\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/1496926-9197a5a2839b4498\" data-original-width=\"212\" data-original-height=\"473\" data-original-format=\"image/jpeg\" data-original-filesize=\"20209\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cem\u003e关于非监督式预训练的另一种看法，利用自动代码取代RBM。\u003c/em\u003e\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e他们还提出了为什么附加非监督式预训练，并总结这不仅仅以更优化的方式初始权重，而且更加重要的是导致了更有用的可学习数据显示，让算法可以有更加普遍化的模型。实际上，利用RBM并不是那么重要——普通神经网络层的非监督式预训练利用简单的自动代码层反向传播证明了其有效性。同样的，与此同时，另一种叫做分散编码的方法也表明，非监督式特征学习对于改进监督式学习的性能非常有力。\u003c/p\u003e\n\u003cp\u003e因此，关键在于有着足够多的显示层，这样优良的高层数据显示能够被学习——与传统的手动设计一些特征提取步骤并以提取到的特征进行机器学习方式完全不同。Hinton与Bengio的工作有着实践上的证明，但是更重要的是，展示了深层神经网络并不能被训练好的假设是错误的。LeCun已经在整个九十年代证明了CNN，但是大部分研究团体却拒绝接受。Bengio与Yann LeCun一起，在「实现AI的算法（Scaling Algorithms Towards AI）」研究之上证明了他们自己：\u003c/p\u003e\n\u003cp\u003e「直至最近，许多人相信训练深层架构是一个太过困难的优化问题。然而，至少有两个不同的方法对此都很有效：应用于卷积神经网络的简单梯度下降[LeCun et al., 1989, LeCun et al., 1998]（适用于信号和图像），以及近期的逐层非监督式学习之后的梯度下降[Hinton et al., 2006, Bengio et al., 2007, Ranzato et al., 2006]。深层架构的研究仍然处于雏形之中，更好的学习算法还有待发现。从更广泛的观点来看待以发现能够引出AI的学习准则为目标这事已经成为指导性观念。我们希望能够激发他人去寻找实现AI的机器学习方法。」\u003c/p\u003e\n\u003cp\u003e他们的确做到了。或者至少，他们开始了。尽管深度学习还没有达到今天山呼海应的效果，它已经如冰面下的潜流，不容忽视地开始了涌动。那个时候的成果还不那么引人注意——大部分论文中证明的表现都限于MNIST数据库，一个经典的机器学习任务，成为了十年间算法的标准化基准。Hinton在2006年发布的论文展现出惊人的错误率，在测试集上仅有1.25%的错误率，但SVMs已经达到了仅1.4%的错误率，甚至简单的算法在个位数上也能达到较低的错误率，正如在论文中所提到的，LeCun已经在1998年利用CNNs表现出0.95%的错误率。\u003c/p\u003e\n\u003cp\u003e因此，在MNIST上做得很好并不是什么大事。意识到这一点，并自信这就是深度学习踏上舞台的时刻的Hinton与他的两个研究生，Abdel-rahman Mohamed和George Dahl，展现了他们在一个更具有挑战性的任务上的努力：语音识别（ Speech Recognition）。\u003c/p\u003e\n\u003cp\u003e利用DBN，这两个学生与Hinton做到了一件事，那就是改善了十年间都没有进步的标准语音识别数据集。这是一个了不起的成就，但是现在回首来看，那只是暗示着即将到来的未来——简而言之，就是打破更多的记录。\u003cbr\u003e\n\u003cstrong\u003e蛮力的重要性\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e上面所描述的算法对于深度学习的出现有着不容置疑的重要性，但是自上世纪九十年代开始，也有着其他重要组成部分陆续出现：纯粹的计算速度。随着摩尔定律，计算机比起九十年代快了数十倍，让大型数据集和多层的学习更加易于处理。但是甚至这也不够——CPU开始抵达速度增长的上限，计算机能力开始主要通过数个CPU并行计算增长。为了学习深度模型中常有的数百万个权重值，脆弱的CPU并行限制需要被抛弃，并被具有大型并行计算能力的GPUs所代替。意识到这一点也是Abdel-rahman Mohamed，George Dahl与Geoff Hinton做到打破语音识别性能记录的部分原因：\u003c/p\u003e\n\u003cp\u003e「由Hinton的深度神经网络课堂之一所激发，Mohamed开始将它们应用于语音——但是深度神经网络需要巨大的计算能力，传统计算机显然达不到——因此Hinton与Mohamed招募了Dahl。Dahl是Hinton实验室的学生，他发现了如何利用相同的高端显卡（让栩栩如生的计算机游戏能够显示在私人计算机上）有效训练并模拟神经网络。」\u003c/p\u003e\n\u003cp\u003e「他们用相同的方法去解决时长过短的语音中片段的音素识别问题，」Hinton说道，「对比于之前标准化三小时基准的方法，他们有了更好的成果。」\u003c/p\u003e\n\u003cp\u003e在这个案例中利用GPU而不是CPU到底能变得有多快很难说清楚，但是同年《Large-scale Deep Unsupervised Learning using Graphics Processors》这篇论文给出了一个数字：70倍。是的，70倍，这使得数以周记的工作可以被压缩到几天就完成，甚至是一天。之前研发了分散式代码的作者中包括高产的机器学习研究者吴恩达，他逐渐意识到利用大量训练数据与快速计算的能力在之前被赞同学习算法演变愈烈的研究员们低估了。这个想法在2010年的《Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition》（作者之一J. Schimidhuber正是递归LTSM网络（recurrent LTSM networks）的投资者）中也得到了大力支持，展示了MNIST数据库能够达到令人惊叹的0.35%错误率，并且除去大型神经网络、输入的多个变量、以及有效的反向传播GPU实现以外没有任何特殊的地方。这些想法已经存在了数十年，因此尽管可以说算法的改进并不那么重要，但是结果确实强烈表明大型训练数据集与快速腭化计算的蛮力方法是一个关键。\u003c/p\u003e\n\u003cp\u003eDahl与Mohamed利用GPU打破记录是一个早期且相对有限的成功，但是它足以激励人们，并且对这两人来说也为他们带来了在微软研究室实习的机会。在这里，他们可以享受到那时已经出现的计算领域内另一个趋势所带来的益处：大数据。这个词语定义宽松，在机器学习的环境下则很容易理解——大量训练数据。大量的训练数据非常重要，因为没有它神经网络仍然不能做到很好——它们有些过拟合了（完美适用于训练数据，但无法推广到新的测试数据）。这说得通——大型神经网络能够计算的复杂度需要许多数据来使它们避免学习训练集中那些不重要的方面——这也是过去研究者面对的主要难题。因此现在，大型公司的计算与数据集合能力证明了其不可替代性。这两个学生在三个月的实习期中轻易地证明了深度学习的能力，微软研究室也自此成为了深度学习语音识别研究的前沿地带。\u003c/p\u003e\n\u003cp\u003e微软不是唯一一个意识到深度学习力量的大公司（尽管起初它很灵巧）。Navdeep Jaitly是Hinton的另一个学生，2011年曾在谷歌当过暑假实习生。他致力于谷歌的语音识别项目，通过结合深度学习能够让他们现存的设备大大提高。修正后的方法不久就加强了安卓的语音识别技术，替代了许多之前的解决方案。\u003c/p\u003e\n\u003cp\u003e除了博士实习生给大公司的产品带来的深刻影响之外，这里最著名的是两家公司都在用相同的方法——这方法对所有使用它的人都是开放的。实际上，微软和谷歌的工作成果，以及IBM和Hinton实验室的工作成果，在2012 年发布了令人印象深刻的名为「深层神经网络语音识别的声学建模：分享四个研究小组的观点」的文章。\u003c/p\u003e\n\u003cp\u003e这四个研究小组——有三个是来自企业，确定能从伤脑筋的深度学习这一新兴技术专利中获益，而大学研究小组推广了技术——共同努力并将他们的成果发布给更广泛的研究社区。如果有什么理想的场景让行业接受研究中的观念，似乎就是这一刻了。\u003c/p\u003e\n\u003cp\u003e这并不是说公司这么做是为了慈善。这是他们所有人探索如何把技术商业化的开始，其中最为突出的是谷歌。但是也许并非Hinton，而是吴恩达造成了这一切，他促使公司成为世界最大的商业化采用者和技术用户者。在2011年，吴恩达在巡视公司时偶遇到了传说中的谷歌人Jeff Dean，聊了一些他用谷歌的计算资源来训练神经网络所做的努力。\u003c/p\u003e\n\u003cp\u003e这使Dean着迷，于是与吴恩达一起创建了谷歌大脑（Google Brain）——努力构建真正巨大的神经网络并且探索它们能做什么。这项工作引发了一个规模前所未有的无监督式神经网络学习——16000个CPU核，驱动高达10亿权重的学习（作为比较，Hinton在2006年突破性的DBN大约有100万权重）。神经网络在YouTube视频上被训练，完全无标记，并且学着在这些视频中去辨认最平常的物体——而神经网络对于猫的发现，引起了互联网的集体欢乐。\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 640px; max-height: 556px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 86.88%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"640\" data-height=\"556\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/1496926-3b81b0bae7b4864c\" data-original-width=\"640\" data-original-height=\"556\" data-original-format=\"image/jpeg\" data-original-filesize=\"17404\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cem\u003e谷歌最著名的神经网络学习猫。这是输入到一个神经元中最佳的一张。\u003c/em\u003e\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e它很可爱，也很有用。正如他们常规发表的一篇论文中所报道的，由模型学习的特征能用来记录标准的计算机视觉基准的设置性能。\u003c/p\u003e\n\u003cp\u003e这样一来，谷歌训练大规模的神经网络的内部工具诞生了，自此他们仅需继续发展它。深度学习研究的浪潮始于2006年，现在已经确定进入行业使用。\u003cbr\u003e\n\u003cstrong\u003e深度学习的上升\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e当深度学习进入行业使用时，研究社区很难保持平静。有效的利用GPU和计算能力的发现是如此重要，它让人们检查长久存疑的假设并且问一些也许很久之前被提及过的问题——也就是，反向传播到底为何没什么用呢？为什么旧的方法不起作用，而不是新的方法能奏效，这样的问题观点让Xavier Glort 和 Yoshua Bengio在2010年写了「理解训练深度前馈神经网络的难点」（Understanding the difficulty of training deep feedforward neural networks）一文。\u003c/p\u003e\n\u003cp\u003e在文中，他们讨论了两个有重大意义的发现：\u003c/p\u003e\n\u003cp\u003e为神经网络中神经元选取的特定非线性激活函数，对性能有巨大影响，而默认使用的函数不是最好的选择。\u003c/p\u003e\n\u003cp\u003e相对于随机选取权重，不考虑神经层的权重就随机选取权重的问题要大得多。以往消失的梯度问题重现，根本上，由于反向传播引入一系列乘法，不可避免地导致给前面的神经层带来细微的偏差。就是这样，除非依据所在的神经层不同分别选取不同的权重 ——否则很小的变化会引起结果巨大变化。\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 560px; max-height: 420px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 75.0%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"560\" data-height=\"420\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/1496926-540672fd868ac270\" data-original-width=\"560\" data-original-height=\"420\" data-original-format=\"image/png\" data-original-filesize=\"3841\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cem\u003e不同的激活函数。ReLU是\u003c/em\u003e\u003cem\u003e修正线性单元\u003c/em\u003e**\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e第二点的结论已经很清楚了，但是第一点提出了这样的问题：『然而，什么是最好的激活函数？』有三个不同的团队研究了这个问题：LeCun所在的团队，他们研究的是「针对对象识别最好的多级结构是什么？」；另一组是Hinton所在的团队，研究「修正的线性单元改善受限玻尔兹曼机器」；第三组是Bengio所在的团队——「深度稀缺的修正神经网络」。他们都发现惊人的相似结论：近乎不可微的、十分简单的函数f(x)=max(0,x)似乎是最好的。令人吃惊的是，这个函数有点古怪——它不是严格可微的，确切地说，在零点不可微，因此 就 数学而言论文看起来很糟糕。但是，清楚的是零点是很小的数学问题——更严重的问题是为什么这样一个零点两侧导数都是常数的简单函数，这么好用。答案还未揭晓，但一些想法看起来已经成型：\u003c/p\u003e\n\u003cp\u003e修正的激活导致了表征稀疏，这意味着在给定输入时，很多神经元实际上最终需要输出非零值。这些年的结论是，稀疏对深度学习十分有利，一方面是由于它用更具鲁棒性的方式表征信息，另一方面由于它带来极高的计算效率（如果大多数的神经元在输出零，实际上就可以忽略它们，计算也就更快）。顺便提一句，计算神经科学的研究者首次在大脑视觉系统中引入稀疏计算，比机器学习的研究早了10年。\u003c/p\u003e\n\u003cp\u003e相比指数函数或者三角函数，简单的函数及其导数，使它能非常快地工作。当使用GPU时，这就不仅仅是一个很小的改善，而是十分重要，因为这能规模化神经网络以很好地完成极具挑战的问题。\u003c/p\u003e\n\u003cp\u003e后来吴恩达联合发表的「修正的非线性改善神经网络的语音模型 」（Rectifier Nonlinearities Improve Neural Network Acoustic Models）一文，也证明了ReLU导数为常数0或1对学习并无害处。实际上，它有助于避免梯度消失的问题，而这正是反向传播的祸根。此外，除了生成更稀疏的表征，它还能生成更发散的表征——这样就可以结合多个神经元的多重值，而不局限于从单个神经元中获取有意义的结论。\u003c/p\u003e\n\u003cp\u003e目前，结合2006年以来的这些发现，很清楚的是非监督预训练对深度学习来说不是必要的。虽然，它的确有帮助，但是在某些情况下也表明，纯粹的监督学习（有正确的初始权重规模和激活函数）能超越含非监督训练的学习方式。那么，到底为什么基于反向传播的纯监督学习在过去表现不佳？Geoffrey Hinton总结了目前发现的四个方面问题：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e带标签的数据集很小，只有现在的千分之一.\n\n计算性能很慢，只有现在的百万分之一.\n\n权重的初始化方式笨拙.\n\n使用了错误的非线性模型。\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e好了，就到这里了。深度学习。数十年研究的积累，总结成一个公式就是：\u003c/p\u003e\n\u003cp\u003e深度学习=许多训练数据+并行计算+规模化、灵巧的的算法\u003c/p\u003e\n\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 640px; max-height: 145px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 22.66%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"640\" data-height=\"145\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/1496926-088afe4fb9bd4415.jpeg\" data-original-width=\"640\" data-original-height=\"145\" data-original-format=\"image/jpeg\" data-original-filesize=\"18981\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e640.jpeg\u003c/div\u003e\n\u003c/div\u003e\n\u003cp\u003e我希望我是第一个提出这个赏心悦目的方程的，但是看起来有人走在我前面了。\u003c/p\u003e\n\u003cp\u003e更不要说这里就是希望弄清楚这点。差远了！被想通的东西刚好是相反的：人们的直觉经常出错，尤其是一些看似没有问题的决定及假设通常都是没有根据的。问简单的问题，尝试简单的东西——这些对于改善最新的技术有很大的帮助。其实这一直都在发生，我们看到更多的想法及方法在深度学习领域中被发掘、被分享。例如 G. E. Hinton等的「透过预防特征检测器的互相适应改善神经网络」（ Improving neural networks by preventing co-adaptation of feature detectors）。\u003c/p\u003e\n\u003cp\u003e其构思很简单：为了避免过度拟合，我们可以随机假装在训练当中有些神经元并不在那儿。想法虽然非常简单——被称为丢弃法（dropout）——但对于实施非常强大的集成学习方法又非常有效，这意味着我们可以在训练数据中实行多种不同的学习方法。随机森林——一种在当今机器学习领域中占主导地位的方法——主要就是得益于集成学习而非常有效。训练多个不同的神经网络是可能的，但它在计算上过于昂贵，而这个简单的想法在本质上也可取得相同的结果，而且性能也可有显著提高。\u003c/p\u003e\n\u003cp\u003e然而，自2006年以来的所有这些研究发现都不是促使计算机视觉及其他研究机构再次尊重神经网络的原因。这个原因远没有看来的高尚：在现代竞争的基准上完全摧毁其他非深度学习的方法。Geoffrey Hinton召集与他共同写丢弃法的两位作家，Alex Krizhevsky 与 Ilya Sutskever，将他们所发现的想法在ILSVRC-2012计算机视觉比赛中创建了一个条目。\u003c/p\u003e\n\u003cp\u003e对于我来说，了解他们的工作是非常惊人的，他们的「使用深度卷积神经网络在ImageNet上分类」（ImageNet Classification with deep convolutional neural networks）一文其实就是将一些很旧的概念（例如卷积神经网络的池化及卷积层，输入数据的变化）与一些新的关键观点（例如十分高性能的GPU、ReLU神经元、丢弃法等）重新组合，而这点，正是这一点，就是现代深度网络的所有深意了。但他们如何做到的呢？\u003c/p\u003e\n\u003cp\u003e远比下一个最近的条目好：它们的误差率是15.3%，第二个最近的是26.2%。在这点上——第一个及唯一一个在比赛中的CNN条目——对于CNNs及深度学习整体来说是一个无可争议的标志，对于计算机视觉，它应该被认真对待。如今，几乎所有的比赛条目都是CNNs——这就是Yann LeCun自1989年以来在上面花费大量心血的神经网络模型。还记得上世纪90年代由Sepp Hochreiter 及 Jürgen Schmidhuber为了解决反向传播问题而开发的LSTM循环神经网络吗？这些在现在也是最新的连续任务比如语音处理的处理方法。\u003c/p\u003e\n\u003cp\u003e这就是转折点。一波对于其可能发展的狂欢在其无可否认的成绩中达到了高潮，这远远超过了其他已知方法所能处理的。这就是我们在第一部分开头所描写的山呼海应比喻的起点，而且它到如今还一直在增长，强化。深度学习就在这儿，我们看不到寒冬。\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 490px; max-height: 323px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 65.92%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"490\" data-height=\"323\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/1496926-8d610fda7cec0db6\" data-original-width=\"490\" data-original-height=\"323\" data-original-format=\"image/png\" data-original-filesize=\"41614\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cbr\u003e\n\u003cem\u003e我们列举了对深度学习的发展做出重要贡献的人物。我相信我不需要再指出自从2012年以来其飞涨的趋势了。\u003c/em\u003e\u003cbr\u003e\n\u003cstrong\u003e后记：现状\u003c/strong\u003e\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e如果这是一部电影，2012年ImageNet比赛将是其高潮，而现在在电影结束的时候，我们将会出现这几个字：「他们如今在哪里」。Yann Lecun：Facebook； Geoffrey Hinton： 谷歌； 吴恩达： Coursera、谷歌、百度； Bengi、Schmidhuber 及 Hochreiter 依然还留在学术界——但我们可以很容易推测，这个领域将会有更多的引用及毕业生。\u003c/p\u003e\n\u003cp\u003e虽然深度学习的理念及成绩令人振奋，但当我在写这几篇文章的时候，我也不由自主地被他们所感动，他们在一个几乎被人遗弃的领域里深耕数十年，他们现在富裕、成功，但重要的是他们如今更确信自己的研究。这些人的思想依然保持开放，而这些大公司也一直在开源他们的深度学习模型，犹如一个由工业界领导研究界的理想国。多美好的故事啊啊。\u003c/p\u003e\n\u003cp\u003e我愚蠢的以为我可以在这一部分写一个过去几年让人印象深刻的成果总结，但在此，我清楚知道我已经没有足够的空间来写这些。可能有一天我会继续写第五部分，那就可以完成这个故事了。但现在，让我提供以下一个简短的清单：\u003c/p\u003e\n\u003cp\u003e1.LTSM RNNs的死灰复燃以及分布式表征的代表\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"image-package\"\u003e\n\u003cdiv class=\"image-container\" style=\"max-width: 640px; max-height: 260px;\"\u003e\n\u003cdiv class=\"image-container-fill\" style=\"padding-bottom: 40.63%;\"\u003e\u003c/div\u003e\n\u003cdiv class=\"image-view\" data-width=\"640\" data-height=\"260\"\u003e\u003cimg data-original-src=\"//upload-images.jianshu.io/upload_images/1496926-62eb4790d03ce2cf\" data-original-width=\"640\" data-original-height=\"260\" data-original-format=\"image/jpeg\" data-original-filesize=\"35188\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-caption\"\u003e\u003c/div\u003e\n\u003c/div\u003e\u003cem\u003e去年的结果。看看吧！\u003c/em\u003e\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e2.利用深度学习来加强学习\u003c/p\u003e\n\u003cp\u003e3.附加外部可读写存储\u003c/p\u003e\n\u003cp\u003e参考文献：\u003cbr\u003e\nKate Allen. How a Toronto professor’s research revolutionized artificial intelligence Science and Technology reporter, Apr 17 2015 \u003ca href=\"https://link.jianshu.com?t=http://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html\" target=\"_blank\" rel=\"nofollow\"\u003ehttp://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html\u003c/a\u003e\u003cbr\u003e\nHinton, G. E., Osindero, S., \u0026amp; Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural computation, 18(7), 1527-1554.\u003cbr\u003e\nHinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural computation, 14(8), 1771-1800.\u003cbr\u003e\nBengio, Y., Lamblin, P., Popovici, D., \u0026amp; Larochelle, H. (2007). Greedy layer-wise training of deep networks. Advances in neural information processing systems, 19, 153.\u003cbr\u003e\nBengio, Y., \u0026amp; LeCun, Y. (2007). Scaling learning algorithms towards AI. Large-scale kernel machines, 34(5).\u003cbr\u003e\nMohamed, A. R., Sainath, T. N., Dahl, G., Ramabhadran, B., Hinton, G. E., \u0026amp; Picheny, M. (2011, May). Deep belief networks using discriminative features for phone recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on (pp. 5060-5063). IEEE.\u003cbr\u003e\nNovember 26, 2012. Leading breakthroughs in speech recognition software at Microsoft, Google, IBM Source: \u003ca href=\"https://link.jianshu.com?t=http://news.utoronto.ca/leading-breakthroughs-speech-recognition-software-microsoft-google-ibm\" target=\"_blank\" rel=\"nofollow\"\u003ehttp://news.utoronto.ca/leading-breakthroughs-speech-recognition-software-microsoft-google-ibm\u003c/a\u003e\u003cbr\u003e\nRaina, R., Madhavan, A., \u0026amp; Ng, A. Y. (2009, June). Large-scale deep unsupervised learning using graphics processors. In Proceedings of the 26th annual international conference on machine learning (pp. 873-880). ACM.\u003cbr\u003e\nClaudiu Ciresan, D., Meier, U., Gambardella, L. M., \u0026amp; Schmidhuber, J. (2010). Deep big simple neural nets excel on handwritten digit recognition. arXiv preprint arXiv:1003.0358.\u003cbr\u003e\nHinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A. R., Jaitly, N., … \u0026amp; Kingsbury, B. (2012). Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6), 82-97.\u003cbr\u003e\nLe, Q. V. (2013, May). Building high-level features using large scale unsupervised learning. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on (pp. 8595-8598). IEEE. ↩\u003cbr\u003e\nGlorot, X., \u0026amp; Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In International conference on artificial intelligence and statistics (pp. 249-256).\u003cbr\u003e\nJarrett, K., Kavukcuoglu, K., Ranzato, M. A., \u0026amp; LeCun, Y. (2009, September). What is the best multi-stage architecture for object recognition?. In Computer Vision, 2009 IEEE 12th International Conference on (pp. 2146-2153). IEEE.\u003cbr\u003e\nNair, V., \u0026amp; Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10) (pp. 807-814).\u003cbr\u003e\nGlorot, X., Bordes, A., \u0026amp; Bengio, Y. (2011). Deep sparse rectifier neural networks. In International Conference on Artificial Intelligence and Statistics (pp. 315-323).\u003cbr\u003e\nMaas, A. L., Hannun, A. Y., \u0026amp; Ng, A. Y. (2013, June). Rectifier nonlinearities improve neural network acoustic models. In Proc. ICML (Vol. 30).\u003cbr\u003e\nHinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., \u0026amp; Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.\u003cbr\u003e\nKrizhevsky, A., Sutskever, I., \u0026amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105).\u003cbr\u003e\n\u003ca href=\"https://link.jianshu.com?t=http://www.technologyreview.com/news/524026/is-google-cornering-the-market-on-deep-learning/\" target=\"_blank\" rel=\"nofollow\"\u003ehttp://www.technologyreview.com/news/524026/is-google-cornering-the-market-on-deep-learning/\u003c/a\u003e\u003c/p\u003e\n","voted_down":false,"rewardable":true,"show_paid_comment_tips":false,"share_image_url":"http://upload-images.jianshu.io/upload_images/1496926-3b81b0bae7b4864c","slug":"e1bac195f06d","user":{"liked_by_user":false,"following_count":447,"gender":0,"avatar_widget":null,"slug":"8ad7903302b3","intro":"敏于行而慎于言,讷于言而敏于行\n\nhttp://blog.csdn.net/jiandanji...","likes_count":479,"nickname":"jiandanjinxin","badges":[],"total_fp_amount":"12854642470228002750","wordage":184235,"avatar":"https://upload.jianshu.io/users/upload_avatars/1496926/af15a9ba-97c7-4d07-91d2-c8a4c9a61418.png","id":1496926,"liked_user":false},"likes_count":6,"paid_type":"free","show_ads":true,"paid_content_accessible":false,"total_fp_amount":"312000000000000000","trial_open":false,"reprintable":true,"bookmarked":false,"wordage":8203,"featured_comments_count":0,"downvotes_count":0,"wangxin_trial_open":null,"guideShow":{"new_money_time_reward_type":6,"audit_user_nickname_spliter":0,"pc_note_bottom_btn":0,"pc_like_author_guidance":1,"audit_user_background_image_spliter":0,"audit_note_spliter":0,"new_user_no_ads":1,"launch_tab":0,"include_post":0,"pc_login_guidance":1,"audit_comment_spliter":1,"pc_note_bottom_qrcode":1,"audit_user_avatar_spliter":0,"flow_ad_check_detail_button_style":1,"audit_collection_spliter":0,"pc_top_lottery_guidance":1,"subscription_guide_entry":1,"creation_muti_function_on":1,"audit_user_spliter":1,"pc_note_popup":0},"commentable":true,"total_rewards_count":0,"id":18136088,"notebook":{"name":""},"description":"来自Andrey KurenkovA 'Brief' History of Neural Nets and Deep Learning, Part4深度 | 神经网络和深度学...","first_shared_at":1507620481,"views_count":294,"notebook_id":3059498},"baseList":{"likeList":[],"rewardList":[]},"status":"success","statusCode":0},"user":{"isLogin":false,"userInfo":{}},"comments":{"list":[],"featuredList":[]}},"initialProps":{"pageProps":{"query":{"slug":"e1bac195f06d"}},"localeData":{"common":{"jianshu":"简书","diamond":"简书钻","totalAssets":"总资产{num}","diamondValue":" (约{num}元)","login":"登录","logout":"注销","register":"注册","on":"开","off":"关","follow":"关注","followBook":"关注连载","following":"已关注","cancelFollow":"取消关注","publish":"发布","wordage":"字数","audio":"音频","read":"阅读","reward":"赞赏","zan":"赞","comment":"评论","expand":"展开","prevPage":"上一页","nextPage":"下一页","floor":"楼","confirm":"确定","delete":"删除","report":"举报","fontSong":"宋体","fontBlack":"黑体","chs":"简体","cht":"繁体","jianChat":"简信","postRequest":"投稿请求","likeAndZan":"喜欢和赞","rewardAndPay":"赞赏和付费","home":"我的主页","markedNotes":"收藏的文章","likedNotes":"喜欢的文章","paidThings":"已购内容","wallet":"我的钱包","setting":"设置","feedback":"帮助与反馈","loading":"加载中...","needLogin":"请登录后进行操作","trialing":"文章正在审核中...","reprintTip":"禁止转载，如需转载请通过简信或评论联系作者。"},"error":{"rewardSelf":"无法打赏自己的文章哟~"},"message":{"paidNoteTip":"付费购买后才可以参与评论哦","CommentDisableTip":"作者关闭了评论功能","contentCanNotEmptyTip":"回复内容不能为空","addComment":"评论发布成功","deleteComment":"评论删除成功","likeComment":"评论点赞成功","setReadMode":"阅读模式设置成功","setFontType":"字体设置成功","setLocale":"显示语言设置成功","follow":"关注成功","cancelFollow":"取消关注成功","copySuccess":"复制代码成功"},"header":{"homePage":"首页","download":"下载APP","discover":"发现","message":"消息","reward":"赞赏支持","editNote":"编辑文章","writeNote":"写文章"},"note":{},"noteMeta":{"lastModified":"最后编辑于 ","wordage":"字数 {num}","viewsCount":"阅读 {num}"},"divider":{"selfText":"以下内容为付费内容，定价 ¥{price}","paidText":"已付费，可查看以下内容","notPaidText":"还有 {percent} 的精彩内容","modify":"点击修改"},"paidPanel":{"buyNote":"支付 ¥{price} 继续阅读","buyBook":"立即拿下 ¥{price}","freeTitle":"该作品为付费连载","freeText":"购买即可永久获取连载内的所有内容，包括将来更新的内容","paidTitle":"还没看够？拿下整部连载！","paidText":"永久获得连载内的所有内容, 包括将来更新的内容"},"book":{"last":"已是最后","lookCatalog":"查看连载目录","header":"文章来自以下连载"},"action":{"like":"{num}人点赞","collection":"收入专题","report":"举报文章"},"comment":{"allComments":"全部评论","featuredComments":"精彩评论","closed":"评论已关闭","close":"关闭评论","open":"打开评论","desc":"按时间倒序","asc":"按时间正序","disableText1":"用户已关闭评论，","disableText2":"与Ta简信交流","placeholder":"写下你的评论...","publish":"发表","create":" 添加新评论","reply":" 回复","restComments":"还有{num}条评论，","expandImage":"展开剩余{num}张图","deleteText":"确定要删除评论么？"},"collection":{"title":"被以下专题收入，发现更多相似内容","putToMyCollection":"收入我的专题"},"seoList":{"title":"推荐阅读","more":"更多精彩内容"},"sideList":{"title":"推荐阅读"},"wxShareModal":{"desc":"打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮"},"bookChapterModal":{"try":"试读","toggle":"切换顺序"},"collectionModal":{"title":"收入到我管理的专题","search":"搜索我管理的专题","newCollection":"新建专题","create":"创建","nothingFound":"未找到相关专题","loadMore":"展开查看更多"},"contributeModal":{"search":"搜索专题投稿","newCollection":"新建专题","addNewOne":"去新建一个","nothingFound":"未找到相关专题","loadMore":"展开查看更多","managed":"我管理的专题","recommend":"推荐专题"},"QRCodeShow":{"payTitle":"微信扫码支付","payText":"支付金额"},"rewardModal":{"title":"给作者送糖","custom":"自定义","placeholder":"给Ta留言...","choose":"选择支付方式","balance":"简书余额","tooltip":"网站该功能暂时下线，如需使用，请到简书App操作","confirm":"确认支付","success":"赞赏成功"},"payModal":{"payBook":"购买连载","payNote":"购买文章","promotion":"优惠券","promotionFetching":"优惠券获取中...","noPromotion":"无可用优惠券","promotionNum":"{num}张可用","noUsePromotion":"不使用优惠券","validPromotion":"可用优惠券","invalidPromotion":"不可用优惠券","total":"支付总额","tip1":"· 你将购买的商品为虚拟内容服务，购买后不支持退订、转让、退换，请斟酌确认。","tip2":"· 购买后可在“已购内容”中查看和使用。","success":"购买成功"},"reportModal":{"ad":"广告及垃圾信息","plagiarism":"抄袭或未授权转载","placeholder":"写下举报的详情情况（选填）","success":"举报成功"},"guidModal":{"modalAText":"相似文章推荐","subText":"下载简书APP，浏览更多相似文章","btnAText":"先不下载，下次再说","followOkText":"关注作者成功！","followTextTip":"下载简书APP，作者更多精彩内容更新及时提醒！","followBtn":"下次再说","downloadTipText":"更多精彩内容下载简书APP","footerDownLoadText":"下载简书APP","modabTitle":"免费送你2次抽奖机会","modalbTip":"你有很大概率抽取AirPods Pro","modalbFooterTip":"下载简书APP，天天参与抽大奖","modalReward":"抽奖","scanQrtip":"扫码下载简书APP","downloadAppText":"下载简书APP，随时随地发现和创作内容","redText":"阅读","likesText":"赞","downLoadLeft":"下载App"}},"currentLocale":"zh-CN","asPath":"/p/e1bac195f06d"}},"page":"/p/[slug]","query":{"slug":"e1bac195f06d"},"buildId":"mJJ-kJwXxgtj1jdtJSnxx","assetPrefix":"https://cdn2.jianshu.io/shakespeare"}</script><script nomodule="" src="https://cdn2.jianshu.io/shakespeare/_next/static/runtime/polyfills-83c9f0eea3aa0edfd89e.js"></script><script async="" data-next-page="/p/[slug]" src="https://cdn2.jianshu.io/shakespeare/_next/static/mJJ-kJwXxgtj1jdtJSnxx/pages/p/%5Bslug%5D.js"></script><script async="" data-next-page="/_app" src="https://cdn2.jianshu.io/shakespeare/_next/static/mJJ-kJwXxgtj1jdtJSnxx/pages/_app.js"></script><script src="https://cdn2.jianshu.io/shakespeare/_next/static/runtime/webpack-69a7d3bdb55520eaee8f.js" async=""></script><script src="https://cdn2.jianshu.io/shakespeare/_next/static/chunks/commons.46a9d5f73f088e7dfd5a.js" async=""></script><script src="https://cdn2.jianshu.io/shakespeare/_next/static/chunks/styles.e6b2d8c62a1ec682db37.js" async=""></script><script src="https://cdn2.jianshu.io/shakespeare/_next/static/runtime/main-8f9d517354ec2be84ab8.js" async=""></script></body></html>